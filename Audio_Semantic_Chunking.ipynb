{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97jDW6dhYI0_"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upoYo6G_-5kf"
      },
      "source": [
        "\n",
        "\n",
        "**Task: Semantic Chunking of a YouTube Video** ğŸ“¹\n",
        "- Dive into extracting meaningful audio-text pairs from a specific video. Show us your skill in achieving precise segmentation and alignment!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzqLZrA2UEmL"
      },
      "source": [
        "## Semantic Chunking of a Youtube Video\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "The objective is to extract high-quality, meaningful (semantic) segments from the specified YouTube video: [Watch Video](https://www.youtube.com/watch?v=Sby1uJ_NFIY).\n",
        "\n",
        "Suggested workflow:\n",
        "1. **Download Video and Extract Audio:** Download the video and separate the audio component.\n",
        "2. **Transcription of Audio:** Utilize an open-source Speech-to-Text model to transcribe the audio. *Provide an explanation of the chosen model and any techniques used to enhance the quality of the transcription.*\n",
        "3. **Time-Align Transcript with Audio:** *Describe the methodology and steps for aligning the transcript with the audio.*\n",
        "4. **Semantic Chunking of Data:** Slice the data into audio-text pairs, using both semantic information from the text and voice activity information from the audio, with each audio-chunk being less than 15s in length. *Explain the logic used for semantic chunking and discuss the strengths and weaknesses of your approach.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgrEbe8Qg1Mf"
      },
      "source": [
        "# Code Starts Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7nfddvS3k_n"
      },
      "source": [
        "# Detailed Explanations and Generalization of the Code\n",
        "\n",
        "## Detailed Explanations\n",
        "1. The code starts by defining a function `video2mp3` that converts a video file to an audio file using the FFmpeg library. It takes the video file path and the desired output extension as input and returns the path of the generated audio file.\n",
        "\n",
        "2. The `download_video` function is defined to download a YouTube video using the `pytube` library. It takes the video URL as input, selects the highest resolution progressive stream, and downloads the video. It returns the path of the downloaded video file.\n",
        "\n",
        "3. The `transcribe` function is defined to transcribe the audio file using the Whisper model. It loads the \"large-v3\" model, transcribes the audio, and returns the transcribed text.\n",
        "\n",
        "4. The `align_transcript` function aligns the transcribed text with the audio using the `ctc-forced-aligner` library. It takes the audio file path and the transcript as input, writes the transcript to a temporary file, and runs the forced alignment using the specified parameters. It returns the path of the alignment output file.\n",
        "\n",
        "5. The `time_to_seconds` function is a helper function that converts a time string in the format \"HH:MM:SS\" or \"MM:SS\" or \"SS\" to seconds.\n",
        "\n",
        "6. The `parse_vad_file` function parses the Voice Activity Detection (VAD) output file and extracts the speech segments. It reads the file line by line, matches the start and end times using regular expressions, and returns a list of dictionaries containing the start and end times of each speech segment.\n",
        "\n",
        "7. The `parse_text_timestamps_file` function parses the aligned text timestamps file and extracts the text segments along with their start and end times. It reads the file line by line, matches the start time, end time, and text using regular expressions, and returns a list of dictionaries containing the start time, end time, and text of each segment.\n",
        "\n",
        "8. The `combine_segments` function combines the VAD segments with the text segments to create audio-text pairs. It iterates over each text segment, finds the overlapping VAD segments, and combines them into chunks of a specified maximum duration. It returns a list of dictionaries containing the chunk ID, chunk length, text, start time, and end time of each combined segment.\n",
        "\n",
        "9. The `perform_vad` function performs Voice Activity Detection on the audio file using the `pyannote` library. It loads a pre-trained segmentation model, applies it to the audio file, and saves the VAD results to a file.\n",
        "\n",
        "10. The `process_youtube_video` function is the main function that orchestrates the entire process. It takes a YouTube video URL as input, downloads the video, converts it to audio, performs VAD, transcribes the audio, aligns the transcript, parses the VAD and text segments, combines them, and returns the combined audio-text pairs as a JSON string.\n",
        "\n",
        "11. Finally, the code creates a Gradio interface using the `gradio` library. It defines an interface with the `process_youtube_video` function as the main function, a text box for entering the YouTube video URL as input, and a JSON output for displaying the combined audio-text pairs. The interface is then launched using `iface.launch()`.\n",
        "\n",
        "## Generalization\n",
        "- The code provides a general approach for extracting audio-text pairs from YouTube videos. It can be applied to various types of videos, such as lectures, interviews, or presentations, where the goal is to align the spoken content with the corresponding text.\n",
        "\n",
        "- The approach relies on several libraries and models, including FFmpeg for video-to-audio conversion, pytube for downloading YouTube videos, Whisper for audio transcription, ctc-forced-aligner for transcript alignment, and pyannote for Voice Activity Detection. These libraries and models have been trained on diverse datasets and are generally applicable to a wide range of audio and video content.\n",
        "\n",
        "- However, there are potential failure modes and limitations to consider:\n",
        "  - The accuracy of the transcription and alignment may vary depending on the audio quality, background noise, accents, and speaking styles present in the video. Videos with poor audio quality, heavy accents, or overlapping speech may result in less accurate transcriptions and alignments.\n",
        "  - The code assumes that the video contains spoken content in English. For videos in other languages, the Whisper model and the forced alignment library would need to be adapted or replaced with models trained on the target language.\n",
        "  - The VAD model used in the code is pre-trained and may not be optimal for all types of audio. In some cases, custom VAD models trained on domain-specific data may yield better results.\n",
        "  - The maximum chunk duration is set to a fixed value (15 seconds in the code). Depending on the nature of the content, this value may need to be adjusted to ensure semantically meaningful segments.\n",
        "\n",
        "- To adapt the code for other languages, the following modifications would be required:\n",
        "  - Replace the Whisper model with a model trained on the target language for audio transcription.\n",
        "  - Modify the forced alignment library or use a different alignment tool compatible with the target language.\n",
        "  - Update the regular expressions used for parsing the VAD and text timestamp files to match the format of the output generated by the language-specific tools.\n",
        "  - Adjust any language-specific parameters or settings in the code, such as the VAD model or the text preprocessing steps.\n",
        "\n",
        "Overall, the code provides a starting point for extracting audio-text pairs from YouTube videos, but it may require further customization and fine-tuning based on the specific characteristics of the videos being processed and the desired output quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhJ5asBFKLof",
        "outputId": "e9469489-fde6-4d70-de2d-97aac114aad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed May 29 19:01:08 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   62C    P0              30W /  72W |   2839MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck3d51qEhvhN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBOzeKtohs0o"
      },
      "outputs": [],
      "source": [
        "def video2mp3(video_file, output_ext=\"wav\"):\n",
        "    filename, ext = os.path.splitext(video_file)\n",
        "    subprocess.call([\"ffmpeg\", \"-y\", \"-i\", video_file, f\"{filename}.{output_ext}\"],\n",
        "                    stdout=subprocess.DEVNULL,\n",
        "                    stderr=subprocess.STDOUT)\n",
        "    return f\"{filename}.{output_ext}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giDXcEk6hz9i",
        "outputId": "f0d00bea-04b8-4446-c769-d1184cb58638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.14.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=e2d20ab67fa8592bf62f337c8fafbb25172817734ace5ec818720e0a6e2ab243\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-m8h49177\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-m8h49177\n",
            "  Resolved https://github.com/huggingface/transformers to commit 5c88253556b7f15cf7d7e9793d7b2a39b4aa588a\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.42.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.42.0.dev0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.0.dev0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.0.dev0) (2024.2.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.42.0.dev0-py3-none-any.whl size=9131386 sha256=0cf69938bdb9d8ab0327942f3c86f0f28a3f13ce0126fdaa126b15b5de0bf0c1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-roz44ucu/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.1\n",
            "    Uninstalling transformers-4.41.1:\n",
            "      Successfully uninstalled transformers-4.41.1\n",
            "Successfully installed transformers-4.42.0.dev0\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.1)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.11.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.4-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.4 rapidfuzz-3.9.2\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.31.5-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.16.4 (from gradio)\n",
            "  Downloading gradio_client-0.16.4-py3-none-any.whl (315 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m315.9/315.9 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.4.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.4->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.16.4->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=c514d113a147624f52b402173b1c2d022cf547a59496fb08b677d5aa4c9cc9d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, ujson, tomlkit, shellingham, semantic-version, ruff, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, aiofiles, watchfiles, uvicorn, starlette, httpcore, email_validator, typer, httpx, gradio-client, fastapi-cli, fastapi, gradio\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 gradio-4.31.5 gradio-client-0.16.4 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.3 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.4.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 typer-0.12.3 ujson-5.10.0 uvicorn-0.30.0 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchaudio) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchaudio) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube # installing library to download youtube video\n",
        "!pip install -U openai-whisper\n",
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa soundfile\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install gradio\n",
        "!pip install -q bitsandbytes datasets accelerate\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git@main\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install torchaudio\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "24ZX0DnXh_ZS",
        "outputId": "89150e7f-2b5a-466b-de37-5896b1cb628b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.mp4'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "YouTube('https://youtu.be/Sby1uJ_NFIY').streams.first().download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n10sKMnKiAIl"
      },
      "outputs": [],
      "source": [
        "input_video = '/content/Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k9rqhhPiGfc",
        "outputId": "fbc94a9c-d45c-4ae7-d11a-230f08831727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpLd7dXjiNiW"
      },
      "outputs": [],
      "source": [
        "audio_file = video2mp3(input_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ffn5paKik2v"
      },
      "source": [
        "## Transcribing with Open AI Whisper Model ver-large-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGtrbl-PicFP"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "\n",
        "def translate(audio, output_file=\"transcription.txt\"):\n",
        "    model = whisper.load_model(\"large-v3\")\n",
        "    options = dict(beam_size=5, best_of=5)\n",
        "    translate_options = dict(task=\"translate\", **options)\n",
        "    result = model.transcribe(audio_file,**translate_options)\n",
        "    # Save the result to a text file\n",
        "    with open(output_file, \"w\") as file:\n",
        "        file.write(result[\"text\"])\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhkgBvAcixNP",
        "outputId": "2e5ef9e9-5bdd-46dd-f56e-0313f0c31690"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.88G/2.88G [00:24<00:00, 124MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Congratulations to you Mr. Raghavan for that. Thank you so much for joining us. Over to you. Hi everybody. How are you? I am not hearing this at all. It's like a post lunch energy downer or something. Let's hear it. Are you guys awake? Alright. You better be because we have a superstar guest here. You heard the $41 million and I didn't hear honestly anything she said after that. So we are going to ask for about $40 million from him by the end of this conversation. But let's get started. I want to introduce Vivek and Pratyush, his co-founder who is not here. We wanted to start with playing a video of what OpenHearty does. I encourage all of you to go to the website serverum.ai and check it out. But first of all, I want to thank you all for joining us. Let me start by introducing Vivek. Vivek is a dear friend and he is very, very modest, one of the most modest guys that I know. But his personal journey, Vivek, you got a PhD from Carnegie Mellon, you started and sold a company to Magma. And Vivek and I moved back to India from, we were both in the valley on the same day actually. And you have been in India for the last 16 years. And what most people don't know is your journey at Aadhaar. He spent 13 years, working selflessly at Aadhaar, nobody would have heard of him, but he was a pioneering technology visionary behind Aadhaar, which we all take for granted today. So, please give it out, so, honestly, when people, when I think of selfless service, truly selfless service, I always think of Vivek. And since then, he also was at AI for Bharat, which we are going to touch on, where he met Pratyush's other co-founder. Pratyush had a PhD from ETH at Zurich, he was at IBM Research, he was at Microsoft Research, playing a key role, and a faculty at IIT Madras, and at AI for Bharat. So, that's a little brief introduction about them, these guys are modest, modest engineers, so they don't toot their own horn. So, forgive me for tooting their horn in this case. But let's jump right in, about the money, funding. 41 million bucks man, that's a lot of money. Right, every entrepreneur here is saying, what the hell did these guys do? What did the investors see to write such a big cheque? No, I think it's a trend, a new trend of what's going on in India. I think that for the very first time, I think the investors have looked at, you know, let's try and build something deep tech out of the country. And let's try to figure out how to build something as a foundational technology out of the country. And that's really what's really exciting, you know. About, you know, as Bala was mentioning for the past 15 years, I've been kind of working in kind of, you know, both digital public infrastructure and kind of non-profit kind of things. But when this whole thing of generative AI came about, you know, we said, okay, how can I actually make a difference in this space? And I said, maybe this is the opportunity to actually come out. And really build something, you know, and the only way that we realized that you can do it is actually in the private sector. And I think that's, and then we went out there and we said we want to build something which is a continuation, right? I mean, and fundamentally the question is, the reason of what we want to do at Sarvam AI is we want to basically make generative AI available and accessible to the people in the country. And that's the intent. And when we said that we want to do this, there was a resonance in the investment community. And I think it's a responsibility to really to show that something like this can be built out of India. So we see that as confidence and a responsibility. And I also hope it's a trend that, you know, that there are many more people like us who are backed. Because if you look at it, maybe it's a large number in a, you know, in the Indian context. But in the global context, I think there is just, there should be many, many more people. There should be many more entrepreneurs who are backed to do things in India. I'm going to come back to the many more entrepreneurs. I'm obviously going to ask you about Bhavesh's Krutrim. So we're going to come back to that question. But again, $41 million, I mean, all of what you said, you know, $2 million, you know, that's a good amount of money for a startup which, you know, which has not yet built anything. What are you going to do with all this money? I can solve the problem. I can have a perfect solution for the problem. I think in the last week, I've got lots of calls from lots of people telling me how I can do. I know you first, okay. I'll be landed in the country the same day. I'm in front of the queue. No, but honestly, I think the key thing in this is to putting together an amazing team. And we actually have an amazing team, but we believe that it is talent that will drive this kind of thing. And so it is to get key talent. And of course, the other thing is compute. This is extremely expensive compute-wise to actually do these kinds of things. And I think that those are the two primary things that, you know, we'd use this for. Okay. I'm computing in my own head as an entrepreneur. Talent, okay, you have like 20, 15 people. How much are you paying these guys? But okay, we won't touch on that. But let's talk about what you guys actually built. What is Open Hathi? How would you explain Open Hathi to many people here who might not have known about it? So I think Open Hathi is, so first of all, right, we come from, I personally come from the open source ecosystem and also from the DPI ecosystem. So we believe that for this to work, we need the ecosystem to be successful. And as a result of that, one of the first things we did was, hey, there are these open source large language models that exist, right? I mean, everybody knows about the Lama family from Meta. They also, there are others like Mistral. There are a bunch of open source, you know, large language models. And then we said, is there any way that take an existing open source model and teach it language skills, right? I mean, and that is really the, you know, what we decide, what we said that can we do something like that? And is this a, you know, relatively frugal way of actually, you know, making models, you know, work in diverse languages? Because the truth is still today, I mean, if you look at the amount of data and knowledge, it is still English dominates these things. And I think that how do you actually take and make it understand Indian language, understand Indian context, and all of those things in actually a, in an efficient way. And therefore, this was an attempt to do that. And Open Hathi is, you know, is currently based on the Lama 7 billion model, but we will be releasing many more models in different languages, different sizes, and things like that as part of this, as part of this series. And of course, you know, we will be building further models on those and doing other things to actually, and we will also have endpoints that people can use. So therefore, it is not, it is definitely, you know, something that people can use to things. And that is the essence of what this Open Hathi is. So what does it mean to people in the audience here who are either doing their own startups or a business or developers? How should they look at OpenAI? Sorry, Sarman, not OpenAI. No, I think the way you look at it is that we, one of the important things that we are doing is we are not just building models. We are also going to be building a platform. A platform for developers where you can actually use a combination of various different kinds of models, some which are from us, some which are open source, some which may not be open source, and actually to actually pull together and figure out how to deploy, you know, generative AI applications at scale and understand and evaluate their performance in an efficient manner. And that is something that we are planning to do. And this platform is, you know, in the next couple of months will be coming out there. So that is something that we are planning to do to developers. But of course, those who want to start with the open source things and hack for that, of course, please go ahead and do that as well. That is phenomenal. But how does it compare to OpenAI itself or Google? See, at least the things that we are doing now, right? I mean, one of the things that when we thought about building Sarman, we said we want to build a full stack generative AI company. And what different people are understanding of a full stack is that we need to know how to train models from scratch. We need to know how to kind of figure out how to deploy models to solve real world use cases. And we need to play in the ecosystem to make sure that we can actually deploy population scale applications, right? So we were thinking about all of these things. But still the models we were talking about are, you know, fairly small models. They are fairly small models, right? The 7 to maybe up to 70 billion kind of range we are talking about. While these models like OpenAI and Google are obviously much bigger models, right? But we want to understand the techniques and be able to build that muscle to do all of these things to make it available to people. Now those models are, I mean, as I said, you know, I think that there is space for all of those things. And I think as even Sridhar was talking about earlier in the day, we believe that these smaller models can do very, I mean, many, many kind of domain specific tasks extremely well. Probably even better than the larger models. And that is really one of the key areas. And so therefore the value of these kinds of things, right? We are not aiming in these set of models to build any AGI, right? That's not our goal here. Our goal is to make things that work extremely well for domain specific use cases or increase accessibility through language and all of those kinds of things. And obviously all of this unique to India. But what is unique about India? I mean, like what is, is there anything special in our ecosystem that makes small models focused with Indian languages better for, more suited for our problems? So I think that, I mean, there are quite a few things that are unique about India, right? The first thing is I think that we are a voice first nation. So therefore I think voice has to be the core to doing things. The other thing, of course, India is extremely, it's a cost conscious country from a cost perspective. Now I would say that there are lots of interesting use cases where you can use open AI and the cost structure works depending on your application. But when you want to scale things to a massive level and make it work, then you have to figure out how small models work. So that's something that is also specific to India. The third thing which is specific to India is really the success that India has had in building all this digital public infrastructure. When you add the AI layer on top of it, then you can actually get dramatic, you know, dramatic, I think, multiplicative, combinatorial effects based on doing things like that. That's a phenomenal point. Like, you know, it's like DPI to the power of AI almost in some ways. And as a part of Aadhaar, building Aadhaar, no better person than you. So in summary, what I'm hearing is small models specialized with, trained with Indic specific language data suited for Indian problems at a compelling cost point will be suited for us. We are not solving some world autonomous vehicles or some complex problem. We are solving some basic problems specifically focused on voice with multiple languages. That is what you see as the future. Am I paraphrasing this correctly? No, yeah. So I think that certainly, I mean, voice and Indian languages are an important part of our strategy. But we will be building, you know, custom models to solve various other kinds of problems as well, right? It's not just limited to, I think, in different domains, working in different domains, making, building things based on unique data that enterprises have and things like that. So that's something that we'll also look at. Fair enough. So coming back to the elephant in the room, no fun intended with open Hathi. What about Bhavesh Agarwal and Krutrim? What is your take on that? I think it's great. I think it's wonderful, right? I mean, the fact that the technology AI is so important that we need multiple people working on it. The fact that there are other people thinking is actually validates that this is an important problem to be solved. And I think thatâ€¦ And we need everybody to come together and do that. So I really welcome that. I think it's great. And I think that there will be different people will have different takes as to how to solve this kind of problem. And hopefully as a result of that, the entire ecosystem benefits. I have one more question and then I want to talk about some of the predictions that you've boldly made. So Vivek, I usually ask people about what do you think the future will be and everybody usually hedges. I asked Vivek, what do you think is going to happen by December 2024? What do you think sitting in this room one year later we can expect? And he made three bold predictions. So I want to talk about that. Before that, I have one last question. What are the top three applications that you think are relevant for India? You heard Sridhar talk about medical. Quick summary, what do you think the top three apps are for India for AI? So, I mean, I think that, as you said, things like education and medical are clear. They are clearly areas where I think that things can be leveraged. The whole idea of all these kind of the DPI aspect of it is another major application where things can happen. And here I'm talking about country specific. And I think the whole idea which Sridhar also talked about was the concept of software, right? And I think that and clearly we have a very large software industry and how to reimagine those things in this context is also something that's going to be. Fair enough. Are you guys ready for Vivek Raghavan's bold predictions? Yes? No, I'm not hearing any yes. This is like a big deal. He's like one of the smartest guys that I know. He wants to make three predictions. You don't want to hear it? All right. So I asked him, what do you think, you know, a year later, what do you think we can expect? And he came up with three things. And usually people give very blah answers when you ask a question like this because they don't want to be caught wrong. Not Vivek. So he basically said three things and I'm going to list out the three things and then ask him about it. So number one, he says, I would prefer to talk to an automated customer service than a real person because they'll give me a better answer. So that is Vivek Raghavan's prediction number one. So number two is that when everybody is talking about a GPU shortage, Vivek predicts that there will be a GPU glut in India. He thinks there will be too much GPU. So if you want a short NVIDIA stock, this is a good time. And number three, which was extremely unexpected, he said some companies will suddenly die. So Vivek, these are not what I expected. So do you want to quickly talk about each of them, why you just came up with these and then we'll throw the open for audience questions. Vivek Raghavan So I don't think I quite said it the way that Bala is kind of saying it. But it's interesting. But I think the first thing that we said is I think that, and I don't think that this is, I think there will come a time when, you know, in areas of customer service, etc., when you want to do something very specific. Today, you know, when you call some kind of a bot, you actually end up, you mostly try to disconnect the call or, you know, you're extremely upset that you're talking to a bot. But I think that there will come a time, and I'm predicting it is sooner than later, that you will actually get better responses from the bot than what the human representative, that at least the average human representative that you could talk to could give. And I think that that's just a, so I just said that there will come a time where, you know, it's not a human you're talking to, but it's probably more likely to solve your intent than the human person. That's just something that I think that could happen. Okay, definitely controversial, but we'll let it go. What about the GPU glut? No, no. Yeah, so I don't think that, so I think that the fact that there is a tremendous shortage right now, I think that shortage will ease because that is how the cycles of things go, right? When, you know, I think the fact that there was such a severe shortage last year, you know, basically caused a number of different players to ramp up in various kinds of forms. And I think that that will always go in a cycle. But you may, we may find out that there are many, many more interesting problems that people will be able to solve. I still remember, you know, we were at a GenAI event in Bangalore and we were talking to people and we said, you know, how many people have access to, you know, a 4A? This was the question that I'd asked. And nobody in the room, and these are all extremely enthusiastic GenAI people, and nobody had access. And I think that thing is going to change. You will be able to get these kinds of things and people who want to hack and do things will have access to these things without, you know, having to write a, you know, a major check. Vivek is also a semiconductor guy before he went into Aadhaar, so I would take his predictions very seriously. So I don't know what I'm going to sell my media stock. I would not do that. That's not what I said. I want to blame you for this if it goes up. But the third one is pretty strange. You know, companies are born, companies die. But you said some companies will suddenly die. What does that mean? No, I think, see, I think the interesting thing is, and I think that it comes back to the fundamental nature of AI. AI is a tool, right? And you have to use that. And you have to use that within your business process. And how AI is being used and so, and what's going to happen is that, I mean, I think this is true with, you know, when someone said in terms of, you know, people, they said that the people who leverage AI will be more effective than those who don't leverage AI. And that is true for organizations also. Organizations that leverage AI in fundamentally in their core business processes will be more effective than those who don't. Right? And I think that's the thing. And you won't know the difference until one day it becomes too obvious and it will be too late. And I think that's the reason why everybody needs to think about what it means for your business. Because you will, everything will be fine. Everything will be fine. Then one day somebody in your, either your competitor in your space or somebody brand new coming into your space will be reimagining your business process completely. And at that stage you will find that it's, you know, it's a very big, very tall, you know, mountain to climb. And that's why I think it's important for both people and entities to think about how they will, you know, they will upgrade themselves or they will modify their business processes to, you know, to. That's a very nuanced answer and everybody here who is running a business should really think about it because life will be the same. And then suddenly, suddenly something will, you know, then it will be a step change. Vivek, I have a few more questions but I'm sure the audience has a lot of questions for you. So how are we doing on time? Okay. So does, okay, a lot of questions so love to, is there a mic that we can pass around? Thank you. My name is Karthik. I work for IT service industry. So you're saying that you're working on LLM, sorry, it's a fine tuned LLM on top of Lama. My basic question, fundamental question is we don't have a foundational model for India. Most of the models are basically using English or those kind of things. For example, even Andrew was talking about the tokenizers and things like that. So are you working on anything like that or do you want to use mostly the existing models and run on top of them? You asked a good question. You asked a cherry question for him, sir. No, I think the interesting thing is that if you look at and then we have actually a blog on this on our website. I think one of the things that we've built is we've actually built a customized tokenizer which actually fundamentally changes the cost of some of these generations in Indian languages. And I think that we're not just fine tuning. We're actually, we are leveraging the existing pre-training. But we are doing what's known as continual pre-training which actually, but having said that, you know, I think that when we have to figure out where is the data to train an extremely large model from scratch. And some of those things are things which will happen over time. But I think that, I think that, yes, I think that we will be doing various kinds of things. But the interesting thing is that if I want to change the accessibility problem with an existing open source model, how do I do that? And that's the problem that we have, that we think we have solved and it's going to be the heart of this OpenRTC series. Extremely well explained in the blog. Even I could understand it, so. Hi, I'm Prashant. I work for a fintech company. My question is like unlike China, we never had a consumer facing application coming out from India and in Web 1, Web 2, crypto and all. Why do you think it will be different this time in like... AI? Because will the DPI and other things will serve the same purpose what the Great Firewall did in China? Or do you think like in, because AI is a strategic sector, no outside country can work in NASA projects. Maybe all government contract will go to them. What exactly is the moat here for an Indian company? So, I don't, I think the question is, I don't know the answer to these questions, right? I mean, I think that it's difficult to predict, but I do believe and as I'm repeating that the combinatorial effect of being using GenAI at a large scale in addition along with the DPI work that we've done in India will have people. And I think that in the end, it is, the intent is that people need to be able to use it and they will vote by things that are useful for them. And if that doesn't happen, you're right that, and I think that we have to figure out what is the mechanism of delivery of apps, right? I mean, how, where do Indians consume content? That's the question. I'm so sorry, but we are out of time. Vivek will be outside. Yeah. So, he would be able to answer the question. Do we have time for one last question? Can I, can I just take one last? Yeah. Thank you. Thank you. I'm Manish Kothari. I'm from ISBR Business School. Good that I got a chance to ask you this question. During lunchtime, there were a few of our educationists whom we were talking about and we were, there was one from school and we are from the MBA institutions. We were thinking of these present generations, how do we get them into what you are doing? There is one thing that they have been regularly that the concentrations that they are working on, but artificial intelligence and getting into this. Getting them into their academics and making them a part of it is very important, including the trainers who train them. Making them future ready into what you are doing is amazing and the speed that which is growing, it is calling for a lot of training that needs to be done. Can you, from your angle, throw some light on how we could make them future ready? How these people who are, who are management graduates and from schools who are coming out, how do we get into this part of technology that you spoke about? So this is, this is really a challenge because I think everyone will need to understand at some level what this technology does. And I think that we have to rethink how we get everyone into this. And this kind of education has to be at many different levels, right? There are from a core set of having people who are extremely good at some, and there you don't need as many. Yes. There are vast numbers of people who can actually leverage these tools. By the way, the most important thing about, and maybe that's part of what makes an LLM interesting, is that how you use it, your mileage varies by that. And to understand how to actually leverage this in an interesting way is something that we have to widely teach many, many people. And because asking the, you know, things in the right way and having the right kind of applications will make a huge difference to how people can leverage these tools. Awesome. Thank you. Thank you. Thank you very much Vivek. Very good luck to Sarvam and good luck to India. I think it's going to be a lot right on your shoulders. Thanks Bala. Thank you Mr. Raghavan.\n"
          ]
        }
      ],
      "source": [
        "result = translate(audio_file)\n",
        "print(result[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ojg7UrXmQ_I"
      },
      "source": [
        "*As in the video it can be seen that the speakers on the stage are Indian so Fine-tuning Whisper on Indian English accents can significantly improve the transcription accuracy, especially for technical or domain-specific terms, as evident from the errors in the above transcript.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgD6JaHDooQz"
      },
      "source": [
        "*In the given transcript, there are several instances where chemical or technical words are incorrectly transcribed. For example: open hearty should be OpenHathi, Hati in a lot of places is actually Hathi, Ai for Bharat is actually AI4Bharat*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWPKuLUwpc8W"
      },
      "source": [
        "Fine-tuning Whisper on a dataset of Indian English accents can help the model learn and adapt to the specific pronunciation patterns, intonations, and linguistic nuances of Indian speakers. By exposing the model to a diverse range of Indian English accents during training, it can better recognize and transcribe the unique ways in which Indians pronounce certain words, including technical terms and proper nouns.\n",
        "Furthermore, fine-tuning Whisper on domain-specific data, such as conversations or speeches related to technology, AI, or chemistry, can further enhance its ability to accurately transcribe technical terms within those domains. By learning from a corpus of Indian English speech data that includes technical jargon and domain-specific vocabulary, the model can improve its recognition of chemical words and other specialized terms. So I went onto finetuning Whisper Large-V3 on the Svarah dataset created by AI4Bharat Team."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489,
          "referenced_widgets": [
            "56094354fe604dc38e3ef245fe4b6169",
            "7ae0f628aae94e368c929b0a8959a77a",
            "d80c56618e2e49b2ae8a6387962bd392",
            "5bc0bceec68841688c2b1dded7236056",
            "30becc36a8bf4333ba7b72f5d1027731",
            "743c4121f60c41f5ab9ba621322ba9c9",
            "190349a826344506b65a0a64f9aaeb7f",
            "35433bfd59c74d209b73fbbcb8bb7c2b",
            "c205a19fe8e14f1c97afa0cda2445c95",
            "2b15ba5caf564fbcb0dc43d3a0d33c11",
            "6298f658e7d74cd2a65e123555902806",
            "0d4910a7cc674d628b465105629f7ffa",
            "e2727108623a45aba30bfa9290eadb15",
            "4cb181b9886d486b859e8d8f384d0eb3",
            "c07a544bab5f4c489acb7321b4186f0c",
            "2c9afd0476b64685a6dabd116c23cf93",
            "f703716be588407cb1bc8b8d69a86cef",
            "781232b3a5ee416491de3f86e77c835c",
            "933092c73f2c4be79c1ee3e513eaf7ff",
            "9c72657dc5f846108ee5c9fa09826be4",
            "4382e0dd0d8e4089827f3b82aa042130",
            "1d784a56e98b410bb793169e066e9782",
            "81fceb9226b54b268a0d084abc7712bb",
            "92cc37f9369049e0aa96c26df482f1e5",
            "a50341e0ce6d423b82e3c7f3b2b37d51",
            "8ca5cc57adeb42a588ae0d6bbfd7c30a",
            "f6f1d1c1179f490ca16864caaff33c03",
            "452b28f0c75c47609a5d88f80ff8854a",
            "1c4b80da5a754e7381926bb01de12e2a",
            "2ba6265c0bce4d8387bff4b7b1d83928",
            "e3efe8bb0df1418885086235f37e642a",
            "5b885b3769564515a45223c0d06eff36",
            "eb65558f1f94460b9f4da9e1f9271083",
            "8fc9666c395641a8a6b2624c0017c32b",
            "40c7a0487cc344b09fd435802cbf9278",
            "b4db7a44620b4980b278ac4b930a5e59",
            "822f030d7f4d4ac9b56ee20ad7daafd9",
            "598c0ef4b53448559a9d4fa38917d232",
            "428aa8e8d063405e8dbca9269973ce85",
            "eb20742584f14a1a8162fa70162c1f8a",
            "d8b8a36590af4faca16e5b2cfed064b9",
            "60c65e0c14564a3192f852980bba90ab",
            "817dc66dcdd349dfa12b66a6f6a90502",
            "f444fff4de73484e8bbec1046e23b0b1",
            "4a908170020f46abbb94cf21135e68c7",
            "ed15e746ca0f450e9d2abb747e8680e3",
            "837b6c50109e4cab8a62a31cb12888fb",
            "85674eaf6dea4f0281c35bad82f47869",
            "e7d1151b76ec4526828c44355deb8689",
            "13f59e2ccd844d788570f176acf769c2",
            "b082c55371394c29bec699a6177b056f",
            "cceb2a151bd54a39abb11b743a3603c3",
            "be6bc2e1de61476289ad1532c7208c5c",
            "511e08a898614bda952eede1bb20822c",
            "a7ec526932fc41b088480a90e56c8bca",
            "5bc84f33a2434f1f825db47d40fa2739",
            "2ce338b0b9624e679a75825f1040e2ec",
            "84e705fda43f4efdbffaa443f2619cfd",
            "73983045b6e6492882a247cb2cf04cb1",
            "ed60357f7c60484ab609d1ab8066bade",
            "29ed7cbe7301411f805aca69657feb53",
            "ebaf7333eee34b69a629b3fd3ef967a1",
            "e1c61b153e714c1f92db915e9316c384",
            "83f7268639274d409e3dc08c7f24c352",
            "ec81609203484e209c070c2ecb24c7de",
            "0abbd7d7de9d425ca4fca03ea7c5c9c4",
            "1d34984ebd6a4cef82eb970294a451ff",
            "3d4cca895efc4bd4aab8e18e85e9e72f",
            "afa1fb1306df45b2920320c159c66644",
            "d9c64566a9f4493a92b0d5510b264872",
            "1747b4b3036d4442bc5b1a843ade94ca",
            "94c6383c98504cabab8b162cca1214c9",
            "d25de3bfa9c24af4a1a25b0400e9e20d",
            "f7569c3838a445fa918f4872d073eac1",
            "349936cae56345c59befaa0500120b76",
            "f896c7bef5c1431dbf056b69eb12c40e",
            "806f9f0b340d4cbaa3d6eff4cee3a404",
            "724a2bbdb6c34759be0ad3290e6ab47f",
            "a7b54d0620fb45dfa3f930aacdbc3cf2",
            "7111e0e805e64c3b90b8db7764f7bb8d",
            "4961955cfb4e421a97add6a1de50f19c",
            "b30f14f1704e44c2b5a92d3b5e8e14f9",
            "fec59df4e6f546ef976d8a20e542f24c",
            "7ed3d38024e8427ea8039ab4e58f2c71",
            "4f9e9e0e61b54d4b9bd2dad21271344a",
            "a62c8083bafe40b0a8f5a4bd5608b78e",
            "99c36fc0bd6c42f28435d1ce7613259f",
            "fa7bf1bc98134d31b0818fc76fadf286",
            "c6526dfb49f042e4aee5ccb580b87733",
            "0acc165857db47219da645c00c244d65",
            "02d2e85cab484b3a9091341c44780c62",
            "eb0fbe5ce2d04136bc711a4b146c44e4",
            "afcc3d193cad475c821d407d30028b46",
            "d715b73411c24233ab69abd640018e97",
            "702a05d600714cc791042a1d977130e4",
            "1e8a28c89e7847689790f1a9e006d9e3",
            "cea11a12caaa4bee9b2d682c4f68207f",
            "027b21f5f03a41d7bcdb36de15571423",
            "1fee5e3a3b9e47eda98de86b711c97c1",
            "1c69f9b7e5094de4984a74c6554956a1",
            "aac0a67eb85340abac426355616ec816",
            "491fd71c28104ee4a6ffe21f6f2cecfe",
            "7a9e265782264db8b2f36d5fccd37e27",
            "110f10a327c04a62b18d7e0f4247d6ef",
            "a3e38d5da3dc4368a234319de5ed2180",
            "fe32e2aa853e4083a5cdee671e556204",
            "e096cb47d6ca4dff98e4166efec79c92",
            "5f48a730b379428cb907f5cba43d560b",
            "f817e9acfd3242609e0f9102ddff45b2",
            "3857690d87de4b0bb41d102b7910eea7",
            "456e8542bf3e4e7caaad92cd8569a07b",
            "5616f8eb0079488fb860a836fbc7e441",
            "674322fe6a084d30ab4a8cd50a5fa431",
            "d8fcece450524a47b92a9792825141bf",
            "f075a5d2c2f54d1f97f17d9a5c47b80b",
            "23bf5ee24e3247aba1f8681e957b6fb3",
            "43a01895af9945e79358c730b3bfc028",
            "4a384350d66d46eaa67c63ff5992e024",
            "39f494c8811644c18526e8c06f75e1f0",
            "6b4201ef260a4dc8afb9c5c3e7ff2d6d",
            "e9d2c4e90bdd401a85ef7005ac1da98b",
            "66c1c4eb76df4ba5af46bb176209da67",
            "dd38c873cacf4e008cf56a30ce3b79eb",
            "791f516d0cec46dfaab43edb023981aa",
            "3c3b9c6bfbe84a4b8838c98fbd147192",
            "ade73ae644ba42c696d4879f11f039af",
            "4c3bc919f654439ba0ef81940630a668",
            "a3ac13ce490e477980d25dc78588dc06",
            "e67b2c92d0a446ca9a9c43d8bd14cbbd",
            "d51d6414acbd4fc2a08264021e317a47",
            "d8126038da974a3cafedefad5b12ba3d",
            "7ba2d4bf061b410e8e2b247b6480a8fb",
            "a33574da14104f5d95713859ce69f848",
            "23002b38f77e48679442d6479c0162c1",
            "03ba1fea762644e8a76fd9488b884e6e",
            "1627b08f94b845cba676b0c8a1e983fe",
            "d22bb179d08b4842bf7ae02e1ec73ce7",
            "85308ba994034a04968aa06e3187f429",
            "dc0cb85116124b0ab4d3719082860695",
            "76dadf0106c441b1a457ad906fe0e05f",
            "be79fbc8ae974bd092568e0b65250859",
            "171255372fbe4e94b31c4a23932b5dcc",
            "9f33c1a96bf54cf3be460ec8c4a29be0"
          ]
        },
        "id": "eOc2AFqiqFyW",
        "outputId": "dea40a79-a61a-4091-94bd-35322e4a85de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56094354fe604dc38e3ef245fe4b6169",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/771 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d4910a7cc674d628b465105629f7ffa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81fceb9226b54b268a0d084abc7712bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fc9666c395641a8a6b2624c0017c32b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a908170020f46abbb94cf21135e68c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/63.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bc84f33a2434f1f825db47d40fa2739",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d34984ebd6a4cef82eb970294a451ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "724a2bbdb6c34759be0ad3290e6ab47f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6526dfb49f042e4aee5ccb580b87733",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c69f9b7e5094de4984a74c6554956a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "456e8542bf3e4e7caaad92cd8569a07b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66c1c4eb76df4ba5af46bb176209da67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a33574da14104f5d95713859ce69f848",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import (\n",
        "    AutomaticSpeechRecognitionPipeline,\n",
        "    WhisperForConditionalGeneration,\n",
        "    WhisperTokenizer,\n",
        "    WhisperProcessor,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "\n",
        "peft_model_id = \"rs545837/finetuned_whisper\"\n",
        "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
        "language = \"English\"\n",
        "task = \"transcribe\"\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    peft_config.base_model_name_or_path, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
        "processor = WhisperProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
        "feature_extractor = processor.feature_extractor\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
        "pipe = AutomaticSpeechRecognitionPipeline(model=model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
        "\n",
        "def transcribe(audio, output_file):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        text = pipe(audio, generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids}, max_new_tokens=255)[\"text\"]\n",
        "\n",
        "    # Save the transcription to a file\n",
        "    with open(output_file, \"w\") as file:\n",
        "        file.write(text)\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "yEeC9NhXvhMp",
        "outputId": "be67bef9-5427-47b6-8343-1360d7d31c15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:480: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Congratulations to you Mr. Raghavan for that. Thank you so much for joining us. Over to you. Hi everybody. How are you? I am not hearing this at all. It's like a post lunch energy downer or something. Let's hear it. Are you guys awake? Alright. You better be because we have a superstar guest here. You heard the $41 million and I didn't hear honestly anything she said after that. So we're going to ask for about $40 million from him by the end of this conversation, okay? But let's get started. I want to introduce Vivek and Pratyush, his co-founder who's not here. We wanted to start with playing a video of what OpenHearty does. I encourage all of you to go to the website serverum.ai and check it out. but let me start by introducing Vivek Vivek is a dear friend and he is very very modest one of the most modest guys that I know but his personal journey Vivek you got a PhD from Carnegie Mellon, you started and sold a company to Magma and Vivek and I moved back to India we were both in the valley on the same day actually and you have been in India for the last 16 years and what most people don't know is your journey at Aadhaar He spent 13 years selflessly at Aadhaar. Nobody would have heard of him, but he was a pioneering technology visionary behind Aadhaar, which we all take for granted today. So please give it out. Honestly, when I think of selfless service, truly selfless service, I always think of Vivek. And since then, he also was at AI for Bharat, which we're going to touch on, where he met Pratyush's other co-founder. Pratyush had a PhD from ETH at Zurich. He was in the IBM research. He was at Microsoft research playing a key role and a faculty at IIT Madras and at AF Abharat. So that's a little brief introduction about them. These guys are modest, modest engineers. So they don't toot their own horn. So forgive me for tooting their horn in this case. But let's jump right in about the money, funding. 41 million bucks, man, that's a lot of money, right? Every entrepreneur here is saying, what the hell did these guys do? What did the investors see to write such a big check? No, I think it's a new trend of what's going on in India. I think that for the very first time, I think the investors have looked at, you know, let's try and build something deep tech out of the country and let's try to figure out how to build something as a foundational technology out of the country. And that's really what's really exciting. And I think that about, as Bala was mentioning, for the past 15 years, I've been kind of working in both digital public infrastructure and kind of non-profit kind of things. But when this whole thing of generative AI came about, we said, okay, how can I actually make a difference in this space? And I said, maybe this is the opportunity to actually come out and really build something. And the only way that we realized that you can do it is actually in the private sector. And then we went out there and we said we want to build something which is a continuation. And fundamentally the question is, the reason of what we want to do at Servam.ai is we want to basically make generative AI available and accessible to the people in the country. And that's the intent. And when we said that we want to do this, there was a resonance in the investment community. And I think it's a responsibility to really to show that something like this can be built out of India. So we see that as confidence and a responsibility. And I also hope it's a trend that there are many more people like us who are backed because if you look at it, maybe it's a large number in a, you know, in the Indian context but in the global context, I think there is just, there should be many, many more entrepreneurs who are backed to do things in India. I'm going to come back to the many more entrepreneurs. I'm obviously going to ask you about Bhavesh's Krutrim. So we're going to come back to that question but again, 41 million dollars, I mean all of what you said, you know, 2 million dollars, you know, that's a good amount of money for of startup which, you know, which has not yet built anything, what are you going to do with all this money? I can solve the problem. I can have a perfect solution for the problem. I think in the last week I've got lots of calls from lots of people telling me how I can do it. I know you first, okay? I'll be landed in the country the same day. I'm in front of the queue. But honestly, I think the key thing in this is to putting together an amazing team. and we actually have an amazing team but we believe that it is talent that will drive this kind of thing and so it is to get key talent and of course the other thing is compute. This is extremely expensive compute wise to actually do these kinds of things and I think that those are the two primary things that we would use this for. Okay. I'm computing in my own head as an entrepreneur. Talent, okay, you have like 20, 15 people. How much are you paying these guys? but okay, we won't touch on that. But let's talk about what you guys actually built. What is Open Hathi? How would you explain Open Hathi to many people here who might not have known about it? So I think Open Hathi is, so first of all, right, we come from, I personally come from the open source ecosystem and also from the DPI ecosystem. So we believe that for this to work, we need the ecosystem to be successful. And as a result of that, one of the first things we did was hey, there are these open source large language models that exist, right? I mean, everybody knows about the Lama family from Meta. There are others like Mistral. There are a bunch of open source large language models. And then we said, is there any way that take an existing open source model and teach it language skills, right? And that is really what we said, that can we do something like that? And is this a, you know, relatively frugal way of actually, you know, making models, you know, work in diverse languages? Because the truth is still today, I mean, if you look at the amount of data and knowledge, it is still English dominates these things. And I think that how do you actually take and make it understand Indian language, understand Indian context, and all of those things in actually an efficient way. And therefore, this was an attempt to do that. And Open Hathi is currently based on the LAMA 7 billion model, but we'll be releasing many more models in different languages, different sizes, and things like that as part of this series. And of course, we will be building further models on those and doing other things to actually... And we'll also have endpoints that people can use. So therefore, it's definitely something that people can use to things. And that's the essence of what this Open Hathi is. So what does it mean to people in the audience here who are either doing their own startups or a business or developers? How should they look at Open AI? Sorry, Sarman. Not Open AI. No, I think the way you look at it is that one of the important things that we are doing is we're not just building models. We are also going to be building a platform, a platform for developers where you can actually use a combination of various different kinds of models, some which are from us, some which are open source, some which may not be open source, and actually to actually pull together and figure out how to deploy, you know, generative AI applications at scale and understand and evaluate their performance in an efficient manner. And that's something that we're planning to do. And this platform is, you know, in the next couple of months will be coming out there. It will be available to developers. But, of course, those who want to start with the open source things and hack for that, of course, please go ahead and do that as well. That's phenomenal. But how does it compare to OpenAI itself or Google? See, at least the things that we are doing now, right? I mean, one of the things that when we thought about building Sarvam, we said we want to build a full stack generative AI company. And different people are understanding of a stack is that we need to know how to train models from scratch. We need to know how to kind of figure out how to deploy models to solve real world use cases. And we need to play in the ecosystem to make sure that we can actually deploy population scale applications. So we were thinking about all of these things. But still the models we were talking about are fairly small models. They are fairly small models, right? The 7 to maybe up to 70 billion kind of range we're talking about. While these models like OpenAI and Google are obviously much bigger models. But we want to understand the techniques and be able to build that muscle to do all of these things to make it available to people. Now, those models are, I mean, as I said, you know, I think that there is space for all of those things. And I think as even Sridhar was talking about earlier in the day, we believe that these smaller models can do very, I mean, many, many kind of domain specific tasks extremely well, probably even better than the larger models. And that is really one of the key areas. And so, therefore, the value of these kinds of things, right? We are not aiming in these set of models to build any AGI, right? That's not our goal here. Our goal is to make things that work extremely well for domain-specific use cases or increase accessibility through language and all of those kinds of things. And obviously all of this is unique to India. But what is unique about India? I mean, is there anything special in our ecosystem that makes small models focused with Indian languages better for, more suited for our problems? So I think that, I mean, there are quite a few things that are unique about India, right? The first thing is, I think, that we are a voice-first nation, so therefore I think voice has to be the core to doing things. The other thing, of course, India is extremely, it's a cost-conscious country from a cost perspective. Now, I would say that there are lots of interesting use cases where you can use open AI and the cost structure works depending on your application. But when you want to scale things to a massive level and make it work, then you have to figure out how small models work. So that's something that is also specific to India. The third thing which is specific to India is really the success that India has had in building all this digital public infrastructure. When you add the AI layer on top of it, then you can actually get dramatic, you know dramatic I think multiplicative combinatorial effects based on doing things like that. That's a phenomenal point like you know it's like DPI to the power of AI almost in some ways and as a part of Aadhaar building Aadhaar no better person than you. So in summary what I'm hearing is small models specialized with trained with Indic specific language data suited for Indian problems at a compelling cost point will be suited for us. We're not solving some world autonomous vehicles or some complex problem. We're solving some basic problems specifically focused on voice with multiple languages. That is what you see as the future. Am I paraphrasing this correctly? I think that certainly voice and Indian languages are an important part of our strategy but we will be building custom models to solve various other kinds of problems as well. It's not just limited to I think in different domains, working in different domains making, building things based on unique data that enterprises have and things like that. So that's something that we'll also look at. Fair enough. So coming back to the elephant in the room, no fun intended with open Hathi, what about Bhave Shagriwal and Kruthrim? What is your take on that? I think it's great. I think it's wonderful, right? I mean, the fact that the technology AI is so important that we need multiple people working on it. The fact that there are other people thinking actually validates that this is an important problem to be solved. And I think that we need everybody to come together and do that. So I really welcome that. I think it's great. And I think that there will be different people who will have different takes as to how to solve this kind of problem. And hopefully as a result of that, the entire ecosystem benefits. One more question and then I want to talk about some of the predictions that you've boldly made. So Vivek, I usually ask people about what do you think the future will be and everybody usually hedges. I asked Vivek, what do you think is going to happen by December 2024? What do you think sitting in this room one year later we can expect? And he made three bold predictions. So I want to talk about that. Before that, I have one last question. What are the top three applications that you think are relevant for India? You heard Sridhar talk about medical. Any quick summary? what do you think the top three apps are for India for AI? So, I mean, I think that, as you said, things like education and medical are clearly areas where I think that things can be leveraged. The whole idea of all these kind of... The DPI aspect of it is another major application where things can happen. And here I'm talking about country-specific work. And I think the whole idea which Sridhar also talked about was the concept of software, right? and I think that and clearly we have a very large software industry and how to reimagine those things in this context is also something that's going to be. Fair enough. Are you guys ready for Vivek Raghavan's bold predictions? Yes? No? I'm not hearing any yes. This is like a big deal. He's like one of the smartest guys that I know. He wants to make three predictions. You don't want to hear it? All right. So I asked him what do you think a year later what do you think we can expect? and he came up with three things and usually people give very blah answers when you ask a question like this because they don't want to be caught wrong. Not Vivek. Vivek is bold. So he basically said three things and I'm going to list out the three things and then ask him about it. So number one, he says, I would prefer to talk to an automated customer service than a real person because they'll give me a better answer. So that is Vivek Raghavan's prediction number one. So number two is that when everybody is talking about a GPU shortage, Vivek predicts that there will be a GPU glut in India. He thinks there will be too much GPU. So if you want a short NVIDIA stock, this is a good time. And number three, which was extremely unexpected, he said some companies will suddenly die. So Vivek, these are not what I expected. So do you want to quickly talk about each of them, why you just came up with these, and then we'll throw the open for audience questions. So I don't think I quite said it the way that Pala was going to say it. But it's interesting. But I think the first thing that we said is I think that, and I don't think that this is, I think there will come a time when, you know, in areas of customer service, et cetera, when you want to do something very specific. Today, you know, when you call some kind of a bot, you actually end up, you mostly try to disconnect the call or you're extremely upset that you're talking to a bot. But I think that there will come a time, and I'm predicting it is sooner than later, that you will actually get better responses from the bot than what the human representative, at least the average human representative that you could talk to could give. And I think that that's just a, so I just said that there will come a time where you know it's not a human you're talking to, but it's probably more likely to solve your intent than the human person. That's just something that I think that could happen. Okay, definitely controversial, but we'll let it go. What about the GPU glut? No, no, yeah, so I don't think that, so I think that the fact that there is a tremendous shortage right now, I think that shortage will ease because that is how the cycles of things go, right? when I think the fact that there was such a severe shortage last year basically caused a number of different players to ramp up in various kinds of forms. And I think that that will always go in a cycle. But we may find out that there are many, many more interesting problems that people will be able to solve. I still remember we were at a Gen AI event in Bangalore and we were talking to people and we said, you know, how many people have access to, you know, four A100s, this was the question that I'd asked, and nobody in the room, and these are all extremely enthusiastic Gen AI people, and nobody had access. And I think that thing is going to change. You will be able to get these kinds of things and people who want to hack and do things will have access to these things without, you know, having to write a, you know, a major check. Vivek is also a semiconductor guy before he went into Aadhaar, So I would take his predictions very seriously. So I don't know what I'm going to sell my media stock. I would not do that. But that's not what I said. I want to blame you for this if it goes up. But the third one is pretty strange. You know, companies are born, companies die. But you said some companies will suddenly die. What does that mean? No, I think, see, I think the interesting thing is, and I think that it comes back to the fundamental nature of AI. AI is a tool, right? And you have to use that. And you have to use that within your business process, right? And how AI is being used. And what's going to happen is that, I mean, I think this is true with, you know, when someone said in terms of, you know, people, they said that the people who leverage AI will be more effective than those who don't leverage AI. And that is true for organizations also. So organizations that leverage AI fundamentally in their core business processes will be more effective than those who don't. And I think that's the thing. And you won't know the difference until one day it becomes too obvious and it will be too late. And I think that's the reason why everybody needs to think about what it means for your business. Because everything will be fine. Everything will be fine and one day somebody in your, either your competitor in your space or somebody brand new coming into your space will be reimagining your business process completely. And at that stage you will find that it's a very big, very tall mountain to climb. and that's why I think it's important for both people and entities to think about how they will upgrade themselves or they will modify their business processes. That's a very nuanced answer and everybody here who's running a business should really think about it because life will be the same and then suddenly something will you know, then it will be a step change. Vivek, I have a few more questions but I'm sure the audience has a lot of questions for you. So, how are we doing on time? Okay. So, does, okay, a lot of questions, so love to, is there a mic that we can pass around? Thank you. My name is Karthik. I work for IT service industry. So you're saying that you're working on LLM, sorry, it's a fine-tuned LLM on top of LAMA. My basic question, fundamental question is we don't have a foundational model for India. Most of the models are basically using English or those kind of things. For example, Andrew was talking about the tokenizers and things like that. So are you working on anything like that or do you want to use mostly the existing models and run on top of it? You asked a good question. You asked a cherry question for himself. No, I think the interesting thing is that if you look at, and we have actually a blog on this on our website, I think one of the things that we've done is we've actually built a customized tokenizer, which actually fundamentally changes the cost of some of these generations in Indian languages. and I think that we are not just fine tuning, we are actually, we are leveraging the existing pre-training but we are doing what's known as continual pre-training which actually, but having said that, you know, I think that when we have to figure out where is the data to train an extremely large model from scratch and some of those things are things which will happen over time but I think that, I think that, yes, I think that we will be doing various kinds of things but the interesting thing is that if I want to change the accessibility problem with an existing open source model, how do I do that and that's the problem that we have that we think we have solved and it's going to be the heart of this OpenRTC series. It's extremely well explained in the blog even I could understand it so. Hi, I'm Prashant, I work for a Fintech company. My question is like unlike China we never had a consumer facing application coming out from India and in web 1, web 2, crypto and all. Why do you think it will be different this time in like AI because will the DPI and other things will serve the same purpose what the great fire wall did in China or do you think like in because AI is a strategic sector no outside country can work in NASA projects, maybe all government contract will go to them, what is the moat here for an Indian company? So I think the question is, I don't know the answer to these questions, right, I mean I think that it's difficult to predict but I do believe and as I'm repeating that the combinatorial effect of using Gen AI at a large scale in addition along with the DPI work that we've done in India will have people and I think that in the end it is the intent is that people need to be able to use it and they will vote by things that are useful for them and if that doesn't happen you're right and I think that we have to figure out what is the mechanism of delivery of apps right? Where do Indians consume contact. That's the question. I'm so sorry but we are out of time. Vivek will be outside so he would be able to answer the question. Do we have time for one last question? Can I just take one last? Yeah. Thank you. Thank you. I'm Manish Kothari. I'm from ISBR Business School. Good that I got a chance to ask you this question. During lunch time there were a few of our educationists whom we were talking about and there was one from school and we are from the MBA institutions. We were thinking of these present generations how do we get them into what you are doing. There is one thing that they have been regularly that the concentrations that they are working on but artificial intelligence and getting into this, getting them into their academics and making them a part of it is very important including the trainers who train them. making them future ready into what you are doing is amazing and the speed that which is growing it is calling for a lot of training that needs to be done can you from your angle throw some light on how we could make them future ready how these people who are who are management graduates and from schools who are coming out how do we get into this part of technology that you spoke about So this is really a challenge because I think everyone will need to understand at some level what this technology does and I think that we have to rethink how we get everyone into these and this kind of education has to be at many different levels, right? There are from a core set of having people who are extremely good at some and there you don't need as many but then there are basically vast numbers of people who can actually leverage By the way, the most important thing about, and maybe that's part of what makes an LLM interesting is that how you use it, your mileage varies by that. And to understand how to actually leverage this in an interesting way is something that we have to widely teach many, many people and because asking the things in the right way and having the right kind of applications will make a huge difference to how people can leverage these tools. Thank you. Thank you very much Vivek. Very good luck to Sarvam and good luck to India. I think it's going to be a lot of . Thanks Bala. Thank you Mr. Raghavan.\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transcribe(\"/content/Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.wav\", \"my_transcription.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDGY624u2yiT",
        "outputId": "287214eb-620b-40e6-e579-37e936208c6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcript formatted successfully.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Function to format the transcript\n",
        "def format_transcript(transcript):\n",
        "    # Ensure there is a space after every full stop\n",
        "    transcript = re.sub(r'\\.([A-Za-z])', r'. \\1', transcript)\n",
        "\n",
        "    # Replace specific words with the correct spelling\n",
        "    replacements = {\n",
        "        'open hathi': 'OpenHathi',\n",
        "        'kruthrim': 'Krutrim',\n",
        "        'open hearty': 'OpenHathi',\n",
        "        'Aadhar': 'Aadhaar',\n",
        "        'open AI': 'OpenAI',\n",
        "        'open happy': 'OpenHathi',\n",
        "        'sarom': 'Sarvam',\n",
        "        'Sarwam': 'Sarvam',\n",
        "        'Bhave Shakarwal': 'Bhavish Aggarwal',\n",
        "        'Lama': 'Llama',\n",
        "        'Bhavesh': 'Bhavish',\n",
        "        'Bhavesh Agarwal': 'Bhavish Aggarwal'\n",
        "    }\n",
        "\n",
        "    for old, new in replacements.items():\n",
        "        transcript = re.sub(re.escape(old), new, transcript, flags=re.IGNORECASE)\n",
        "\n",
        "    return transcript\n",
        "\n",
        "# Read the transcript from the file\n",
        "with open('my_transcription.txt', 'r') as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Format the transcript\n",
        "formatted_transcript = format_transcript(transcript)\n",
        "\n",
        "# Write the formatted transcript back to a file\n",
        "with open('formatted_transcript.txt', 'w') as file:\n",
        "    file.write(formatted_transcript)\n",
        "\n",
        "print(\"Transcript formatted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M12X1iPQ3GZA",
        "outputId": "65dd7852-531a-4a81-c786-1314c7290005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/MahmoudAshraf97/ctc-forced-aligner.git\n",
            "  Cloning https://github.com/MahmoudAshraf97/ctc-forced-aligner.git to /tmp/pip-req-build-trzicv56\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MahmoudAshraf97/ctc-forced-aligner.git /tmp/pip-req-build-trzicv56\n",
            "  Resolved https://github.com/MahmoudAshraf97/ctc-forced-aligner.git to commit 704fed8c8aecfa914b04d76be15aa2f5a0cf8103\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ctc-forced-aligner==0.2) (3.8.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ctc-forced-aligner==0.2) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from ctc-forced-aligner==0.2) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.34 in /usr/local/lib/python3.10/dist-packages (from ctc-forced-aligner==0.2) (4.42.0.dev0)\n",
            "Collecting Unidecode (from ctc-forced-aligner==0.2)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (4.66.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->ctc-forced-aligner==0.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->ctc-forced-aligner==0.2) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->ctc-forced-aligner==0.2) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ctc-forced-aligner==0.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34->ctc-forced-aligner==0.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34->ctc-forced-aligner==0.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34->ctc-forced-aligner==0.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34->ctc-forced-aligner==0.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ctc-forced-aligner==0.2) (1.3.0)\n",
            "Building wheels for collected packages: ctc-forced-aligner\n",
            "  Building wheel for ctc-forced-aligner (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctc-forced-aligner: filename=ctc_forced_aligner-0.2-cp310-cp310-linux_x86_64.whl size=1133732 sha256=e2b15f27afa209fe668ae0384b228bbdeb06359ac5d029316225c197a1094270\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ewixoeid/wheels/1c/98/3b/f4b23220ebc8d8d2357d6d743a779511a220572d2103ab9060\n",
            "Successfully built ctc-forced-aligner\n",
            "Installing collected packages: Unidecode, ctc-forced-aligner\n",
            "Successfully installed Unidecode-1.3.8 ctc-forced-aligner-0.2\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.43.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/MahmoudAshraf97/ctc-forced-aligner.git\n",
        "!pip install wheel torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZeBjttv4BmV",
        "outputId": "921d1e16-0397-4e88-bd28-b36a8fa5eee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-05-29 16:55:24.701732: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-29 16:55:24.701782: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-29 16:55:24.703630: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-29 16:55:25.850538: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "# Align on a sentence level\n",
        "!ctc-forced-aligner --audio_path \"/content/Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.wav\" --text_path \"/content/formatted_transcript.txt\" --language \"eng\" --split_size \"sentence\" --romanize --window_size \"15\" --device \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m7afkNvb4PW5",
        "outputId": "f96aed35-c736-4b82-d63f-efbe260155aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyannote.audio\n",
            "  Downloading pyannote.audio-3.2.0-py2.py3-none-any.whl (873 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m873.5/873.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n",
            "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
            "Collecting einops>=0.6.0 (from pyannote.audio)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.23.1)\n",
            "Collecting lightning>=2.0.1 (from pyannote.audio)\n",
            "  Downloading lightning-2.2.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<3.0,>=2.1 (from pyannote.audio)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.core>=5.0.0 (from pyannote.audio)\n",
            "  Downloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.database>=5.0.1 (from pyannote.audio)\n",
            "  Downloading pyannote.database-5.1.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.metrics>=3.2 (from pyannote.audio)\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n",
            "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
            "Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n",
            "  Downloading pytorch_metric_learning-2.5.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.1/119.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (13.7.1)\n",
            "Collecting semver>=3.0.0 (from pyannote.audio)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.12.1)\n",
            "Collecting speechbrain>=0.5.14 (from pyannote.audio)\n",
            "  Downloading speechbrain-1.0.0-py3-none-any.whl (760 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m760.1/760.1 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX>=2.6 (from pyannote.audio)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.3.0+cu121)\n",
            "Collecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n",
            "  Downloading torch_audiomentations-0.11.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.3.0+cu121)\n",
            "Collecting torchmetrics>=0.11.0 (from pyannote.audio)\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (1.25.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.66.4)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning>=2.0.1->pyannote.audio)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio)\n",
            "  Downloading pytorch_lightning-2.2.5-py3-none-any.whl (802 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m802.3/802.3 kB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf<3.0,>=2.1->pyannote.audio)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.11.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.0.3)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.12.3)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.2.2)\n",
            "Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.7.1)\n",
            "Requirement already satisfied: sympy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.12)\n",
            "Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio) (2.16.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.16.0)\n",
            "Collecting hyperpyyaml (from speechbrain>=0.5.14->pyannote.audio)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain>=0.5.14->pyannote.audio) (1.4.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain>=0.5.14->pyannote.audio) (0.1.99)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.6->pyannote.audio) (3.20.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pyannote.audio) (12.5.40)\n",
            "Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (0.10.2.post1)\n",
            "Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n",
            "  Downloading torch_pitch_shift-1.2.4-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (3.9.5)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (3.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.58.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (1.8.1)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning>=2.0.1->pyannote.audio) (67.7.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.8.2)\n",
            "Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.30)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.5.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio) (1.3.0)\n",
            "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n",
            "  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=0.5.14->pyannote.audio)\n",
            "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (4.0.3)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (4.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.16.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=0.5.14->pyannote.audio)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.0.3)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, docopt, julius\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=b856f840b71fe86c06419c6834bc004a0a265e1ef5c1d2505d6dca06435f0c4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=f548ce05ac1c475ea0c916eb9118c4d0289b1e951ee460b32bfaacb4a2d98cc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=defc758e29152e25ff01bf4905a122b8fbfada5de926b14c6f046b03a4662b72\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/b2/05/f883527ffcb7f2ead5438a2c23439aa0c881eaa9a4c80256f4\n",
            "Successfully built antlr4-python3-runtime docopt julius\n",
            "Installing collected packages: primePy, docopt, antlr4-python3-runtime, tensorboardX, semver, ruamel.yaml.clib, omegaconf, Mako, lightning-utilities, einops, colorlog, ruamel.yaml, pyannote.core, alembic, optuna, hyperpyyaml, torchmetrics, pytorch-metric-learning, pyannote.database, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.1 antlr4-python3-runtime-4.9.3 asteroid-filterbanks-0.4.0 colorlog-6.8.2 docopt-0.6.2 einops-0.8.0 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.2.5 lightning-utilities-0.11.2 omegaconf-2.3.0 optuna-3.6.1 primePy-1.3 pyannote.audio-3.2.0 pyannote.core-5.0.0 pyannote.database-5.1.0 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.2.5 pytorch-metric-learning-2.5.0 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 semver-3.0.2 speechbrain-1.0.0 tensorboardX-2.6.2.2 torch-audiomentations-0.11.1 torch-pitch-shift-1.2.4 torchmetrics-1.4.0.post0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "70da626faf7243aabaef82f4e6dcd83d",
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install pyannote.audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "c0b39e10035449fe8b5209f11603a14e",
            "e8237d6473584c079a392589708f9dae",
            "ecaefa7978ca49189f01ca0b49367746",
            "6decc45abd4a4a0db3323dc1d7dddd08",
            "58d8dae90d16418ab362d0a3ab2d47e4",
            "f09875eb3ae54b06bb05c39a169fa8a1",
            "da792b4d9ff742bf8a13dd078f368daa",
            "b8dbe72f63dd490ab48d72e439d1b502",
            "4f7cd9541d9343a2a7cb05fd685ba5f6",
            "ed667cf91a2444f5946de470ed9d5adb",
            "65711156fc334393b147f46cb068bab0",
            "e3021f5703154044bd05843fd0f1e6a6",
            "b939b94ae190443b80a2b053f4751545",
            "f4c002f6418d4200b7afa94b54497406",
            "ac7a1907bf2847c3a21aaf535428ef39",
            "7d8ed61c028c4df0a40a46d7d696043c",
            "1c8cabee92294e168e241810200c45bb",
            "7302f9148882415f874f87bd2433da7c",
            "0d4a8bfb80324bba880c526553b965d4",
            "cb7c2e49cc494bfead9548a69a39cb80",
            "21ece2c7669f4ee2bb12c0131747ee9f",
            "d3c4fc6445ea48a0876826c698f463bc"
          ]
        },
        "id": "GscMdJjk4SWN",
        "outputId": "55a3b603-5e2d-4539-d9d5-6a9910b8d5fc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0b39e10035449fe8b5209f11603a14e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3021f5703154044bd05843fd0f1e6a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyannote.audio import Model, Inference\n",
        "\n",
        "model = Model.from_pretrained(\"pyannote/segmentation-3.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPW72PeY4cUZ",
        "outputId": "1b06478a-cbe9-492b-fa87-b62295ead1b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 00:00:00.030 -->  00:00:04.350] 0 SPEECH\n",
            "[ 00:00:08.687 -->  00:00:10.223] 0 SPEECH\n",
            "[ 00:00:11.877 -->  00:00:17.091] 0 SPEECH\n",
            "[ 00:00:18.222 -->  00:00:18.930] 0 SPEECH\n",
            "[ 00:00:19.724 -->  00:00:20.736] 0 SPEECH\n",
            "[ 00:00:21.900 -->  00:00:22.474] 0 SPEECH\n",
            "[ 00:00:22.964 -->  00:00:24.685] 0 SPEECH\n",
            "[ 00:00:25.157 -->  00:00:55.482] 0 SPEECH\n",
            "[ 00:00:56.174 -->  00:00:59.937] 0 SPEECH\n",
            "[ 00:01:00.291 -->  00:01:19.647] 0 SPEECH\n",
            "[ 00:01:19.917 -->  00:01:42.023] 0 SPEECH\n",
            "[ 00:01:42.242 -->  00:01:47.085] 0 SPEECH\n",
            "[ 00:01:47.862 -->  00:02:20.211] 0 SPEECH\n",
            "[ 00:02:20.312 -->  00:02:22.270] 0 SPEECH\n",
            "[ 00:02:22.624 -->  00:02:31.669] 0 SPEECH\n",
            "[ 00:02:32.597 -->  00:03:45.717] 0 SPEECH\n",
            "[ 00:03:46.459 -->  00:04:25.052] 0 SPEECH\n",
            "[ 00:04:25.862 -->  00:04:44.594] 0 SPEECH\n",
            "[ 00:04:45.032 -->  00:04:46.416] 0 SPEECH\n",
            "[ 00:04:48.914 -->  00:05:30.274] 0 SPEECH\n",
            "[ 00:05:30.814 -->  00:05:31.709] 0 SPEECH\n",
            "[ 00:05:32.249 -->  00:05:48.769] 0 SPEECH\n",
            "[ 00:05:49.292 -->  00:08:41.991] 0 SPEECH\n",
            "[ 00:08:42.210 -->  00:08:42.227] 0 SPEECH\n",
            "[ 00:08:42.312 -->  00:08:48.420] 0 SPEECH\n",
            "[ 00:08:50.260 -->  00:10:47.153] 0 SPEECH\n",
            "[ 00:10:48.030 -->  00:11:56.914] 0 SPEECH\n",
            "[ 00:11:57.572 -->  00:12:21.164] 0 SPEECH\n",
            "[ 00:12:21.349 -->  00:12:44.974] 0 SPEECH\n",
            "[ 00:12:45.345 -->  00:12:54.087] 0 SPEECH\n",
            "[ 00:12:54.509 -->  00:13:30.604] 0 SPEECH\n",
            "[ 00:13:31.684 -->  00:14:04.557] 0 SPEECH\n",
            "[ 00:14:05.046 -->  00:14:38.340] 0 SPEECH\n",
            "[ 00:14:39.589 -->  00:14:43.842] 0 SPEECH\n",
            "[ 00:14:44.584 -->  00:14:52.397] 0 SPEECH\n",
            "[ 00:14:52.718 -->  00:14:55.705] 0 SPEECH\n",
            "[ 00:14:55.941 -->  00:15:44.355] 0 SPEECH\n",
            "[ 00:15:45.132 -->  00:17:08.241] 0 SPEECH\n",
            "[ 00:17:08.325 -->  00:18:30.507] 0 SPEECH\n",
            "[ 00:18:31.080 -->  00:18:38.843] 0 SPEECH\n",
            "[ 00:18:39.062 -->  00:18:39.231] 0 SPEECH\n",
            "[ 00:18:39.400 -->  00:19:24.405] 0 SPEECH\n",
            "[ 00:19:24.574 -->  00:20:10.710] 0 SPEECH\n",
            "[ 00:20:10.862 -->  00:20:27.180] 0 SPEECH\n",
            "[ 00:20:28.244 -->  00:20:34.403] 0 SPEECH\n",
            "[ 00:20:34.639 -->  00:20:35.078] 0 SPEECH\n",
            "[ 00:20:40.579 -->  00:22:23.550] 0 SPEECH\n",
            "[ 00:22:25.272 -->  00:22:38.873] 0 SPEECH\n",
            "[ 00:22:39.396 -->  00:22:42.096] 0 SPEECH\n",
            "[ 00:22:42.687 -->  00:22:43.092] 0 SPEECH\n",
            "[ 00:22:43.632 -->  00:23:08.556] 0 SPEECH\n",
            "[ 00:23:11.189 -->  00:23:56.835] 0 SPEECH\n",
            "[ 00:23:57.359 -->  00:24:00.767] 0 SPEECH\n",
            "[ 00:24:00.919 -->  00:24:06.454] 0 SPEECH\n",
            "[ 00:24:07.129 -->  00:24:24.747] 0 SPEECH\n",
            "[ 00:24:25.185 -->  00:24:26.940] 0 SPEECH\n",
            "[ 00:24:27.514 -->  00:25:03.576] 0 SPEECH\n",
            "[ 00:25:04.842 -->  00:26:11.582] 0 SPEECH\n",
            "[ 00:26:13.624 -->  00:26:15.075] 0 SPEECH\n",
            "VAD results saved to vad.txt\n"
          ]
        }
      ],
      "source": [
        "from pyannote.audio.pipelines import VoiceActivityDetection\n",
        "pipeline = VoiceActivityDetection(segmentation=model)\n",
        "HYPER_PARAMETERS = {\n",
        "  # remove speech regions shorter than that many seconds.\n",
        "  \"min_duration_on\": 0.0,\n",
        "  # fill non-speech regions shorter than that many seconds.\n",
        "  \"min_duration_off\": 0.0\n",
        "}\n",
        "pipeline.instantiate(HYPER_PARAMETERS)\n",
        "vad = pipeline(\"/content/Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.wav\")\n",
        "# `vad` is a pyannote.core.Annotation instance containing speech regions\n",
        "print(vad)\n",
        "# Save VAD results to file\n",
        "with open(\"vad.txt\", 'w') as f:\n",
        "    for segment, _, label in vad.itertracks(yield_label=True):\n",
        "        if label == 'SPEECH':\n",
        "            f.write(f\"[{segment.start:.2f} --> {segment.end:.2f}] SPEECH\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"VAD results saved to vad.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtYTSeis4m7f",
        "outputId": "6aa0def5-9b61-4caf-bbde-ad2de0bdd691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAD segments: [{'start': '0.03', 'end': '4.35'}, {'start': '8.69', 'end': '10.22'}, {'start': '11.88', 'end': '17.09'}, {'start': '18.22', 'end': '18.93'}, {'start': '19.72', 'end': '20.74'}, {'start': '21.90', 'end': '22.47'}, {'start': '22.96', 'end': '24.69'}, {'start': '25.16', 'end': '55.48'}, {'start': '56.17', 'end': '59.94'}, {'start': '60.29', 'end': '79.65'}, {'start': '79.92', 'end': '102.02'}, {'start': '102.24', 'end': '107.09'}, {'start': '107.86', 'end': '140.21'}, {'start': '140.31', 'end': '142.27'}, {'start': '142.62', 'end': '151.67'}, {'start': '152.60', 'end': '225.72'}, {'start': '226.46', 'end': '265.05'}, {'start': '265.86', 'end': '284.59'}, {'start': '285.03', 'end': '286.42'}, {'start': '288.91', 'end': '330.27'}, {'start': '330.81', 'end': '331.71'}, {'start': '332.25', 'end': '348.77'}, {'start': '349.29', 'end': '521.99'}, {'start': '522.21', 'end': '522.23'}, {'start': '522.31', 'end': '528.42'}, {'start': '530.26', 'end': '647.15'}, {'start': '648.03', 'end': '716.91'}, {'start': '717.57', 'end': '741.16'}, {'start': '741.35', 'end': '764.97'}, {'start': '765.35', 'end': '774.09'}, {'start': '774.51', 'end': '810.60'}, {'start': '811.68', 'end': '844.56'}, {'start': '845.05', 'end': '878.34'}, {'start': '879.59', 'end': '883.84'}, {'start': '884.58', 'end': '892.40'}, {'start': '892.72', 'end': '895.71'}, {'start': '895.94', 'end': '944.36'}, {'start': '945.13', 'end': '1028.24'}, {'start': '1028.33', 'end': '1110.51'}, {'start': '1111.08', 'end': '1118.84'}, {'start': '1119.06', 'end': '1119.23'}, {'start': '1119.40', 'end': '1164.41'}, {'start': '1164.57', 'end': '1210.71'}, {'start': '1210.86', 'end': '1227.18'}, {'start': '1228.24', 'end': '1234.40'}, {'start': '1234.64', 'end': '1235.08'}, {'start': '1240.58', 'end': '1343.55'}, {'start': '1345.27', 'end': '1358.87'}, {'start': '1359.40', 'end': '1362.10'}, {'start': '1362.69', 'end': '1363.09'}, {'start': '1363.63', 'end': '1388.56'}, {'start': '1391.19', 'end': '1436.84'}, {'start': '1437.36', 'end': '1440.77'}, {'start': '1440.92', 'end': '1446.45'}, {'start': '1447.13', 'end': '1464.75'}, {'start': '1465.19', 'end': '1466.94'}, {'start': '1467.51', 'end': '1503.58'}, {'start': '1504.84', 'end': '1571.58'}, {'start': '1573.62', 'end': '1575.08'}]\n",
            "Text segments: [{'start': '0.06', 'end': '1.28', 'text': 'Congratulations to you Mr.'}, {'start': '1.28', 'end': '2.18', 'text': 'Raghavan for that.'}, {'start': '2.18', 'end': '3.6', 'text': 'Thank you so much for joining us.'}, {'start': '3.6', 'end': '6.42', 'text': 'Over to you.'}, {'start': '6.42', 'end': '9.48', 'text': 'Hi everybody.'}, {'start': '9.48', 'end': '11.16', 'text': 'How are you?'}, {'start': '11.16', 'end': '13.76', 'text': 'I am not hearing this at all.'}, {'start': '13.76', 'end': '17.6', 'text': \"It's like a post lunch energy downer or something.\"}, {'start': '17.6', 'end': '19.22', 'text': \"Let's hear it.\"}, {'start': '19.22', 'end': '21.26', 'text': 'Are you guys awake?'}, {'start': '21.26', 'end': '22.68', 'text': 'Alright.'}, {'start': '22.68', 'end': '27.66', 'text': 'You better be because we have a superstar guest here.'}, {'start': '27.66', 'end': '32.38', 'text': \"You heard the $41 million and I didn't hear honestly anything she said after that.\"}, {'start': '32.38', 'end': '39.06', 'text': \"So we're going to ask for about $40 million from him by the end of this conversation, okay?\"}, {'start': '39.06', 'end': '41.24', 'text': \"But let's get started.\"}, {'start': '41.24', 'end': '45.36', 'text': \"I want to introduce Vivek and Pratyush, his co-founder who's not here.\"}, {'start': '45.36', 'end': '51.0', 'text': 'We wanted to start with playing a video of what OpenHearty does.'}, {'start': '51.0', 'end': '54.16', 'text': 'I encourage all of you to go to the website serverum.'}, {'start': '54.16', 'end': '56.06', 'text': 'ai and check it out.'}, {'start': '56.06', 'end': '88.3', 'text': \"but let me start by introducing Vivek Vivek is a dear friend and he is very very modest one of the most modest guys that I know but his personal journey Vivek you got a PhD from Carnegie Mellon, you started and sold a company to Magma and Vivek and I moved back to India we were both in the valley on the same day actually and you have been in India for the last 16 years and what most people don't know is your journey at Aadhaar He spent 13 years selflessly at Aadhaar.\"}, {'start': '88.3', 'end': '98.0', 'text': 'Nobody would have heard of him, but he was a pioneering technology visionary behind Aadhaar, which we all take for granted today.'}, {'start': '98.0', 'end': '101.26', 'text': 'So please give it out.'}, {'start': '101.26', 'end': '107.46', 'text': 'Honestly, when I think of selfless service, truly selfless service, I always think of Vivek.'}, {'start': '107.46', 'end': '115.0', 'text': \"And since then, he also was at AI for Bharat, which we're going to touch on, where he met Pratyush's other co-founder.\"}, {'start': '115.0', 'end': '118.2', 'text': 'Pratyush had a PhD from ETH at Zurich.'}, {'start': '118.2', 'end': '120.4', 'text': 'He was in the IBM research.'}, {'start': '120.4', 'end': '126.3', 'text': 'He was at Microsoft research playing a key role and a faculty at IIT Madras and at AF Abharat.'}, {'start': '126.3', 'end': '128.7', 'text': \"So that's a little brief introduction about them.\"}, {'start': '128.7', 'end': '131.42', 'text': 'These guys are modest, modest engineers.'}, {'start': '131.42', 'end': '133.64', 'text': \"So they don't toot their own horn.\"}, {'start': '133.64', 'end': '137.32', 'text': 'So forgive me for tooting their horn in this case.'}, {'start': '137.32', 'end': '142.48', 'text': \"But let's jump right in about the money, funding.\"}, {'start': '142.48', 'end': '145.74', 'text': \"41 million bucks, man, that's a lot of money, right?\"}, {'start': '145.74', 'end': '148.76', 'text': 'Every entrepreneur here is saying, what the hell did these guys do?'}, {'start': '148.76', 'end': '152.08', 'text': 'What did the investors see to write such a big check?'}, {'start': '152.08', 'end': '158.14', 'text': \"No, I think it's a new trend of what's going on in India.\"}, {'start': '158.14', 'end': '170.9', 'text': \"I think that for the very first time, I think the investors have looked at, you know, let's try and build something deep tech out of the country and let's try to figure out how to build something as a foundational technology out of the country.\"}, {'start': '170.9', 'end': '173.4', 'text': \"And that's really what's really exciting.\"}, {'start': '173.4', 'end': '190.68', 'text': \"And I think that about, as Bala was mentioning, for the past 15 years, I've been kind of working in both digital public infrastructure and kind of non-profit kind of things.\"}, {'start': '190.68', 'end': '199.16', 'text': 'But when this whole thing of generative AI came about, we said, okay, how can I actually make a difference in this space?'}, {'start': '199.16', 'end': '205.9', 'text': 'And I said, maybe this is the opportunity to actually come out and really build something.'}, {'start': '205.9', 'end': '212.7', 'text': 'And the only way that we realized that you can do it is actually in the private sector.'}, {'start': '212.7', 'end': '218.06', 'text': 'And then we went out there and we said we want to build something which is a continuation.'}, {'start': '218.06', 'end': '222.72', 'text': 'And fundamentally the question is, the reason of what we want to do at Servam.'}, {'start': '222.72', 'end': '229.78', 'text': 'ai is we want to basically make generative AI available and accessible to the people in the country.'}, {'start': '229.78', 'end': '231.54', 'text': \"And that's the intent.\"}, {'start': '231.54', 'end': '236.4', 'text': 'And when we said that we want to do this, there was a resonance in the investment community.'}, {'start': '236.4', 'end': '242.92', 'text': \"And I think it's a responsibility to really to show that something like this can be built out of India.\"}, {'start': '242.92', 'end': '246.76', 'text': 'So we see that as confidence and a responsibility.'}, {'start': '246.76', 'end': '265.3', 'text': \"And I also hope it's a trend that there are many more people like us who are backed because if you look at it, maybe it's a large number in a, you know, in the Indian context but in the global context, I think there is just, there should be many, many more entrepreneurs who are backed to do things in India.\"}, {'start': '265.3', 'end': '267.58', 'text': \"I'm going to come back to the many more entrepreneurs.\"}, {'start': '267.58', 'end': '271.12', 'text': \"I'm obviously going to ask you about Bhavish's Krutrim.\"}, {'start': '271.12', 'end': '287.76', 'text': \"So we're going to come back to that question but again, 41 million dollars, I mean all of what you said, you know, 2 million dollars, you know, that's a good amount of money for of startup which, you know, which has not yet built anything, what are you going to do with all this money?\"}, {'start': '287.76', 'end': '290.16', 'text': 'I can solve the problem.'}, {'start': '290.16', 'end': '292.14', 'text': 'I can have a perfect solution for the problem.'}, {'start': '292.14', 'end': '298.36', 'text': \"I think in the last week I've got lots of calls from lots of people telling me how I can do it.\"}, {'start': '298.36', 'end': '299.32', 'text': 'I know you first, okay?'}, {'start': '299.32', 'end': '300.94', 'text': \"I'll be landed in the country the same day.\"}, {'start': '300.94', 'end': '303.98', 'text': \"I'm in front of the queue.\"}, {'start': '303.98', 'end': '309.52', 'text': 'But honestly, I think the key thing in this is to putting together an amazing team.'}, {'start': '309.52', 'end': '319.7', 'text': 'and we actually have an amazing team but we believe that it is talent that will drive this kind of thing and so it is to get key talent and of course the other thing is compute.'}, {'start': '319.7', 'end': '330.9', 'text': 'This is extremely expensive compute wise to actually do these kinds of things and I think that those are the two primary things that we would use this for.'}, {'start': '330.92', 'end': '331.02', 'text': 'Okay.'}, {'start': '331.04', 'end': '334.92', 'text': \"I'm computing in my own head as an entrepreneur.\"}, {'start': '334.92', 'end': '337.44', 'text': 'Talent, okay, you have like 20, 15 people.'}, {'start': '337.44', 'end': '338.88', 'text': 'How much are you paying these guys?'}, {'start': '338.88', 'end': '341.1', 'text': \"but okay, we won't touch on that.\"}, {'start': '341.1', 'end': '343.9', 'text': \"But let's talk about what you guys actually built.\"}, {'start': '343.9', 'end': '345.1', 'text': 'What is OpenHathi?'}, {'start': '345.1', 'end': '349.02', 'text': 'How would you explain OpenHathi to many people here who might not have known about it?'}, {'start': '349.02', 'end': '360.2', 'text': 'So I think OpenHathi is, so first of all, right, we come from, I personally come from the open source ecosystem and also from the DPI ecosystem.'}, {'start': '360.2', 'end': '365.3', 'text': 'So we believe that for this to work, we need the ecosystem to be successful.'}, {'start': '365.3', 'end': '372.72', 'text': 'And as a result of that, one of the first things we did was hey, there are these open source large language models that exist, right?'}, {'start': '372.72', 'end': '376.5', 'text': 'I mean, everybody knows about the Llama family from Meta.'}, {'start': '376.5', 'end': '378.52', 'text': 'There are others like Mistral.'}, {'start': '378.52', 'end': '383.38', 'text': 'There are a bunch of open source large language models.'}, {'start': '383.38', 'end': '391.1', 'text': 'And then we said, is there any way that take an existing open source model and teach it language skills, right?'}, {'start': '391.1', 'end': '397.28', 'text': 'And that is really what we said, that can we do something like that?'}, {'start': '397.28', 'end': '407.48', 'text': 'And is this a, you know, relatively frugal way of actually, you know, making models, you know, work in diverse languages?'}, {'start': '407.48', 'end': '415.32', 'text': 'Because the truth is still today, I mean, if you look at the amount of data and knowledge, it is still English dominates these things.'}, {'start': '415.32', 'end': '425.04', 'text': 'And I think that how do you actually take and make it understand Indian language, understand Indian context, and all of those things in actually an efficient way.'}, {'start': '425.04', 'end': '427.5', 'text': 'And therefore, this was an attempt to do that.'}, {'start': '427.5', 'end': '442.28', 'text': \"And OpenHathi is currently based on the Llama 7 billion model, but we'll be releasing many more models in different languages, different sizes, and things like that as part of this series.\"}, {'start': '442.28', 'end': '450.02', 'text': \"And of course, we will be building further models on those and doing other things to actually... And we'll also have endpoints that people can use.\"}, {'start': '450.02', 'end': '455.34', 'text': \"So therefore, it's definitely something that people can use to things.\"}, {'start': '455.34', 'end': '460.16', 'text': \"And that's the essence of what this OpenHathi is.\"}, {'start': '460.16', 'end': '467.32', 'text': 'So what does it mean to people in the audience here who are either doing their own startups or a business or developers?'}, {'start': '467.32', 'end': '469.82', 'text': 'How should they look at OpenAI?'}, {'start': '469.82', 'end': '470.74', 'text': 'Sorry, Sarman.'}, {'start': '470.74', 'end': '473.08', 'text': 'Not OpenAI.'}, {'start': '473.08', 'end': '480.92', 'text': \"No, I think the way you look at it is that one of the important things that we are doing is we're not just building models.\"}, {'start': '480.92', 'end': '506.5', 'text': 'We are also going to be building a platform, a platform for developers where you can actually use a combination of various different kinds of models, some which are from us, some which are open source, some which may not be open source, and actually to actually pull together and figure out how to deploy, you know, generative AI applications at scale and understand and evaluate their performance in an efficient manner.'}, {'start': '506.5', 'end': '508.98', 'text': \"And that's something that we're planning to do.\"}, {'start': '508.98', 'end': '513.9', 'text': 'And this platform is, you know, in the next couple of months will be coming out there.'}, {'start': '513.9', 'end': '515.54', 'text': 'It will be available to developers.'}, {'start': '515.54', 'end': '522.22', 'text': 'But, of course, those who want to start with the open source things and hack for that, of course, please go ahead and do that as well.'}, {'start': '522.22', 'end': '524.32', 'text': \"That's phenomenal.\"}, {'start': '524.32', 'end': '529.32', 'text': 'But how does it compare to OpenAI itself or Google?'}, {'start': '529.32', 'end': '533.2', 'text': 'See, at least the things that we are doing now, right?'}, {'start': '533.2', 'end': '541.3', 'text': 'I mean, one of the things that when we thought about building Sarvam, we said we want to build a full stack generative AI company.'}, {'start': '541.3', 'end': '548.36', 'text': 'And different people are understanding of a stack is that we need to know how to train models from scratch.'}, {'start': '548.36', 'end': '554.16', 'text': 'We need to know how to kind of figure out how to deploy models to solve real world use cases.'}, {'start': '554.16', 'end': '562.5', 'text': 'And we need to play in the ecosystem to make sure that we can actually deploy population scale applications.'}, {'start': '562.5', 'end': '565.04', 'text': 'So we were thinking about all of these things.'}, {'start': '565.04', 'end': '569.42', 'text': 'But still the models we were talking about are fairly small models.'}, {'start': '569.42', 'end': '570.72', 'text': 'They are fairly small models, right?'}, {'start': '570.72', 'end': '574.98', 'text': \"The 7 to maybe up to 70 billion kind of range we're talking about.\"}, {'start': '574.98', 'end': '580.08', 'text': 'While these models like OpenAI and Google are obviously much bigger models.'}, {'start': '580.08', 'end': '589.94', 'text': 'But we want to understand the techniques and be able to build that muscle to do all of these things to make it available to people.'}, {'start': '589.94', 'end': '596.32', 'text': 'Now, those models are, I mean, as I said, you know, I think that there is space for all of those things.'}, {'start': '596.32', 'end': '612.54', 'text': 'And I think as even Sridhar was talking about earlier in the day, we believe that these smaller models can do very, I mean, many, many kind of domain specific tasks extremely well, probably even better than the larger models.'}, {'start': '612.54', 'end': '614.84', 'text': 'And that is really one of the key areas.'}, {'start': '614.84', 'end': '618.0', 'text': 'And so, therefore, the value of these kinds of things, right?'}, {'start': '618.0', 'end': '622.28', 'text': 'We are not aiming in these set of models to build any AGI, right?'}, {'start': '622.28', 'end': '623.62', 'text': \"That's not our goal here.\"}, {'start': '623.62', 'end': '633.22', 'text': 'Our goal is to make things that work extremely well for domain-specific use cases or increase accessibility through language and all of those kinds of things.'}, {'start': '633.22', 'end': '635.04', 'text': 'And obviously all of this is unique to India.'}, {'start': '635.04', 'end': '636.62', 'text': 'But what is unique about India?'}, {'start': '636.62', 'end': '647.54', 'text': 'I mean, is there anything special in our ecosystem that makes small models focused with Indian languages better for, more suited for our problems?'}, {'start': '647.54', 'end': '653.62', 'text': 'So I think that, I mean, there are quite a few things that are unique about India, right?'}, {'start': '653.62', 'end': '661.96', 'text': 'The first thing is, I think, that we are a voice-first nation, so therefore I think voice has to be the core to doing things.'}, {'start': '661.96', 'end': '669.7', 'text': \"The other thing, of course, India is extremely, it's a cost-conscious country from a cost perspective.\"}, {'start': '669.7', 'end': '678.84', 'text': 'Now, I would say that there are lots of interesting use cases where you can use OpenAI and the cost structure works depending on your application.'}, {'start': '678.84', 'end': '685.14', 'text': 'But when you want to scale things to a massive level and make it work, then you have to figure out how small models work.'}, {'start': '685.14', 'end': '688.44', 'text': \"So that's something that is also specific to India.\"}, {'start': '688.44', 'end': '695.94', 'text': 'The third thing which is specific to India is really the success that India has had in building all this digital public infrastructure.'}, {'start': '695.94', 'end': '707.74', 'text': 'When you add the AI layer on top of it, then you can actually get dramatic, you know dramatic I think multiplicative combinatorial effects based on doing things like that.'}, {'start': '707.74', 'end': '716.72', 'text': \"That's a phenomenal point like you know it's like DPI to the power of AI almost in some ways and as a part of Aadhaar building Aadhaar no better person than you.\"}, {'start': '716.72', 'end': '728.88', 'text': \"So in summary what I'm hearing is small models specialized with trained with Indic specific language data suited for Indian problems at a compelling cost point will be suited for us.\"}, {'start': '728.88', 'end': '732.72', 'text': \"We're not solving some world autonomous vehicles or some complex problem.\"}, {'start': '732.72', 'end': '737.9', 'text': \"We're solving some basic problems specifically focused on voice with multiple languages.\"}, {'start': '737.9', 'end': '739.42', 'text': 'That is what you see as the future.'}, {'start': '739.44', 'end': '741.58', 'text': 'Am I paraphrasing this correctly?'}, {'start': '741.58', 'end': '753.02', 'text': 'I think that certainly voice and Indian languages are an important part of our strategy but we will be building custom models to solve various other kinds of problems as well.'}, {'start': '753.02', 'end': '763.46', 'text': \"It's not just limited to I think in different domains, working in different domains making, building things based on unique data that enterprises have and things like that.\"}, {'start': '763.46', 'end': '765.16', 'text': \"So that's something that we'll also look at.\"}, {'start': '765.16', 'end': '766.0', 'text': 'Fair enough.'}, {'start': '766.0', 'end': '774.2', 'text': 'So coming back to the elephant in the room, no fun intended with OpenHathi, what about Bhave Shagriwal and Krutrim?'}, {'start': '774.2', 'end': '775.42', 'text': 'What is your take on that?'}, {'start': '775.42', 'end': '776.44', 'text': \"I think it's great.\"}, {'start': '776.44', 'end': '778.2', 'text': \"I think it's wonderful, right?\"}, {'start': '778.2', 'end': '785.56', 'text': 'I mean, the fact that the technology AI is so important that we need multiple people working on it.'}, {'start': '785.56', 'end': '791.94', 'text': 'The fact that there are other people thinking actually validates that this is an important problem to be solved.'}, {'start': '791.94', 'end': '797.52', 'text': 'And I think that we need everybody to come together and do that.'}, {'start': '797.52', 'end': '799.0', 'text': 'So I really welcome that.'}, {'start': '799.0', 'end': '800.18', 'text': \"I think it's great.\"}, {'start': '800.18', 'end': '806.52', 'text': 'And I think that there will be different people who will have different takes as to how to solve this kind of problem.'}, {'start': '806.52', 'end': '811.2', 'text': 'And hopefully as a result of that, the entire ecosystem benefits.'}, {'start': '811.2', 'end': '816.24', 'text': \"One more question and then I want to talk about some of the predictions that you've boldly made.\"}, {'start': '816.24', 'end': '820.84', 'text': 'So Vivek, I usually ask people about what do you think the future will be and everybody usually hedges.'}, {'start': '820.84', 'end': '824.42', 'text': 'I asked Vivek, what do you think is going to happen by December 2024?'}, {'start': '824.42', 'end': '828.08', 'text': 'What do you think sitting in this room one year later we can expect?'}, {'start': '828.08', 'end': '830.1', 'text': 'And he made three bold predictions.'}, {'start': '830.1', 'end': '831.58', 'text': 'So I want to talk about that.'}, {'start': '831.58', 'end': '833.38', 'text': 'Before that, I have one last question.'}, {'start': '833.38', 'end': '837.78', 'text': 'What are the top three applications that you think are relevant for India?'}, {'start': '837.78', 'end': '839.7', 'text': 'You heard Sridhar talk about medical.'}, {'start': '839.7', 'end': '841.32', 'text': 'Any quick summary?'}, {'start': '841.32', 'end': '844.74', 'text': 'what do you think the top three apps are for India for AI?'}, {'start': '844.74', 'end': '855.18', 'text': 'So, I mean, I think that, as you said, things like education and medical are clearly areas where I think that things can be leveraged.'}, {'start': '855.18', 'end': '862.4', 'text': 'The whole idea of all these kind of... The DPI aspect of it is another major application where things can happen.'}, {'start': '862.4', 'end': '864.76', 'text': \"And here I'm talking about country-specific work.\"}, {'start': '864.76', 'end': '869.98', 'text': 'And I think the whole idea which Sridhar also talked about was the concept of software, right?'}, {'start': '869.98', 'end': '878.94', 'text': \"and I think that and clearly we have a very large software industry and how to reimagine those things in this context is also something that's going to be.\"}, {'start': '878.94', 'end': '880.52', 'text': 'Fair enough.'}, {'start': '880.52', 'end': '884.2', 'text': \"Are you guys ready for Vivek Raghavan's bold predictions?\"}, {'start': '884.2', 'end': '885.16', 'text': 'Yes?'}, {'start': '885.16', 'end': '885.64', 'text': 'No?'}, {'start': '885.64', 'end': '886.56', 'text': \"I'm not hearing any yes.\"}, {'start': '886.56', 'end': '887.88', 'text': 'This is like a big deal.'}, {'start': '887.88', 'end': '889.92', 'text': \"He's like one of the smartest guys that I know.\"}, {'start': '889.92', 'end': '891.3', 'text': 'He wants to make three predictions.'}, {'start': '891.32', 'end': '892.64', 'text': \"You don't want to hear it?\"}, {'start': '892.64', 'end': '893.98', 'text': 'All right.'}, {'start': '893.98', 'end': '899.42', 'text': 'So I asked him what do you think a year later what do you think we can expect?'}, {'start': '899.42', 'end': '905.92', 'text': \"and he came up with three things and usually people give very blah answers when you ask a question like this because they don't want to be caught wrong.\"}, {'start': '905.92', 'end': '906.8', 'text': 'Not Vivek.'}, {'start': '906.8', 'end': '908.08', 'text': 'Vivek is bold.'}, {'start': '908.08', 'end': '913.34', 'text': \"So he basically said three things and I'm going to list out the three things and then ask him about it.\"}, {'start': '913.34', 'end': '921.46', 'text': \"So number one, he says, I would prefer to talk to an automated customer service than a real person because they'll give me a better answer.\"}, {'start': '921.46', 'end': '924.12', 'text': \"So that is Vivek Raghavan's prediction number one.\"}, {'start': '924.12', 'end': '932.6', 'text': 'So number two is that when everybody is talking about a GPU shortage, Vivek predicts that there will be a GPU glut in India.'}, {'start': '932.6', 'end': '934.38', 'text': 'He thinks there will be too much GPU.'}, {'start': '934.38', 'end': '938.34', 'text': 'So if you want a short NVIDIA stock, this is a good time.'}, {'start': '938.34', 'end': '945.42', 'text': 'And number three, which was extremely unexpected, he said some companies will suddenly die.'}, {'start': '945.42', 'end': '948.76', 'text': 'So Vivek, these are not what I expected.'}, {'start': '948.76', 'end': '956.74', 'text': \"So do you want to quickly talk about each of them, why you just came up with these, and then we'll throw the open for audience questions.\"}, {'start': '956.74', 'end': '962.98', 'text': \"So I don't think I quite said it the way that Pala was going to say it.\"}, {'start': '962.98', 'end': '963.86', 'text': \"But it's interesting.\"}, {'start': '963.86', 'end': '980.58', 'text': \"But I think the first thing that we said is I think that, and I don't think that this is, I think there will come a time when, you know, in areas of customer service, et cetera, when you want to do something very specific.\"}, {'start': '980.58', 'end': '991.08', 'text': \"Today, you know, when you call some kind of a bot, you actually end up, you mostly try to disconnect the call or you're extremely upset that you're talking to a bot.\"}, {'start': '991.08', 'end': '1005.78', 'text': \"But I think that there will come a time, and I'm predicting it is sooner than later, that you will actually get better responses from the bot than what the human representative, at least the average human representative that you could talk to could give.\"}, {'start': '1005.78', 'end': '1017.9', 'text': \"And I think that that's just a, so I just said that there will come a time where you know it's not a human you're talking to, but it's probably more likely to solve your intent than the human person.\"}, {'start': '1017.9', 'end': '1023.24', 'text': \"That's just something that I think that could happen.\"}, {'start': '1023.24', 'end': '1026.26', 'text': \"Okay, definitely controversial, but we'll let it go.\"}, {'start': '1026.26', 'end': '1027.94', 'text': 'What about the GPU glut?'}, {'start': '1027.94', 'end': '1039.82', 'text': \"No, no, yeah, so I don't think that, so I think that the fact that there is a tremendous shortage right now, I think that shortage will ease because that is how the cycles of things go, right?\"}, {'start': '1039.82', 'end': '1051.24', 'text': 'when I think the fact that there was such a severe shortage last year basically caused a number of different players to ramp up in various kinds of forms.'}, {'start': '1051.24', 'end': '1054.5', 'text': 'And I think that that will always go in a cycle.'}, {'start': '1054.5', 'end': '1060.6', 'text': 'But we may find out that there are many, many more interesting problems that people will be able to solve.'}, {'start': '1060.6', 'end': '1081.58', 'text': \"I still remember we were at a Gen AI event in Bangalore and we were talking to people and we said, you know, how many people have access to, you know, four A100s, this was the question that I'd asked, and nobody in the room, and these are all extremely enthusiastic Gen AI people, and nobody had access.\"}, {'start': '1081.58', 'end': '1083.44', 'text': 'And I think that thing is going to change.'}, {'start': '1083.44', 'end': '1094.98', 'text': 'You will be able to get these kinds of things and people who want to hack and do things will have access to these things without, you know, having to write a, you know, a major check.'}, {'start': '1094.98', 'end': '1100.34', 'text': 'Vivek is also a semiconductor guy before he went into Aadhaar, So I would take his predictions very seriously.'}, {'start': '1100.34', 'end': '1103.24', 'text': \"So I don't know what I'm going to sell my media stock.\"}, {'start': '1103.24', 'end': '1104.56', 'text': 'I would not do that.'}, {'start': '1104.56', 'end': '1106.66', 'text': \"But that's not what I said.\"}, {'start': '1106.66', 'end': '1110.16', 'text': 'I want to blame you for this if it goes up.'}, {'start': '1110.16', 'end': '1112.78', 'text': 'But the third one is pretty strange.'}, {'start': '1112.78', 'end': '1115.12', 'text': 'You know, companies are born, companies die.'}, {'start': '1115.12', 'end': '1117.84', 'text': 'But you said some companies will suddenly die.'}, {'start': '1117.84', 'end': '1118.98', 'text': 'What does that mean?'}, {'start': '1118.98', 'end': '1126.94', 'text': 'No, I think, see, I think the interesting thing is, and I think that it comes back to the fundamental nature of AI.'}, {'start': '1126.94', 'end': '1129.06', 'text': 'AI is a tool, right?'}, {'start': '1129.06', 'end': '1130.58', 'text': 'And you have to use that.'}, {'start': '1130.58', 'end': '1134.64', 'text': 'And you have to use that within your business process, right?'}, {'start': '1134.64', 'end': '1136.62', 'text': 'And how AI is being used.'}, {'start': '1136.62', 'end': '1151.56', 'text': \"And what's going to happen is that, I mean, I think this is true with, you know, when someone said in terms of, you know, people, they said that the people who leverage AI will be more effective than those who don't leverage AI.\"}, {'start': '1151.56', 'end': '1154.22', 'text': 'And that is true for organizations also.'}, {'start': '1154.22', 'end': '1162.48', 'text': \"So organizations that leverage AI fundamentally in their core business processes will be more effective than those who don't.\"}, {'start': '1162.48', 'end': '1163.94', 'text': \"And I think that's the thing.\"}, {'start': '1163.94', 'end': '1170.3', 'text': \"And you won't know the difference until one day it becomes too obvious and it will be too late.\"}, {'start': '1170.3', 'end': '1176.76', 'text': \"And I think that's the reason why everybody needs to think about what it means for your business.\"}, {'start': '1176.76', 'end': '1179.36', 'text': 'Because everything will be fine.'}, {'start': '1179.36', 'end': '1191.02', 'text': 'Everything will be fine and one day somebody in your, either your competitor in your space or somebody brand new coming into your space will be reimagining your business process completely.'}, {'start': '1191.02', 'end': '1198.84', 'text': \"And at that stage you will find that it's a very big, very tall mountain to climb.\"}, {'start': '1198.84', 'end': '1209.34', 'text': \"and that's why I think it's important for both people and entities to think about how they will upgrade themselves or they will modify their business processes.\"}, {'start': '1209.34', 'end': '1221.2', 'text': \"That's a very nuanced answer and everybody here who's running a business should really think about it because life will be the same and then suddenly something will you know, then it will be a step change.\"}, {'start': '1221.2', 'end': '1225.42', 'text': \"Vivek, I have a few more questions but I'm sure the audience has a lot of questions for you.\"}, {'start': '1225.42', 'end': '1227.66', 'text': 'So, how are we doing on time?'}, {'start': '1227.66', 'end': '1228.76', 'text': 'Okay.'}, {'start': '1228.76', 'end': '1237.84', 'text': 'So, does, okay, a lot of questions, so love to, is there a mic that we can pass around?'}, {'start': '1237.84', 'end': '1241.06', 'text': 'Thank you.'}, {'start': '1241.06', 'end': '1242.18', 'text': 'My name is Karthik.'}, {'start': '1242.18', 'end': '1246.38', 'text': 'I work for IT service industry.'}, {'start': '1246.38', 'end': '1254.86', 'text': \"So you're saying that you're working on LLM, sorry, it's a fine-tuned LLM on top of Llama.\"}, {'start': '1254.86', 'end': '1259.54', 'text': \"My basic question, fundamental question is we don't have a foundational model for India.\"}, {'start': '1259.54', 'end': '1264.9', 'text': 'Most of the models are basically using English or those kind of things.'}, {'start': '1264.9', 'end': '1270.1', 'text': 'For example, Andrew was talking about the tokenizers and things like that.'}, {'start': '1270.1', 'end': '1277.16', 'text': 'So are you working on anything like that or do you want to use mostly the existing models and run on top of it?'}, {'start': '1277.16', 'end': '1278.72', 'text': 'You asked a good question.'}, {'start': '1278.72', 'end': '1280.7', 'text': 'You asked a cherry question for himself.'}, {'start': '1280.7', 'end': '1298.56', 'text': \"No, I think the interesting thing is that if you look at, and we have actually a blog on this on our website, I think one of the things that we've done is we've actually built a customized tokenizer, which actually fundamentally changes the cost of some of these generations in Indian languages.\"}, {'start': '1298.56', 'end': '1340.42', 'text': \"and I think that we are not just fine tuning, we are actually, we are leveraging the existing pre-training but we are doing what's known as continual pre-training which actually, but having said that, you know, I think that when we have to figure out where is the data to train an extremely large model from scratch and some of those things are things which will happen over time but I think that, I think that, yes, I think that we will be doing various kinds of things but the interesting thing is that if I want to change the accessibility problem with an existing open source model, how do I do that and that's the problem that we have that we think we have solved and it's going to be the heart of this OpenRTC series.\"}, {'start': '1340.42', 'end': '1344.36', 'text': \"It's extremely well explained in the blog even I could understand it so.\"}, {'start': '1344.36', 'end': '1348.2', 'text': \"Hi, I'm Prashant, I work for a Fintech company.\"}, {'start': '1348.2', 'end': '1359.06', 'text': 'My question is like unlike China we never had a consumer facing application coming out from India and in web 1, web 2, crypto and all.'}, {'start': '1359.06', 'end': '1389.88', 'text': 'Why do you think it will be different this time in like AI because will the DPI and other things will serve the same purpose what the great fire wall did in China or do you think like in because AI is a strategic sector no outside country can work in NASA projects, maybe all government contract will go to them, what is the moat here for an Indian company?'}, {'start': '1389.88', 'end': '1433.3', 'text': \"So I think the question is, I don't know the answer to these questions, right, I mean I think that it's difficult to predict but I do believe and as I'm repeating that the combinatorial effect of using Gen AI at a large scale in addition along with the DPI work that we've done in India will have people and I think that in the end it is the intent is that people need to be able to use it and they will vote by things that are useful for them and if that doesn't happen you're right and I think that we have to figure out what is the mechanism of delivery of apps right?\"}, {'start': '1433.3', 'end': '1436.42', 'text': 'Where do Indians consume contact.'}, {'start': '1436.42', 'end': '1437.16', 'text': \"That's the question.\"}, {'start': '1437.16', 'end': '1439.22', 'text': \"I'm so sorry but we are out of time.\"}, {'start': '1439.22', 'end': '1443.2', 'text': 'Vivek will be outside so he would be able to answer the question.'}, {'start': '1443.2', 'end': '1444.6', 'text': 'Do we have time for one last question?'}, {'start': '1444.6', 'end': '1446.22', 'text': 'Can I just take one last?'}, {'start': '1446.22', 'end': '1446.78', 'text': 'Yeah.'}, {'start': '1446.78', 'end': '1447.54', 'text': 'Thank you.'}, {'start': '1447.54', 'end': '1447.98', 'text': 'Thank you.'}, {'start': '1447.98', 'end': '1449.0', 'text': \"I'm Manish Kothari.\"}, {'start': '1449.0', 'end': '1450.66', 'text': \"I'm from ISBR Business School.\"}, {'start': '1450.66', 'end': '1453.16', 'text': 'Good that I got a chance to ask you this question.'}, {'start': '1453.16', 'end': '1461.46', 'text': 'During lunch time there were a few of our educationists whom we were talking about and there was one from school and we are from the MBA institutions.'}, {'start': '1461.46', 'end': '1467.12', 'text': 'We were thinking of these present generations how do we get them into what you are doing.'}, {'start': '1467.12', 'end': '1480.08', 'text': 'There is one thing that they have been regularly that the concentrations that they are working on but artificial intelligence and getting into this, getting them into their academics and making them a part of it is very important including the trainers who train them.'}, {'start': '1480.08', 'end': '1522.1', 'text': 'making them future ready into what you are doing is amazing and the speed that which is growing it is calling for a lot of training that needs to be done can you from your angle throw some light on how we could make them future ready how these people who are who are management graduates and from schools who are coming out how do we get into this part of technology that you spoke about So this is really a challenge because I think everyone will need to understand at some level what this technology does and I think that we have to rethink how we get everyone into these and this kind of education has to be at many different levels, right?'}, {'start': '1522.1', 'end': '1544.4', 'text': \"There are from a core set of having people who are extremely good at some and there you don't need as many but then there are basically vast numbers of people who can actually leverage By the way, the most important thing about, and maybe that's part of what makes an LLM interesting is that how you use it, your mileage varies by that.\"}, {'start': '1544.4', 'end': '1563.12', 'text': 'And to understand how to actually leverage this in an interesting way is something that we have to widely teach many, many people and because asking the things in the right way and having the right kind of applications will make a huge difference to how people can leverage these tools.'}, {'start': '1563.12', 'end': '1564.0', 'text': 'Thank you.'}, {'start': '1564.0', 'end': '1565.54', 'text': 'Thank you very much Vivek.'}, {'start': '1565.54', 'end': '1567.6', 'text': 'Very good luck to Sarvam and good luck to India.'}, {'start': '1567.6', 'end': '1569.34', 'text': \"I think it's going to be a lot of .\"}, {'start': '1569.36', 'end': '1570.8', 'text': 'Thanks Bala.'}, {'start': '1570.8', 'end': '1574.22', 'text': 'Thank you Mr.'}, {'start': '1574.22', 'end': '1574.72', 'text': 'Raghavan.'}]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from datetime import timedelta\n",
        "\n",
        "# Helper function to convert time strings to seconds\n",
        "def time_to_seconds(time_str):\n",
        "    parts = time_str.split(':')\n",
        "    try:\n",
        "        if len(parts) == 3:\n",
        "            h, m, s = map(float, parts)\n",
        "        elif len(parts) == 2:\n",
        "            h = 0\n",
        "            m, s = map(float, parts)\n",
        "        elif len(parts) == 1:\n",
        "            h = 0\n",
        "            m = 0\n",
        "            s = float(parts[0])\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid time format: {time_str}\")\n",
        "        return timedelta(hours=h, minutes=m, seconds=s).total_seconds()\n",
        "    except ValueError:\n",
        "        print(f\"Invalid time format: {time_str}\")  # Print the invalid time string\n",
        "        raise\n",
        "\n",
        "# Parsing functions\n",
        "def parse_vad_file(vad_file):\n",
        "    vad_segments = []\n",
        "    with open(vad_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            match = re.match(r'\\[([\\d:.]+) --> ([\\d:.]+)\\] SPEECH', line)\n",
        "            if match:\n",
        "                start_time = match.group(1)\n",
        "                end_time = match.group(2)\n",
        "                vad_segments.append({\"start\": start_time, \"end\": end_time})\n",
        "    return vad_segments\n",
        "\n",
        "def parse_text_timestamps_file(text_file):\n",
        "    text_segments = []\n",
        "    with open(text_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            match = re.match(r'([\\d:.]+)-([\\d:.]+): (.+)', line)\n",
        "            if match:\n",
        "                start_time = match.group(1)\n",
        "                end_time = match.group(2)\n",
        "                text = match.group(3)\n",
        "                text_segments.append({\"start\": start_time, \"end\": end_time, \"text\": text})\n",
        "    return text_segments\n",
        "\n",
        "# Parse the VAD and text segments\n",
        "vad_segments = parse_vad_file(\"vad.txt\")\n",
        "text_segments = parse_text_timestamps_file(\"/content/Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.txt\")\n",
        "\n",
        "print(\"VAD segments:\", vad_segments)\n",
        "print(\"Text segments:\", text_segments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3cd1LjcSvgU",
        "outputId": "6f2a8ec7-dbb0-4cf2-d4ed-f573511820f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk ID: 1, Length: 1.22s, Start: 0.06s, End: 1.28s, Text: Congratulations to you Mr.\n",
            "Chunk ID: 2, Length: 0.90s, Start: 1.28s, End: 2.18s, Text: Raghavan for that.\n",
            "Chunk ID: 3, Length: 1.42s, Start: 2.18s, End: 3.60s, Text: Thank you so much for joining us.\n",
            "Chunk ID: 4, Length: 0.75s, Start: 3.60s, End: 4.35s, Text: Over to you.\n",
            "Chunk ID: 5, Length: 0.79s, Start: 8.69s, End: 9.48s, Text: Hi everybody.\n",
            "Chunk ID: 6, Length: 0.74s, Start: 9.48s, End: 10.22s, Text: How are you?\n",
            "Chunk ID: 7, Length: 1.88s, Start: 11.88s, End: 13.76s, Text: I am not hearing this at all.\n",
            "Chunk ID: 8, Length: 3.33s, Start: 13.76s, End: 17.09s, Text: It's like a post lunch energy downer or something.\n",
            "Chunk ID: 9, Length: 0.71s, Start: 18.22s, End: 18.93s, Text: Let's hear it.\n",
            "Chunk ID: 10, Length: 1.02s, Start: 19.72s, End: 20.74s, Text: Are you guys awake?\n",
            "Chunk ID: 11, Length: 0.57s, Start: 21.90s, End: 22.47s, Text: Alright.\n",
            "Chunk ID: 12, Length: 4.70s, Start: 22.96s, End: 27.66s, Text: You better be because we have a superstar guest here.\n",
            "Chunk ID: 13, Length: 4.72s, Start: 27.66s, End: 32.38s, Text: You heard the $41 million and I didn't hear honestly anything she said after that.\n",
            "Chunk ID: 14, Length: 6.68s, Start: 32.38s, End: 39.06s, Text: So we're going to ask for about $40 million from him by the end of this conversation, okay?\n",
            "Chunk ID: 15, Length: 2.18s, Start: 39.06s, End: 41.24s, Text: But let's get started.\n",
            "Chunk ID: 16, Length: 4.12s, Start: 41.24s, End: 45.36s, Text: I want to introduce Vivek and Pratyush, his co-founder who's not here.\n",
            "Chunk ID: 17, Length: 5.64s, Start: 45.36s, End: 51.00s, Text: We wanted to start with playing a video of what OpenHearty does.\n",
            "Chunk ID: 18, Length: 3.16s, Start: 51.00s, End: 54.16s, Text: I encourage all of you to go to the website serverum.\n",
            "Chunk ID: 19, Length: 1.32s, Start: 54.16s, End: 55.48s, Text: ai and check it out.\n",
            "Chunk ID: 20, Length: 15.00s, Start: 56.17s, End: 71.17s, Text: but let me start by introducing Vivek Vivek is a dear friend and he is very very modest one of the most modest guys that I know but his personal journey Vivek you got a PhD from Carnegie Mellon, you started and sold a company to Magma and Vivek and I moved back to India we were both in the valley on the same day actually and you have been in India for the last 16 years and what most people don't know is your journey at Aadhaar He spent 13 years selflessly at Aadhaar.\n",
            "Chunk ID: 21, Length: 15.00s, Start: 71.17s, End: 86.17s, Text: \n",
            "Chunk ID: 22, Length: 2.13s, Start: 86.17s, End: 88.30s, Text: \n",
            "Chunk ID: 23, Length: 9.70s, Start: 88.30s, End: 98.00s, Text: Nobody would have heard of him, but he was a pioneering technology visionary behind Aadhaar, which we all take for granted today.\n",
            "Chunk ID: 24, Length: 3.26s, Start: 98.00s, End: 101.26s, Text: So please give it out.\n",
            "Chunk ID: 25, Length: 5.83s, Start: 101.26s, End: 107.09s, Text: Honestly, when I think of selfless service, truly selfless service, I always think of Vivek.\n",
            "Chunk ID: 26, Length: 7.14s, Start: 107.86s, End: 115.00s, Text: And since then, he also was at AI for Bharat, which we're going to touch on, where he met Pratyush's other co-founder.\n",
            "Chunk ID: 27, Length: 3.20s, Start: 115.00s, End: 118.20s, Text: Pratyush had a PhD from ETH at Zurich.\n",
            "Chunk ID: 28, Length: 2.20s, Start: 118.20s, End: 120.40s, Text: He was in the IBM research.\n",
            "Chunk ID: 29, Length: 5.90s, Start: 120.40s, End: 126.30s, Text: He was at Microsoft research playing a key role and a faculty at IIT Madras and at AF Abharat.\n",
            "Chunk ID: 30, Length: 2.40s, Start: 126.30s, End: 128.70s, Text: So that's a little brief introduction about them.\n",
            "Chunk ID: 31, Length: 2.72s, Start: 128.70s, End: 131.42s, Text: These guys are modest, modest engineers.\n",
            "Chunk ID: 32, Length: 2.22s, Start: 131.42s, End: 133.64s, Text: So they don't toot their own horn.\n",
            "Chunk ID: 33, Length: 3.68s, Start: 133.64s, End: 137.32s, Text: So forgive me for tooting their horn in this case.\n",
            "Chunk ID: 34, Length: 4.95s, Start: 137.32s, End: 142.27s, Text: But let's jump right in about the money, funding.\n",
            "Chunk ID: 35, Length: 3.12s, Start: 142.62s, End: 145.74s, Text: 41 million bucks, man, that's a lot of money, right?\n",
            "Chunk ID: 36, Length: 3.02s, Start: 145.74s, End: 148.76s, Text: Every entrepreneur here is saying, what the hell did these guys do?\n",
            "Chunk ID: 37, Length: 2.91s, Start: 148.76s, End: 151.67s, Text: What did the investors see to write such a big check?\n",
            "Chunk ID: 38, Length: 5.54s, Start: 152.60s, End: 158.14s, Text: No, I think it's a new trend of what's going on in India.\n",
            "Chunk ID: 39, Length: 12.76s, Start: 158.14s, End: 170.90s, Text: I think that for the very first time, I think the investors have looked at, you know, let's try and build something deep tech out of the country and let's try to figure out how to build something as a foundational technology out of the country.\n",
            "Chunk ID: 40, Length: 2.50s, Start: 170.90s, End: 173.40s, Text: And that's really what's really exciting.\n",
            "Chunk ID: 41, Length: 15.00s, Start: 173.40s, End: 188.40s, Text: And I think that about, as Bala was mentioning, for the past 15 years, I've been kind of working in both digital public infrastructure and kind of non-profit kind of things.\n",
            "Chunk ID: 42, Length: 2.28s, Start: 188.40s, End: 190.68s, Text: \n",
            "Chunk ID: 43, Length: 8.48s, Start: 190.68s, End: 199.16s, Text: But when this whole thing of generative AI came about, we said, okay, how can I actually make a difference in this space?\n",
            "Chunk ID: 44, Length: 6.74s, Start: 199.16s, End: 205.90s, Text: And I said, maybe this is the opportunity to actually come out and really build something.\n",
            "Chunk ID: 45, Length: 6.80s, Start: 205.90s, End: 212.70s, Text: And the only way that we realized that you can do it is actually in the private sector.\n",
            "Chunk ID: 46, Length: 5.36s, Start: 212.70s, End: 218.06s, Text: And then we went out there and we said we want to build something which is a continuation.\n",
            "Chunk ID: 47, Length: 4.66s, Start: 218.06s, End: 222.72s, Text: And fundamentally the question is, the reason of what we want to do at Servam.\n",
            "Chunk ID: 48, Length: 7.06s, Start: 222.72s, End: 229.78s, Text: ai is we want to basically make generative AI available and accessible to the people in the country.\n",
            "Chunk ID: 49, Length: 1.76s, Start: 229.78s, End: 231.54s, Text: And that's the intent.\n",
            "Chunk ID: 50, Length: 4.86s, Start: 231.54s, End: 236.40s, Text: And when we said that we want to do this, there was a resonance in the investment community.\n",
            "Chunk ID: 51, Length: 6.52s, Start: 236.40s, End: 242.92s, Text: And I think it's a responsibility to really to show that something like this can be built out of India.\n",
            "Chunk ID: 52, Length: 3.84s, Start: 242.92s, End: 246.76s, Text: So we see that as confidence and a responsibility.\n",
            "Chunk ID: 53, Length: 15.00s, Start: 246.76s, End: 261.76s, Text: And I also hope it's a trend that there are many more people like us who are backed because if you look at it, maybe it's a large number in a, you know, in the Indian context but in the global context, I think there is just, there should be many, many more entrepreneurs who are backed to do things in India.\n",
            "Chunk ID: 54, Length: 3.29s, Start: 261.76s, End: 265.05s, Text: \n",
            "Chunk ID: 55, Length: 1.72s, Start: 265.86s, End: 267.58s, Text: I'm going to come back to the many more entrepreneurs.\n",
            "Chunk ID: 56, Length: 3.54s, Start: 267.58s, End: 271.12s, Text: I'm obviously going to ask you about Bhavish's Krutrim.\n",
            "Chunk ID: 57, Length: 15.00s, Start: 271.12s, End: 286.12s, Text: So we're going to come back to that question but again, 41 million dollars, I mean all of what you said, you know, 2 million dollars, you know, that's a good amount of money for of startup which, you know, which has not yet built anything, what are you going to do with all this money?\n",
            "Chunk ID: 58, Length: 0.30s, Start: 286.12s, End: 286.42s, Text: \n",
            "Chunk ID: 59, Length: 1.25s, Start: 288.91s, End: 290.16s, Text: I can solve the problem.\n",
            "Chunk ID: 60, Length: 1.98s, Start: 290.16s, End: 292.14s, Text: I can have a perfect solution for the problem.\n",
            "Chunk ID: 61, Length: 6.22s, Start: 292.14s, End: 298.36s, Text: I think in the last week I've got lots of calls from lots of people telling me how I can do it.\n",
            "Chunk ID: 62, Length: 0.96s, Start: 298.36s, End: 299.32s, Text: I know you first, okay?\n",
            "Chunk ID: 63, Length: 1.62s, Start: 299.32s, End: 300.94s, Text: I'll be landed in the country the same day.\n",
            "Chunk ID: 64, Length: 3.04s, Start: 300.94s, End: 303.98s, Text: I'm in front of the queue.\n",
            "Chunk ID: 65, Length: 5.54s, Start: 303.98s, End: 309.52s, Text: But honestly, I think the key thing in this is to putting together an amazing team.\n",
            "Chunk ID: 66, Length: 10.18s, Start: 309.52s, End: 319.70s, Text: and we actually have an amazing team but we believe that it is talent that will drive this kind of thing and so it is to get key talent and of course the other thing is compute.\n",
            "Chunk ID: 67, Length: 11.20s, Start: 319.70s, End: 330.90s, Text: This is extremely expensive compute wise to actually do these kinds of things and I think that those are the two primary things that we would use this for.\n",
            "Chunk ID: 68, Length: 0.10s, Start: 330.92s, End: 331.02s, Text: Okay.\n",
            "Chunk ID: 69, Length: 3.88s, Start: 331.04s, End: 334.92s, Text: I'm computing in my own head as an entrepreneur.\n",
            "Chunk ID: 70, Length: 2.52s, Start: 334.92s, End: 337.44s, Text: Talent, okay, you have like 20, 15 people.\n",
            "Chunk ID: 71, Length: 1.44s, Start: 337.44s, End: 338.88s, Text: How much are you paying these guys?\n",
            "Chunk ID: 72, Length: 2.22s, Start: 338.88s, End: 341.10s, Text: but okay, we won't touch on that.\n",
            "Chunk ID: 73, Length: 2.80s, Start: 341.10s, End: 343.90s, Text: But let's talk about what you guys actually built.\n",
            "Chunk ID: 74, Length: 1.20s, Start: 343.90s, End: 345.10s, Text: What is OpenHathi?\n",
            "Chunk ID: 75, Length: 3.67s, Start: 345.10s, End: 348.77s, Text: How would you explain OpenHathi to many people here who might not have known about it?\n",
            "Chunk ID: 76, Length: 10.91s, Start: 349.29s, End: 360.20s, Text: So I think OpenHathi is, so first of all, right, we come from, I personally come from the open source ecosystem and also from the DPI ecosystem.\n",
            "Chunk ID: 77, Length: 5.10s, Start: 360.20s, End: 365.30s, Text: So we believe that for this to work, we need the ecosystem to be successful.\n",
            "Chunk ID: 78, Length: 7.42s, Start: 365.30s, End: 372.72s, Text: And as a result of that, one of the first things we did was hey, there are these open source large language models that exist, right?\n",
            "Chunk ID: 79, Length: 3.78s, Start: 372.72s, End: 376.50s, Text: I mean, everybody knows about the Llama family from Meta.\n",
            "Chunk ID: 80, Length: 2.02s, Start: 376.50s, End: 378.52s, Text: There are others like Mistral.\n",
            "Chunk ID: 81, Length: 4.86s, Start: 378.52s, End: 383.38s, Text: There are a bunch of open source large language models.\n",
            "Chunk ID: 82, Length: 7.72s, Start: 383.38s, End: 391.10s, Text: And then we said, is there any way that take an existing open source model and teach it language skills, right?\n",
            "Chunk ID: 83, Length: 6.18s, Start: 391.10s, End: 397.28s, Text: And that is really what we said, that can we do something like that?\n",
            "Chunk ID: 84, Length: 10.20s, Start: 397.28s, End: 407.48s, Text: And is this a, you know, relatively frugal way of actually, you know, making models, you know, work in diverse languages?\n",
            "Chunk ID: 85, Length: 7.84s, Start: 407.48s, End: 415.32s, Text: Because the truth is still today, I mean, if you look at the amount of data and knowledge, it is still English dominates these things.\n",
            "Chunk ID: 86, Length: 9.72s, Start: 415.32s, End: 425.04s, Text: And I think that how do you actually take and make it understand Indian language, understand Indian context, and all of those things in actually an efficient way.\n",
            "Chunk ID: 87, Length: 2.46s, Start: 425.04s, End: 427.50s, Text: And therefore, this was an attempt to do that.\n",
            "Chunk ID: 88, Length: 14.78s, Start: 427.50s, End: 442.28s, Text: And OpenHathi is currently based on the Llama 7 billion model, but we'll be releasing many more models in different languages, different sizes, and things like that as part of this series.\n",
            "Chunk ID: 89, Length: 7.74s, Start: 442.28s, End: 450.02s, Text: And of course, we will be building further models on those and doing other things to actually... And we'll also have endpoints that people can use.\n",
            "Chunk ID: 90, Length: 5.32s, Start: 450.02s, End: 455.34s, Text: So therefore, it's definitely something that people can use to things.\n",
            "Chunk ID: 91, Length: 4.82s, Start: 455.34s, End: 460.16s, Text: And that's the essence of what this OpenHathi is.\n",
            "Chunk ID: 92, Length: 7.16s, Start: 460.16s, End: 467.32s, Text: So what does it mean to people in the audience here who are either doing their own startups or a business or developers?\n",
            "Chunk ID: 93, Length: 2.50s, Start: 467.32s, End: 469.82s, Text: How should they look at OpenAI?\n",
            "Chunk ID: 94, Length: 0.92s, Start: 469.82s, End: 470.74s, Text: Sorry, Sarman.\n",
            "Chunk ID: 95, Length: 2.34s, Start: 470.74s, End: 473.08s, Text: Not OpenAI.\n",
            "Chunk ID: 96, Length: 7.84s, Start: 473.08s, End: 480.92s, Text: No, I think the way you look at it is that one of the important things that we are doing is we're not just building models.\n",
            "Chunk ID: 97, Length: 15.00s, Start: 480.92s, End: 495.92s, Text: We are also going to be building a platform, a platform for developers where you can actually use a combination of various different kinds of models, some which are from us, some which are open source, some which may not be open source, and actually to actually pull together and figure out how to deploy, you know, generative AI applications at scale and understand and evaluate their performance in an efficient manner.\n",
            "Chunk ID: 98, Length: 10.58s, Start: 495.92s, End: 506.50s, Text: \n",
            "Chunk ID: 99, Length: 2.48s, Start: 506.50s, End: 508.98s, Text: And that's something that we're planning to do.\n",
            "Chunk ID: 100, Length: 4.92s, Start: 508.98s, End: 513.90s, Text: And this platform is, you know, in the next couple of months will be coming out there.\n",
            "Chunk ID: 101, Length: 1.64s, Start: 513.90s, End: 515.54s, Text: It will be available to developers.\n",
            "Chunk ID: 102, Length: 6.68s, Start: 515.54s, End: 522.22s, Text: But, of course, those who want to start with the open source things and hack for that, of course, please go ahead and do that as well.\n",
            "Chunk ID: 103, Length: 2.10s, Start: 522.22s, End: 524.32s, Text: That's phenomenal.\n",
            "Chunk ID: 104, Length: 4.10s, Start: 524.32s, End: 528.42s, Text: But how does it compare to OpenAI itself or Google?\n",
            "Chunk ID: 105, Length: 2.94s, Start: 530.26s, End: 533.20s, Text: See, at least the things that we are doing now, right?\n",
            "Chunk ID: 106, Length: 8.10s, Start: 533.20s, End: 541.30s, Text: I mean, one of the things that when we thought about building Sarvam, we said we want to build a full stack generative AI company.\n",
            "Chunk ID: 107, Length: 7.06s, Start: 541.30s, End: 548.36s, Text: And different people are understanding of a stack is that we need to know how to train models from scratch.\n",
            "Chunk ID: 108, Length: 5.80s, Start: 548.36s, End: 554.16s, Text: We need to know how to kind of figure out how to deploy models to solve real world use cases.\n",
            "Chunk ID: 109, Length: 8.34s, Start: 554.16s, End: 562.50s, Text: And we need to play in the ecosystem to make sure that we can actually deploy population scale applications.\n",
            "Chunk ID: 110, Length: 2.54s, Start: 562.50s, End: 565.04s, Text: So we were thinking about all of these things.\n",
            "Chunk ID: 111, Length: 4.38s, Start: 565.04s, End: 569.42s, Text: But still the models we were talking about are fairly small models.\n",
            "Chunk ID: 112, Length: 1.30s, Start: 569.42s, End: 570.72s, Text: They are fairly small models, right?\n",
            "Chunk ID: 113, Length: 4.26s, Start: 570.72s, End: 574.98s, Text: The 7 to maybe up to 70 billion kind of range we're talking about.\n",
            "Chunk ID: 114, Length: 5.10s, Start: 574.98s, End: 580.08s, Text: While these models like OpenAI and Google are obviously much bigger models.\n",
            "Chunk ID: 115, Length: 9.86s, Start: 580.08s, End: 589.94s, Text: But we want to understand the techniques and be able to build that muscle to do all of these things to make it available to people.\n",
            "Chunk ID: 116, Length: 6.38s, Start: 589.94s, End: 596.32s, Text: Now, those models are, I mean, as I said, you know, I think that there is space for all of those things.\n",
            "Chunk ID: 117, Length: 15.00s, Start: 596.32s, End: 611.32s, Text: And I think as even Sridhar was talking about earlier in the day, we believe that these smaller models can do very, I mean, many, many kind of domain specific tasks extremely well, probably even better than the larger models.\n",
            "Chunk ID: 118, Length: 1.22s, Start: 611.32s, End: 612.54s, Text: \n",
            "Chunk ID: 119, Length: 2.30s, Start: 612.54s, End: 614.84s, Text: And that is really one of the key areas.\n",
            "Chunk ID: 120, Length: 3.16s, Start: 614.84s, End: 618.00s, Text: And so, therefore, the value of these kinds of things, right?\n",
            "Chunk ID: 121, Length: 4.28s, Start: 618.00s, End: 622.28s, Text: We are not aiming in these set of models to build any AGI, right?\n",
            "Chunk ID: 122, Length: 1.34s, Start: 622.28s, End: 623.62s, Text: That's not our goal here.\n",
            "Chunk ID: 123, Length: 9.60s, Start: 623.62s, End: 633.22s, Text: Our goal is to make things that work extremely well for domain-specific use cases or increase accessibility through language and all of those kinds of things.\n",
            "Chunk ID: 124, Length: 1.82s, Start: 633.22s, End: 635.04s, Text: And obviously all of this is unique to India.\n",
            "Chunk ID: 125, Length: 1.58s, Start: 635.04s, End: 636.62s, Text: But what is unique about India?\n",
            "Chunk ID: 126, Length: 10.53s, Start: 636.62s, End: 647.15s, Text: I mean, is there anything special in our ecosystem that makes small models focused with Indian languages better for, more suited for our problems?\n",
            "Chunk ID: 127, Length: 5.59s, Start: 648.03s, End: 653.62s, Text: So I think that, I mean, there are quite a few things that are unique about India, right?\n",
            "Chunk ID: 128, Length: 8.34s, Start: 653.62s, End: 661.96s, Text: The first thing is, I think, that we are a voice-first nation, so therefore I think voice has to be the core to doing things.\n",
            "Chunk ID: 129, Length: 7.74s, Start: 661.96s, End: 669.70s, Text: The other thing, of course, India is extremely, it's a cost-conscious country from a cost perspective.\n",
            "Chunk ID: 130, Length: 9.14s, Start: 669.70s, End: 678.84s, Text: Now, I would say that there are lots of interesting use cases where you can use OpenAI and the cost structure works depending on your application.\n",
            "Chunk ID: 131, Length: 6.30s, Start: 678.84s, End: 685.14s, Text: But when you want to scale things to a massive level and make it work, then you have to figure out how small models work.\n",
            "Chunk ID: 132, Length: 3.30s, Start: 685.14s, End: 688.44s, Text: So that's something that is also specific to India.\n",
            "Chunk ID: 133, Length: 7.50s, Start: 688.44s, End: 695.94s, Text: The third thing which is specific to India is really the success that India has had in building all this digital public infrastructure.\n",
            "Chunk ID: 134, Length: 11.80s, Start: 695.94s, End: 707.74s, Text: When you add the AI layer on top of it, then you can actually get dramatic, you know dramatic I think multiplicative combinatorial effects based on doing things like that.\n",
            "Chunk ID: 135, Length: 8.98s, Start: 707.74s, End: 716.72s, Text: That's a phenomenal point like you know it's like DPI to the power of AI almost in some ways and as a part of Aadhaar building Aadhaar no better person than you.\n",
            "Chunk ID: 136, Length: 12.16s, Start: 716.72s, End: 728.88s, Text: So in summary what I'm hearing is small models specialized with trained with Indic specific language data suited for Indian problems at a compelling cost point will be suited for us.\n",
            "Chunk ID: 137, Length: 3.84s, Start: 728.88s, End: 732.72s, Text: We're not solving some world autonomous vehicles or some complex problem.\n",
            "Chunk ID: 138, Length: 5.18s, Start: 732.72s, End: 737.90s, Text: We're solving some basic problems specifically focused on voice with multiple languages.\n",
            "Chunk ID: 139, Length: 1.52s, Start: 737.90s, End: 739.42s, Text: That is what you see as the future.\n",
            "Chunk ID: 140, Length: 2.14s, Start: 739.44s, End: 741.58s, Text: Am I paraphrasing this correctly?\n",
            "Chunk ID: 141, Length: 11.44s, Start: 741.58s, End: 753.02s, Text: I think that certainly voice and Indian languages are an important part of our strategy but we will be building custom models to solve various other kinds of problems as well.\n",
            "Chunk ID: 142, Length: 10.44s, Start: 753.02s, End: 763.46s, Text: It's not just limited to I think in different domains, working in different domains making, building things based on unique data that enterprises have and things like that.\n",
            "Chunk ID: 143, Length: 1.51s, Start: 763.46s, End: 764.97s, Text: So that's something that we'll also look at.\n",
            "Chunk ID: 144, Length: 0.65s, Start: 765.35s, End: 766.00s, Text: Fair enough.\n",
            "Chunk ID: 145, Length: 8.09s, Start: 766.00s, End: 774.09s, Text: So coming back to the elephant in the room, no fun intended with OpenHathi, what about Bhave Shagriwal and Krutrim?\n",
            "Chunk ID: 146, Length: 0.91s, Start: 774.51s, End: 775.42s, Text: What is your take on that?\n",
            "Chunk ID: 147, Length: 1.02s, Start: 775.42s, End: 776.44s, Text: I think it's great.\n",
            "Chunk ID: 148, Length: 1.76s, Start: 776.44s, End: 778.20s, Text: I think it's wonderful, right?\n",
            "Chunk ID: 149, Length: 7.36s, Start: 778.20s, End: 785.56s, Text: I mean, the fact that the technology AI is so important that we need multiple people working on it.\n",
            "Chunk ID: 150, Length: 6.38s, Start: 785.56s, End: 791.94s, Text: The fact that there are other people thinking actually validates that this is an important problem to be solved.\n",
            "Chunk ID: 151, Length: 5.58s, Start: 791.94s, End: 797.52s, Text: And I think that we need everybody to come together and do that.\n",
            "Chunk ID: 152, Length: 1.48s, Start: 797.52s, End: 799.00s, Text: So I really welcome that.\n",
            "Chunk ID: 153, Length: 1.18s, Start: 799.00s, End: 800.18s, Text: I think it's great.\n",
            "Chunk ID: 154, Length: 6.34s, Start: 800.18s, End: 806.52s, Text: And I think that there will be different people who will have different takes as to how to solve this kind of problem.\n",
            "Chunk ID: 155, Length: 4.08s, Start: 806.52s, End: 810.60s, Text: And hopefully as a result of that, the entire ecosystem benefits.\n",
            "Chunk ID: 156, Length: 4.56s, Start: 811.68s, End: 816.24s, Text: One more question and then I want to talk about some of the predictions that you've boldly made.\n",
            "Chunk ID: 157, Length: 4.60s, Start: 816.24s, End: 820.84s, Text: So Vivek, I usually ask people about what do you think the future will be and everybody usually hedges.\n",
            "Chunk ID: 158, Length: 3.58s, Start: 820.84s, End: 824.42s, Text: I asked Vivek, what do you think is going to happen by December 2024?\n",
            "Chunk ID: 159, Length: 3.66s, Start: 824.42s, End: 828.08s, Text: What do you think sitting in this room one year later we can expect?\n",
            "Chunk ID: 160, Length: 2.02s, Start: 828.08s, End: 830.10s, Text: And he made three bold predictions.\n",
            "Chunk ID: 161, Length: 1.48s, Start: 830.10s, End: 831.58s, Text: So I want to talk about that.\n",
            "Chunk ID: 162, Length: 1.80s, Start: 831.58s, End: 833.38s, Text: Before that, I have one last question.\n",
            "Chunk ID: 163, Length: 4.40s, Start: 833.38s, End: 837.78s, Text: What are the top three applications that you think are relevant for India?\n",
            "Chunk ID: 164, Length: 1.92s, Start: 837.78s, End: 839.70s, Text: You heard Sridhar talk about medical.\n",
            "Chunk ID: 165, Length: 1.62s, Start: 839.70s, End: 841.32s, Text: Any quick summary?\n",
            "Chunk ID: 166, Length: 3.24s, Start: 841.32s, End: 844.56s, Text: what do you think the top three apps are for India for AI?\n",
            "Chunk ID: 167, Length: 10.13s, Start: 845.05s, End: 855.18s, Text: So, I mean, I think that, as you said, things like education and medical are clearly areas where I think that things can be leveraged.\n",
            "Chunk ID: 168, Length: 7.22s, Start: 855.18s, End: 862.40s, Text: The whole idea of all these kind of... The DPI aspect of it is another major application where things can happen.\n",
            "Chunk ID: 169, Length: 2.36s, Start: 862.40s, End: 864.76s, Text: And here I'm talking about country-specific work.\n",
            "Chunk ID: 170, Length: 5.22s, Start: 864.76s, End: 869.98s, Text: And I think the whole idea which Sridhar also talked about was the concept of software, right?\n",
            "Chunk ID: 171, Length: 8.36s, Start: 869.98s, End: 878.34s, Text: and I think that and clearly we have a very large software industry and how to reimagine those things in this context is also something that's going to be.\n",
            "Chunk ID: 172, Length: 0.93s, Start: 879.59s, End: 880.52s, Text: Fair enough.\n",
            "Chunk ID: 173, Length: 3.32s, Start: 880.52s, End: 883.84s, Text: Are you guys ready for Vivek Raghavan's bold predictions?\n",
            "Chunk ID: 174, Length: 0.58s, Start: 884.58s, End: 885.16s, Text: Yes?\n",
            "Chunk ID: 175, Length: 0.48s, Start: 885.16s, End: 885.64s, Text: No?\n",
            "Chunk ID: 176, Length: 0.92s, Start: 885.64s, End: 886.56s, Text: I'm not hearing any yes.\n",
            "Chunk ID: 177, Length: 1.32s, Start: 886.56s, End: 887.88s, Text: This is like a big deal.\n",
            "Chunk ID: 178, Length: 2.04s, Start: 887.88s, End: 889.92s, Text: He's like one of the smartest guys that I know.\n",
            "Chunk ID: 179, Length: 1.38s, Start: 889.92s, End: 891.30s, Text: He wants to make three predictions.\n",
            "Chunk ID: 180, Length: 1.08s, Start: 891.32s, End: 892.40s, Text: You don't want to hear it?\n",
            "Chunk ID: 181, Length: 1.26s, Start: 892.72s, End: 893.98s, Text: All right.\n",
            "Chunk ID: 182, Length: 5.44s, Start: 893.98s, End: 899.42s, Text: So I asked him what do you think a year later what do you think we can expect?\n",
            "Chunk ID: 183, Length: 6.50s, Start: 899.42s, End: 905.92s, Text: and he came up with three things and usually people give very blah answers when you ask a question like this because they don't want to be caught wrong.\n",
            "Chunk ID: 184, Length: 0.88s, Start: 905.92s, End: 906.80s, Text: Not Vivek.\n",
            "Chunk ID: 185, Length: 1.28s, Start: 906.80s, End: 908.08s, Text: Vivek is bold.\n",
            "Chunk ID: 186, Length: 5.26s, Start: 908.08s, End: 913.34s, Text: So he basically said three things and I'm going to list out the three things and then ask him about it.\n",
            "Chunk ID: 187, Length: 8.12s, Start: 913.34s, End: 921.46s, Text: So number one, he says, I would prefer to talk to an automated customer service than a real person because they'll give me a better answer.\n",
            "Chunk ID: 188, Length: 2.66s, Start: 921.46s, End: 924.12s, Text: So that is Vivek Raghavan's prediction number one.\n",
            "Chunk ID: 189, Length: 8.48s, Start: 924.12s, End: 932.60s, Text: So number two is that when everybody is talking about a GPU shortage, Vivek predicts that there will be a GPU glut in India.\n",
            "Chunk ID: 190, Length: 1.78s, Start: 932.60s, End: 934.38s, Text: He thinks there will be too much GPU.\n",
            "Chunk ID: 191, Length: 3.96s, Start: 934.38s, End: 938.34s, Text: So if you want a short NVIDIA stock, this is a good time.\n",
            "Chunk ID: 192, Length: 7.08s, Start: 938.34s, End: 945.42s, Text: And number three, which was extremely unexpected, he said some companies will suddenly die.\n",
            "Chunk ID: 193, Length: 3.34s, Start: 945.42s, End: 948.76s, Text: So Vivek, these are not what I expected.\n",
            "Chunk ID: 194, Length: 7.98s, Start: 948.76s, End: 956.74s, Text: So do you want to quickly talk about each of them, why you just came up with these, and then we'll throw the open for audience questions.\n",
            "Chunk ID: 195, Length: 6.24s, Start: 956.74s, End: 962.98s, Text: So I don't think I quite said it the way that Pala was going to say it.\n",
            "Chunk ID: 196, Length: 0.88s, Start: 962.98s, End: 963.86s, Text: But it's interesting.\n",
            "Chunk ID: 197, Length: 15.00s, Start: 963.86s, End: 978.86s, Text: But I think the first thing that we said is I think that, and I don't think that this is, I think there will come a time when, you know, in areas of customer service, et cetera, when you want to do something very specific.\n",
            "Chunk ID: 198, Length: 1.72s, Start: 978.86s, End: 980.58s, Text: \n",
            "Chunk ID: 199, Length: 10.50s, Start: 980.58s, End: 991.08s, Text: Today, you know, when you call some kind of a bot, you actually end up, you mostly try to disconnect the call or you're extremely upset that you're talking to a bot.\n",
            "Chunk ID: 200, Length: 14.70s, Start: 991.08s, End: 1005.78s, Text: But I think that there will come a time, and I'm predicting it is sooner than later, that you will actually get better responses from the bot than what the human representative, at least the average human representative that you could talk to could give.\n",
            "Chunk ID: 201, Length: 12.12s, Start: 1005.78s, End: 1017.90s, Text: And I think that that's just a, so I just said that there will come a time where you know it's not a human you're talking to, but it's probably more likely to solve your intent than the human person.\n",
            "Chunk ID: 202, Length: 5.34s, Start: 1017.90s, End: 1023.24s, Text: That's just something that I think that could happen.\n",
            "Chunk ID: 203, Length: 3.02s, Start: 1023.24s, End: 1026.26s, Text: Okay, definitely controversial, but we'll let it go.\n",
            "Chunk ID: 204, Length: 1.68s, Start: 1026.26s, End: 1027.94s, Text: What about the GPU glut?\n",
            "Chunk ID: 205, Length: 11.88s, Start: 1027.94s, End: 1039.82s, Text: No, no, yeah, so I don't think that, so I think that the fact that there is a tremendous shortage right now, I think that shortage will ease because that is how the cycles of things go, right?\n",
            "Chunk ID: 206, Length: 11.42s, Start: 1039.82s, End: 1051.24s, Text: when I think the fact that there was such a severe shortage last year basically caused a number of different players to ramp up in various kinds of forms.\n",
            "Chunk ID: 207, Length: 3.26s, Start: 1051.24s, End: 1054.50s, Text: And I think that that will always go in a cycle.\n",
            "Chunk ID: 208, Length: 6.10s, Start: 1054.50s, End: 1060.60s, Text: But we may find out that there are many, many more interesting problems that people will be able to solve.\n",
            "Chunk ID: 209, Length: 15.00s, Start: 1060.60s, End: 1075.60s, Text: I still remember we were at a Gen AI event in Bangalore and we were talking to people and we said, you know, how many people have access to, you know, four A100s, this was the question that I'd asked, and nobody in the room, and these are all extremely enthusiastic Gen AI people, and nobody had access.\n",
            "Chunk ID: 210, Length: 5.98s, Start: 1075.60s, End: 1081.58s, Text: \n",
            "Chunk ID: 211, Length: 1.86s, Start: 1081.58s, End: 1083.44s, Text: And I think that thing is going to change.\n",
            "Chunk ID: 212, Length: 11.54s, Start: 1083.44s, End: 1094.98s, Text: You will be able to get these kinds of things and people who want to hack and do things will have access to these things without, you know, having to write a, you know, a major check.\n",
            "Chunk ID: 213, Length: 5.36s, Start: 1094.98s, End: 1100.34s, Text: Vivek is also a semiconductor guy before he went into Aadhaar, So I would take his predictions very seriously.\n",
            "Chunk ID: 214, Length: 2.90s, Start: 1100.34s, End: 1103.24s, Text: So I don't know what I'm going to sell my media stock.\n",
            "Chunk ID: 215, Length: 1.32s, Start: 1103.24s, End: 1104.56s, Text: I would not do that.\n",
            "Chunk ID: 216, Length: 2.10s, Start: 1104.56s, End: 1106.66s, Text: But that's not what I said.\n",
            "Chunk ID: 217, Length: 3.50s, Start: 1106.66s, End: 1110.16s, Text: I want to blame you for this if it goes up.\n",
            "Chunk ID: 218, Length: 2.62s, Start: 1110.16s, End: 1112.78s, Text: But the third one is pretty strange.\n",
            "Chunk ID: 219, Length: 2.34s, Start: 1112.78s, End: 1115.12s, Text: You know, companies are born, companies die.\n",
            "Chunk ID: 220, Length: 2.72s, Start: 1115.12s, End: 1117.84s, Text: But you said some companies will suddenly die.\n",
            "Chunk ID: 221, Length: 1.00s, Start: 1117.84s, End: 1118.84s, Text: What does that mean?\n",
            "Chunk ID: 222, Length: 7.88s, Start: 1119.06s, End: 1126.94s, Text: No, I think, see, I think the interesting thing is, and I think that it comes back to the fundamental nature of AI.\n",
            "Chunk ID: 223, Length: 2.12s, Start: 1126.94s, End: 1129.06s, Text: AI is a tool, right?\n",
            "Chunk ID: 224, Length: 1.52s, Start: 1129.06s, End: 1130.58s, Text: And you have to use that.\n",
            "Chunk ID: 225, Length: 4.06s, Start: 1130.58s, End: 1134.64s, Text: And you have to use that within your business process, right?\n",
            "Chunk ID: 226, Length: 1.98s, Start: 1134.64s, End: 1136.62s, Text: And how AI is being used.\n",
            "Chunk ID: 227, Length: 14.94s, Start: 1136.62s, End: 1151.56s, Text: And what's going to happen is that, I mean, I think this is true with, you know, when someone said in terms of, you know, people, they said that the people who leverage AI will be more effective than those who don't leverage AI.\n",
            "Chunk ID: 228, Length: 2.66s, Start: 1151.56s, End: 1154.22s, Text: And that is true for organizations also.\n",
            "Chunk ID: 229, Length: 8.26s, Start: 1154.22s, End: 1162.48s, Text: So organizations that leverage AI fundamentally in their core business processes will be more effective than those who don't.\n",
            "Chunk ID: 230, Length: 1.46s, Start: 1162.48s, End: 1163.94s, Text: And I think that's the thing.\n",
            "Chunk ID: 231, Length: 6.36s, Start: 1163.94s, End: 1170.30s, Text: And you won't know the difference until one day it becomes too obvious and it will be too late.\n",
            "Chunk ID: 232, Length: 6.46s, Start: 1170.30s, End: 1176.76s, Text: And I think that's the reason why everybody needs to think about what it means for your business.\n",
            "Chunk ID: 233, Length: 2.60s, Start: 1176.76s, End: 1179.36s, Text: Because everything will be fine.\n",
            "Chunk ID: 234, Length: 11.66s, Start: 1179.36s, End: 1191.02s, Text: Everything will be fine and one day somebody in your, either your competitor in your space or somebody brand new coming into your space will be reimagining your business process completely.\n",
            "Chunk ID: 235, Length: 7.82s, Start: 1191.02s, End: 1198.84s, Text: And at that stage you will find that it's a very big, very tall mountain to climb.\n",
            "Chunk ID: 236, Length: 10.50s, Start: 1198.84s, End: 1209.34s, Text: and that's why I think it's important for both people and entities to think about how they will upgrade themselves or they will modify their business processes.\n",
            "Chunk ID: 237, Length: 11.86s, Start: 1209.34s, End: 1221.20s, Text: That's a very nuanced answer and everybody here who's running a business should really think about it because life will be the same and then suddenly something will you know, then it will be a step change.\n",
            "Chunk ID: 238, Length: 4.22s, Start: 1221.20s, End: 1225.42s, Text: Vivek, I have a few more questions but I'm sure the audience has a lot of questions for you.\n",
            "Chunk ID: 239, Length: 1.76s, Start: 1225.42s, End: 1227.18s, Text: So, how are we doing on time?\n",
            "Chunk ID: 240, Length: 0.52s, Start: 1228.24s, End: 1228.76s, Text: Okay.\n",
            "Chunk ID: 241, Length: 6.32s, Start: 1228.76s, End: 1235.08s, Text: So, does, okay, a lot of questions, so love to, is there a mic that we can pass around?\n",
            "Chunk ID: 242, Length: 0.48s, Start: 1240.58s, End: 1241.06s, Text: Thank you.\n",
            "Chunk ID: 243, Length: 1.12s, Start: 1241.06s, End: 1242.18s, Text: My name is Karthik.\n",
            "Chunk ID: 244, Length: 4.20s, Start: 1242.18s, End: 1246.38s, Text: I work for IT service industry.\n",
            "Chunk ID: 245, Length: 8.48s, Start: 1246.38s, End: 1254.86s, Text: So you're saying that you're working on LLM, sorry, it's a fine-tuned LLM on top of Llama.\n",
            "Chunk ID: 246, Length: 4.68s, Start: 1254.86s, End: 1259.54s, Text: My basic question, fundamental question is we don't have a foundational model for India.\n",
            "Chunk ID: 247, Length: 5.36s, Start: 1259.54s, End: 1264.90s, Text: Most of the models are basically using English or those kind of things.\n",
            "Chunk ID: 248, Length: 5.20s, Start: 1264.90s, End: 1270.10s, Text: For example, Andrew was talking about the tokenizers and things like that.\n",
            "Chunk ID: 249, Length: 7.06s, Start: 1270.10s, End: 1277.16s, Text: So are you working on anything like that or do you want to use mostly the existing models and run on top of it?\n",
            "Chunk ID: 250, Length: 1.56s, Start: 1277.16s, End: 1278.72s, Text: You asked a good question.\n",
            "Chunk ID: 251, Length: 1.98s, Start: 1278.72s, End: 1280.70s, Text: You asked a cherry question for himself.\n",
            "Chunk ID: 252, Length: 15.00s, Start: 1280.70s, End: 1295.70s, Text: No, I think the interesting thing is that if you look at, and we have actually a blog on this on our website, I think one of the things that we've done is we've actually built a customized tokenizer, which actually fundamentally changes the cost of some of these generations in Indian languages.\n",
            "Chunk ID: 253, Length: 2.86s, Start: 1295.70s, End: 1298.56s, Text: \n",
            "Chunk ID: 254, Length: 15.00s, Start: 1298.56s, End: 1313.56s, Text: and I think that we are not just fine tuning, we are actually, we are leveraging the existing pre-training but we are doing what's known as continual pre-training which actually, but having said that, you know, I think that when we have to figure out where is the data to train an extremely large model from scratch and some of those things are things which will happen over time but I think that, I think that, yes, I think that we will be doing various kinds of things but the interesting thing is that if I want to change the accessibility problem with an existing open source model, how do I do that and that's the problem that we have that we think we have solved and it's going to be the heart of this OpenRTC series.\n",
            "Chunk ID: 255, Length: 15.00s, Start: 1313.56s, End: 1328.56s, Text: \n",
            "Chunk ID: 256, Length: 11.86s, Start: 1328.56s, End: 1340.42s, Text: \n",
            "Chunk ID: 257, Length: 3.13s, Start: 1340.42s, End: 1343.55s, Text: It's extremely well explained in the blog even I could understand it so.\n",
            "Chunk ID: 258, Length: 2.93s, Start: 1345.27s, End: 1348.20s, Text: Hi, I'm Prashant, I work for a Fintech company.\n",
            "Chunk ID: 259, Length: 10.67s, Start: 1348.20s, End: 1358.87s, Text: My question is like unlike China we never had a consumer facing application coming out from India and in web 1, web 2, crypto and all.\n",
            "Chunk ID: 260, Length: 15.00s, Start: 1359.40s, End: 1374.40s, Text: Why do you think it will be different this time in like AI because will the DPI and other things will serve the same purpose what the great fire wall did in China or do you think like in because AI is a strategic sector no outside country can work in NASA projects, maybe all government contract will go to them, what is the moat here for an Indian company?\n",
            "Chunk ID: 261, Length: 14.16s, Start: 1374.40s, End: 1388.56s, Text: \n",
            "Chunk ID: 262, Length: 15.00s, Start: 1391.19s, End: 1406.19s, Text: So I think the question is, I don't know the answer to these questions, right, I mean I think that it's difficult to predict but I do believe and as I'm repeating that the combinatorial effect of using Gen AI at a large scale in addition along with the DPI work that we've done in India will have people and I think that in the end it is the intent is that people need to be able to use it and they will vote by things that are useful for them and if that doesn't happen you're right and I think that we have to figure out what is the mechanism of delivery of apps right?\n",
            "Chunk ID: 263, Length: 15.00s, Start: 1406.19s, End: 1421.19s, Text: \n",
            "Chunk ID: 264, Length: 12.11s, Start: 1421.19s, End: 1433.30s, Text: \n",
            "Chunk ID: 265, Length: 3.12s, Start: 1433.30s, End: 1436.42s, Text: Where do Indians consume contact.\n",
            "Chunk ID: 266, Length: 0.42s, Start: 1436.42s, End: 1436.84s, Text: That's the question.\n",
            "Chunk ID: 267, Length: 1.86s, Start: 1437.36s, End: 1439.22s, Text: I'm so sorry but we are out of time.\n",
            "Chunk ID: 268, Length: 3.98s, Start: 1439.22s, End: 1443.20s, Text: Vivek will be outside so he would be able to answer the question.\n",
            "Chunk ID: 269, Length: 1.40s, Start: 1443.20s, End: 1444.60s, Text: Do we have time for one last question?\n",
            "Chunk ID: 270, Length: 1.62s, Start: 1444.60s, End: 1446.22s, Text: Can I just take one last?\n",
            "Chunk ID: 271, Length: 0.23s, Start: 1446.22s, End: 1446.45s, Text: Yeah.\n",
            "Chunk ID: 272, Length: 0.41s, Start: 1447.13s, End: 1447.54s, Text: Thank you.\n",
            "Chunk ID: 273, Length: 0.44s, Start: 1447.54s, End: 1447.98s, Text: Thank you.\n",
            "Chunk ID: 274, Length: 1.02s, Start: 1447.98s, End: 1449.00s, Text: I'm Manish Kothari.\n",
            "Chunk ID: 275, Length: 1.66s, Start: 1449.00s, End: 1450.66s, Text: I'm from ISBR Business School.\n",
            "Chunk ID: 276, Length: 2.50s, Start: 1450.66s, End: 1453.16s, Text: Good that I got a chance to ask you this question.\n",
            "Chunk ID: 277, Length: 8.30s, Start: 1453.16s, End: 1461.46s, Text: During lunch time there were a few of our educationists whom we were talking about and there was one from school and we are from the MBA institutions.\n",
            "Chunk ID: 278, Length: 5.48s, Start: 1461.46s, End: 1466.94s, Text: We were thinking of these present generations how do we get them into what you are doing.\n",
            "Chunk ID: 279, Length: 12.57s, Start: 1467.51s, End: 1480.08s, Text: There is one thing that they have been regularly that the concentrations that they are working on but artificial intelligence and getting into this, getting them into their academics and making them a part of it is very important including the trainers who train them.\n",
            "Chunk ID: 280, Length: 15.00s, Start: 1480.08s, End: 1495.08s, Text: making them future ready into what you are doing is amazing and the speed that which is growing it is calling for a lot of training that needs to be done can you from your angle throw some light on how we could make them future ready how these people who are who are management graduates and from schools who are coming out how do we get into this part of technology that you spoke about So this is really a challenge because I think everyone will need to understand at some level what this technology does and I think that we have to rethink how we get everyone into these and this kind of education has to be at many different levels, right?\n",
            "Chunk ID: 281, Length: 15.00s, Start: 1495.08s, End: 1510.08s, Text: \n",
            "Chunk ID: 282, Length: 12.02s, Start: 1510.08s, End: 1522.10s, Text: \n",
            "Chunk ID: 283, Length: 15.00s, Start: 1522.10s, End: 1537.10s, Text: There are from a core set of having people who are extremely good at some and there you don't need as many but then there are basically vast numbers of people who can actually leverage By the way, the most important thing about, and maybe that's part of what makes an LLM interesting is that how you use it, your mileage varies by that.\n",
            "Chunk ID: 284, Length: 7.30s, Start: 1537.10s, End: 1544.40s, Text: \n",
            "Chunk ID: 285, Length: 15.00s, Start: 1544.40s, End: 1559.40s, Text: And to understand how to actually leverage this in an interesting way is something that we have to widely teach many, many people and because asking the things in the right way and having the right kind of applications will make a huge difference to how people can leverage these tools.\n",
            "Chunk ID: 286, Length: 3.72s, Start: 1559.40s, End: 1563.12s, Text: \n",
            "Chunk ID: 287, Length: 0.88s, Start: 1563.12s, End: 1564.00s, Text: Thank you.\n",
            "Chunk ID: 288, Length: 1.54s, Start: 1564.00s, End: 1565.54s, Text: Thank you very much Vivek.\n",
            "Chunk ID: 289, Length: 2.06s, Start: 1565.54s, End: 1567.60s, Text: Very good luck to Sarvam and good luck to India.\n",
            "Chunk ID: 290, Length: 1.74s, Start: 1567.60s, End: 1569.34s, Text: I think it's going to be a lot of .\n",
            "Chunk ID: 291, Length: 1.44s, Start: 1569.36s, End: 1570.80s, Text: Thanks Bala.\n",
            "Chunk ID: 292, Length: 3.42s, Start: 1570.80s, End: 1574.22s, Text: Thank you Mr.\n",
            "Chunk ID: 293, Length: 0.50s, Start: 1574.22s, End: 1574.72s, Text: Raghavan.\n",
            "Audio-text pairs saved to audio_text_pairs.txt\n"
          ]
        }
      ],
      "source": [
        "def combine_segments(vad_segments, text_segments, max_duration=15):\n",
        "    combined_pairs = []\n",
        "    chunk_id = 1\n",
        "\n",
        "    for text_segment in text_segments:\n",
        "        text_start = time_to_seconds(text_segment[\"start\"])\n",
        "        text_end = time_to_seconds(text_segment[\"end\"])\n",
        "\n",
        "        segment_vads = []\n",
        "        for vad_segment in vad_segments:\n",
        "            vad_start = time_to_seconds(vad_segment[\"start\"])\n",
        "            vad_end = time_to_seconds(vad_segment[\"end\"])\n",
        "\n",
        "            # Check if VAD segment overlaps with the text segment\n",
        "            if vad_start < text_end and vad_end > text_start:\n",
        "                segment_vads.append(vad_segment)\n",
        "\n",
        "        if not segment_vads:\n",
        "            continue\n",
        "\n",
        "        current_text = text_segment[\"text\"]\n",
        "        current_start_time = max(text_start, time_to_seconds(segment_vads[0][\"start\"]))\n",
        "        current_end_time = min(text_end, time_to_seconds(segment_vads[-1][\"end\"]))\n",
        "        current_duration = current_end_time - current_start_time\n",
        "\n",
        "        while current_duration > max_duration:\n",
        "            split_end_time = current_start_time + max_duration\n",
        "            combined_pairs.append({\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"chunk_length\": max_duration,\n",
        "                \"text\": current_text.strip(),\n",
        "                \"start_time\": current_start_time,\n",
        "                \"end_time\": split_end_time\n",
        "            })\n",
        "            chunk_id += 1\n",
        "            current_text = \"\"\n",
        "            current_start_time = split_end_time\n",
        "            current_duration = current_end_time - current_start_time\n",
        "\n",
        "        combined_pairs.append({\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"chunk_length\": current_duration,\n",
        "            \"text\": current_text.strip(),\n",
        "            \"start_time\": current_start_time,\n",
        "            \"end_time\": current_end_time\n",
        "        })\n",
        "        chunk_id += 1\n",
        "\n",
        "    return combined_pairs\n",
        "\n",
        "# Parse the VAD and text segments\n",
        "vad_segments = parse_vad_file(\"/content/vad.txt\")\n",
        "text_segments = parse_text_timestamps_file(\"/content/Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.txt\")\n",
        "\n",
        "# Combine the segments\n",
        "audio_text_pairs = combine_segments(vad_segments, text_segments)\n",
        "\n",
        "# Print the audio-text pairs\n",
        "for pair in audio_text_pairs:\n",
        "    print(f\"Chunk ID: {pair['chunk_id']}, Length: {pair['chunk_length']:.2f}s, Start: {pair['start_time']:.2f}s, End: {pair['end_time']:.2f}s, Text: {pair['text']}\")\n",
        "\n",
        "# Save the audio-text pairs to a file\n",
        "with open(\"audio_text_pairs.txt\", \"w\") as f:\n",
        "    for pair in audio_text_pairs:\n",
        "        f.write(f\"Chunk ID: {pair['chunk_id']}, Length: {pair['chunk_length']:.2f}s, Start: {pair['start_time']:.2f}s, End: {pair['end_time']:.2f}s, Text: {pair['text']}\\n\")\n",
        "\n",
        "print(\"Audio-text pairs saved to audio_text_pairs.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5-uEVS1TPHg",
        "outputId": "feb40240-5a6a-40d6-98bd-fe55f5689b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk ID: 1, Length: 13.70s, Start: 0.06s, End: 13.76s, Text: Congratulations to you Mr. Raghavan for that. Thank you so much for joining us. Over to you. Hi everybody. How are you? I am not hearing this at all.\n",
            "Chunk ID: 2, Length: 13.90s, Start: 13.76s, End: 27.66s, Text: It's like a post lunch energy downer or something. Let's hear it. Are you guys awake? Alright. You better be because we have a superstar guest here.\n",
            "Chunk ID: 3, Length: 13.58s, Start: 27.66s, End: 41.24s, Text: You heard the $41 million and I didn't hear honestly anything she said after that. So we're going to ask for about $40 million from him by the end of this conversation, okay? But let's get started.\n",
            "Chunk ID: 4, Length: 14.24s, Start: 41.24s, End: 55.48s, Text: I want to introduce Vivek and Pratyush, his co-founder who's not here. We wanted to start with playing a video of what OpenHearty does. I encourage all of you to go to the website serverum. ai and check it out.\n",
            "Chunk ID: 5, Length: 15.00s, Start: 56.17s, End: 71.17s, Text: but let me start by introducing Vivek Vivek is a dear friend and he is very very modest one of the most modest guys that I know but his personal journey Vivek you got a PhD from Carnegie Mellon, you started and sold a company to Magma and Vivek and I moved back to India we were both in the valley on the same day actually and you have been in India for the last 16 years and what most people don't know is your journey at Aadhaar He spent 13 years selflessly at Aadhaar.\n",
            "Chunk ID: 6, Length: 12.96s, Start: 88.30s, End: 101.26s, Text: Nobody would have heard of him, but he was a pioneering technology visionary behind Aadhaar, which we all take for granted today. So please give it out.\n",
            "Chunk ID: 7, Length: 13.74s, Start: 101.26s, End: 115.00s, Text: Honestly, when I think of selfless service, truly selfless service, I always think of Vivek. And since then, he also was at AI for Bharat, which we're going to touch on, where he met Pratyush's other co-founder.\n",
            "Chunk ID: 8, Length: 13.70s, Start: 115.00s, End: 128.70s, Text: Pratyush had a PhD from ETH at Zurich. He was in the IBM research. He was at Microsoft research playing a key role and a faculty at IIT Madras and at AF Abharat. So that's a little brief introduction about them.\n",
            "Chunk ID: 9, Length: 13.57s, Start: 128.70s, End: 142.27s, Text: These guys are modest, modest engineers. So they don't toot their own horn. So forgive me for tooting their horn in this case. But let's jump right in about the money, funding.\n",
            "Chunk ID: 10, Length: 9.05s, Start: 142.62s, End: 151.67s, Text: 41 million bucks, man, that's a lot of money, right? Every entrepreneur here is saying, what the hell did these guys do? What did the investors see to write such a big check?\n",
            "Chunk ID: 11, Length: 5.54s, Start: 152.60s, End: 158.14s, Text: No, I think it's a new trend of what's going on in India.\n",
            "Chunk ID: 12, Length: 12.76s, Start: 158.14s, End: 170.90s, Text: I think that for the very first time, I think the investors have looked at, you know, let's try and build something deep tech out of the country and let's try to figure out how to build something as a foundational technology out of the country.\n",
            "Chunk ID: 13, Length: 2.50s, Start: 170.90s, End: 173.40s, Text: And that's really what's really exciting.\n",
            "Chunk ID: 14, Length: 15.00s, Start: 173.40s, End: 188.40s, Text: And I think that about, as Bala was mentioning, for the past 15 years, I've been kind of working in both digital public infrastructure and kind of non-profit kind of things.\n",
            "Chunk ID: 15, Length: 8.48s, Start: 190.68s, End: 199.16s, Text: But when this whole thing of generative AI came about, we said, okay, how can I actually make a difference in this space?\n",
            "Chunk ID: 16, Length: 13.54s, Start: 199.16s, End: 212.70s, Text: And I said, maybe this is the opportunity to actually come out and really build something. And the only way that we realized that you can do it is actually in the private sector.\n",
            "Chunk ID: 17, Length: 10.02s, Start: 212.70s, End: 222.72s, Text: And then we went out there and we said we want to build something which is a continuation. And fundamentally the question is, the reason of what we want to do at Servam.\n",
            "Chunk ID: 18, Length: 13.68s, Start: 222.72s, End: 236.40s, Text: ai is we want to basically make generative AI available and accessible to the people in the country. And that's the intent. And when we said that we want to do this, there was a resonance in the investment community.\n",
            "Chunk ID: 19, Length: 10.36s, Start: 236.40s, End: 246.76s, Text: And I think it's a responsibility to really to show that something like this can be built out of India. So we see that as confidence and a responsibility.\n",
            "Chunk ID: 20, Length: 15.00s, Start: 246.76s, End: 261.76s, Text: And I also hope it's a trend that there are many more people like us who are backed because if you look at it, maybe it's a large number in a, you know, in the Indian context but in the global context, I think there is just, there should be many, many more entrepreneurs who are backed to do things in India.\n",
            "Chunk ID: 21, Length: 5.26s, Start: 265.86s, End: 271.12s, Text: I'm going to come back to the many more entrepreneurs. I'm obviously going to ask you about Bhavish's Krutrim.\n",
            "Chunk ID: 22, Length: 15.00s, Start: 271.12s, End: 286.12s, Text: So we're going to come back to that question but again, 41 million dollars, I mean all of what you said, you know, 2 million dollars, you know, that's a good amount of money for of startup which, you know, which has not yet built anything, what are you going to do with all this money?\n",
            "Chunk ID: 23, Length: 12.03s, Start: 288.91s, End: 300.94s, Text: I can solve the problem. I can have a perfect solution for the problem. I think in the last week I've got lots of calls from lots of people telling me how I can do it. I know you first, okay? I'll be landed in the country the same day.\n",
            "Chunk ID: 24, Length: 8.58s, Start: 300.94s, End: 309.52s, Text: I'm in front of the queue. But honestly, I think the key thing in this is to putting together an amazing team.\n",
            "Chunk ID: 25, Length: 10.18s, Start: 309.52s, End: 319.70s, Text: and we actually have an amazing team but we believe that it is talent that will drive this kind of thing and so it is to get key talent and of course the other thing is compute.\n",
            "Chunk ID: 26, Length: 11.32s, Start: 319.70s, End: 331.02s, Text: This is extremely expensive compute wise to actually do these kinds of things and I think that those are the two primary things that we would use this for. Okay.\n",
            "Chunk ID: 27, Length: 14.06s, Start: 331.04s, End: 345.10s, Text: I'm computing in my own head as an entrepreneur. Talent, okay, you have like 20, 15 people. How much are you paying these guys? but okay, we won't touch on that. But let's talk about what you guys actually built. What is OpenHathi?\n",
            "Chunk ID: 28, Length: 3.67s, Start: 345.10s, End: 348.77s, Text: How would you explain OpenHathi to many people here who might not have known about it?\n",
            "Chunk ID: 29, Length: 10.91s, Start: 349.29s, End: 360.20s, Text: So I think OpenHathi is, so first of all, right, we come from, I personally come from the open source ecosystem and also from the DPI ecosystem.\n",
            "Chunk ID: 30, Length: 12.52s, Start: 360.20s, End: 372.72s, Text: So we believe that for this to work, we need the ecosystem to be successful. And as a result of that, one of the first things we did was hey, there are these open source large language models that exist, right?\n",
            "Chunk ID: 31, Length: 10.66s, Start: 372.72s, End: 383.38s, Text: I mean, everybody knows about the Llama family from Meta. There are others like Mistral. There are a bunch of open source large language models.\n",
            "Chunk ID: 32, Length: 13.90s, Start: 383.38s, End: 397.28s, Text: And then we said, is there any way that take an existing open source model and teach it language skills, right? And that is really what we said, that can we do something like that?\n",
            "Chunk ID: 33, Length: 10.20s, Start: 397.28s, End: 407.48s, Text: And is this a, you know, relatively frugal way of actually, you know, making models, you know, work in diverse languages?\n",
            "Chunk ID: 34, Length: 7.84s, Start: 407.48s, End: 415.32s, Text: Because the truth is still today, I mean, if you look at the amount of data and knowledge, it is still English dominates these things.\n",
            "Chunk ID: 35, Length: 12.18s, Start: 415.32s, End: 427.50s, Text: And I think that how do you actually take and make it understand Indian language, understand Indian context, and all of those things in actually an efficient way. And therefore, this was an attempt to do that.\n",
            "Chunk ID: 36, Length: 14.78s, Start: 427.50s, End: 442.28s, Text: And OpenHathi is currently based on the Llama 7 billion model, but we'll be releasing many more models in different languages, different sizes, and things like that as part of this series.\n",
            "Chunk ID: 37, Length: 13.06s, Start: 442.28s, End: 455.34s, Text: And of course, we will be building further models on those and doing other things to actually... And we'll also have endpoints that people can use. So therefore, it's definitely something that people can use to things.\n",
            "Chunk ID: 38, Length: 14.48s, Start: 455.34s, End: 469.82s, Text: And that's the essence of what this OpenHathi is. So what does it mean to people in the audience here who are either doing their own startups or a business or developers? How should they look at OpenAI?\n",
            "Chunk ID: 39, Length: 11.10s, Start: 469.82s, End: 480.92s, Text: Sorry, Sarman. Not OpenAI. No, I think the way you look at it is that one of the important things that we are doing is we're not just building models.\n",
            "Chunk ID: 40, Length: 15.00s, Start: 480.92s, End: 495.92s, Text: We are also going to be building a platform, a platform for developers where you can actually use a combination of various different kinds of models, some which are from us, some which are open source, some which may not be open source, and actually to actually pull together and figure out how to deploy, you know, generative AI applications at scale and understand and evaluate their performance in an efficient manner.\n",
            "Chunk ID: 41, Length: 9.04s, Start: 506.50s, End: 515.54s, Text: And that's something that we're planning to do. And this platform is, you know, in the next couple of months will be coming out there. It will be available to developers.\n",
            "Chunk ID: 42, Length: 12.88s, Start: 515.54s, End: 528.42s, Text: But, of course, those who want to start with the open source things and hack for that, of course, please go ahead and do that as well. That's phenomenal. But how does it compare to OpenAI itself or Google?\n",
            "Chunk ID: 43, Length: 11.04s, Start: 530.26s, End: 541.30s, Text: See, at least the things that we are doing now, right? I mean, one of the things that when we thought about building Sarvam, we said we want to build a full stack generative AI company.\n",
            "Chunk ID: 44, Length: 12.86s, Start: 541.30s, End: 554.16s, Text: And different people are understanding of a stack is that we need to know how to train models from scratch. We need to know how to kind of figure out how to deploy models to solve real world use cases.\n",
            "Chunk ID: 45, Length: 10.88s, Start: 554.16s, End: 565.04s, Text: And we need to play in the ecosystem to make sure that we can actually deploy population scale applications. So we were thinking about all of these things.\n",
            "Chunk ID: 46, Length: 9.94s, Start: 565.04s, End: 574.98s, Text: But still the models we were talking about are fairly small models. They are fairly small models, right? The 7 to maybe up to 70 billion kind of range we're talking about.\n",
            "Chunk ID: 47, Length: 14.96s, Start: 574.98s, End: 589.94s, Text: While these models like OpenAI and Google are obviously much bigger models. But we want to understand the techniques and be able to build that muscle to do all of these things to make it available to people.\n",
            "Chunk ID: 48, Length: 6.38s, Start: 589.94s, End: 596.32s, Text: Now, those models are, I mean, as I said, you know, I think that there is space for all of those things.\n",
            "Chunk ID: 49, Length: 15.00s, Start: 596.32s, End: 611.32s, Text: And I think as even Sridhar was talking about earlier in the day, we believe that these smaller models can do very, I mean, many, many kind of domain specific tasks extremely well, probably even better than the larger models.\n",
            "Chunk ID: 50, Length: 11.08s, Start: 612.54s, End: 623.62s, Text: And that is really one of the key areas. And so, therefore, the value of these kinds of things, right? We are not aiming in these set of models to build any AGI, right? That's not our goal here.\n",
            "Chunk ID: 51, Length: 13.00s, Start: 623.62s, End: 636.62s, Text: Our goal is to make things that work extremely well for domain-specific use cases or increase accessibility through language and all of those kinds of things. And obviously all of this is unique to India. But what is unique about India?\n",
            "Chunk ID: 52, Length: 10.53s, Start: 636.62s, End: 647.15s, Text: I mean, is there anything special in our ecosystem that makes small models focused with Indian languages better for, more suited for our problems?\n",
            "Chunk ID: 53, Length: 13.93s, Start: 648.03s, End: 661.96s, Text: So I think that, I mean, there are quite a few things that are unique about India, right? The first thing is, I think, that we are a voice-first nation, so therefore I think voice has to be the core to doing things.\n",
            "Chunk ID: 54, Length: 7.74s, Start: 661.96s, End: 669.70s, Text: The other thing, of course, India is extremely, it's a cost-conscious country from a cost perspective.\n",
            "Chunk ID: 55, Length: 9.14s, Start: 669.70s, End: 678.84s, Text: Now, I would say that there are lots of interesting use cases where you can use OpenAI and the cost structure works depending on your application.\n",
            "Chunk ID: 56, Length: 9.60s, Start: 678.84s, End: 688.44s, Text: But when you want to scale things to a massive level and make it work, then you have to figure out how small models work. So that's something that is also specific to India.\n",
            "Chunk ID: 57, Length: 7.50s, Start: 688.44s, End: 695.94s, Text: The third thing which is specific to India is really the success that India has had in building all this digital public infrastructure.\n",
            "Chunk ID: 58, Length: 11.80s, Start: 695.94s, End: 707.74s, Text: When you add the AI layer on top of it, then you can actually get dramatic, you know dramatic I think multiplicative combinatorial effects based on doing things like that.\n",
            "Chunk ID: 59, Length: 8.98s, Start: 707.74s, End: 716.72s, Text: That's a phenomenal point like you know it's like DPI to the power of AI almost in some ways and as a part of Aadhaar building Aadhaar no better person than you.\n",
            "Chunk ID: 60, Length: 12.16s, Start: 716.72s, End: 728.88s, Text: So in summary what I'm hearing is small models specialized with trained with Indic specific language data suited for Indian problems at a compelling cost point will be suited for us.\n",
            "Chunk ID: 61, Length: 12.70s, Start: 728.88s, End: 741.58s, Text: We're not solving some world autonomous vehicles or some complex problem. We're solving some basic problems specifically focused on voice with multiple languages. That is what you see as the future. Am I paraphrasing this correctly?\n",
            "Chunk ID: 62, Length: 11.44s, Start: 741.58s, End: 753.02s, Text: I think that certainly voice and Indian languages are an important part of our strategy but we will be building custom models to solve various other kinds of problems as well.\n",
            "Chunk ID: 63, Length: 12.98s, Start: 753.02s, End: 766.00s, Text: It's not just limited to I think in different domains, working in different domains making, building things based on unique data that enterprises have and things like that. So that's something that we'll also look at. Fair enough.\n",
            "Chunk ID: 64, Length: 12.20s, Start: 766.00s, End: 778.20s, Text: So coming back to the elephant in the room, no fun intended with OpenHathi, what about Bhave Shagriwal and Krutrim? What is your take on that? I think it's great. I think it's wonderful, right?\n",
            "Chunk ID: 65, Length: 13.74s, Start: 778.20s, End: 791.94s, Text: I mean, the fact that the technology AI is so important that we need multiple people working on it. The fact that there are other people thinking actually validates that this is an important problem to be solved.\n",
            "Chunk ID: 66, Length: 14.58s, Start: 791.94s, End: 806.52s, Text: And I think that we need everybody to come together and do that. So I really welcome that. I think it's great. And I think that there will be different people who will have different takes as to how to solve this kind of problem.\n",
            "Chunk ID: 67, Length: 14.32s, Start: 806.52s, End: 820.84s, Text: And hopefully as a result of that, the entire ecosystem benefits. One more question and then I want to talk about some of the predictions that you've boldly made. So Vivek, I usually ask people about what do you think the future will be and everybody usually hedges.\n",
            "Chunk ID: 68, Length: 12.54s, Start: 820.84s, End: 833.38s, Text: I asked Vivek, what do you think is going to happen by December 2024? What do you think sitting in this room one year later we can expect? And he made three bold predictions. So I want to talk about that. Before that, I have one last question.\n",
            "Chunk ID: 69, Length: 11.18s, Start: 833.38s, End: 844.56s, Text: What are the top three applications that you think are relevant for India? You heard Sridhar talk about medical. Any quick summary? what do you think the top three apps are for India for AI?\n",
            "Chunk ID: 70, Length: 10.13s, Start: 845.05s, End: 855.18s, Text: So, I mean, I think that, as you said, things like education and medical are clearly areas where I think that things can be leveraged.\n",
            "Chunk ID: 71, Length: 14.80s, Start: 855.18s, End: 869.98s, Text: The whole idea of all these kind of... The DPI aspect of it is another major application where things can happen. And here I'm talking about country-specific work. And I think the whole idea which Sridhar also talked about was the concept of software, right?\n",
            "Chunk ID: 72, Length: 13.86s, Start: 869.98s, End: 883.84s, Text: and I think that and clearly we have a very large software industry and how to reimagine those things in this context is also something that's going to be. Fair enough. Are you guys ready for Vivek Raghavan's bold predictions?\n",
            "Chunk ID: 73, Length: 14.84s, Start: 884.58s, End: 899.42s, Text: Yes? No? I'm not hearing any yes. This is like a big deal. He's like one of the smartest guys that I know. He wants to make three predictions. You don't want to hear it? All right. So I asked him what do you think a year later what do you think we can expect?\n",
            "Chunk ID: 74, Length: 13.92s, Start: 899.42s, End: 913.34s, Text: and he came up with three things and usually people give very blah answers when you ask a question like this because they don't want to be caught wrong. Not Vivek. Vivek is bold. So he basically said three things and I'm going to list out the three things and then ask him about it.\n",
            "Chunk ID: 75, Length: 10.78s, Start: 913.34s, End: 924.12s, Text: So number one, he says, I would prefer to talk to an automated customer service than a real person because they'll give me a better answer. So that is Vivek Raghavan's prediction number one.\n",
            "Chunk ID: 76, Length: 14.22s, Start: 924.12s, End: 938.34s, Text: So number two is that when everybody is talking about a GPU shortage, Vivek predicts that there will be a GPU glut in India. He thinks there will be too much GPU. So if you want a short NVIDIA stock, this is a good time.\n",
            "Chunk ID: 77, Length: 10.42s, Start: 938.34s, End: 948.76s, Text: And number three, which was extremely unexpected, he said some companies will suddenly die. So Vivek, these are not what I expected.\n",
            "Chunk ID: 78, Length: 14.22s, Start: 948.76s, End: 962.98s, Text: So do you want to quickly talk about each of them, why you just came up with these, and then we'll throw the open for audience questions. So I don't think I quite said it the way that Pala was going to say it.\n",
            "Chunk ID: 79, Length: 0.88s, Start: 962.98s, End: 963.86s, Text: But it's interesting.\n",
            "Chunk ID: 80, Length: 15.00s, Start: 963.86s, End: 978.86s, Text: But I think the first thing that we said is I think that, and I don't think that this is, I think there will come a time when, you know, in areas of customer service, et cetera, when you want to do something very specific.\n",
            "Chunk ID: 81, Length: 10.50s, Start: 980.58s, End: 991.08s, Text: Today, you know, when you call some kind of a bot, you actually end up, you mostly try to disconnect the call or you're extremely upset that you're talking to a bot.\n",
            "Chunk ID: 82, Length: 14.70s, Start: 991.08s, End: 1005.78s, Text: But I think that there will come a time, and I'm predicting it is sooner than later, that you will actually get better responses from the bot than what the human representative, at least the average human representative that you could talk to could give.\n",
            "Chunk ID: 83, Length: 12.12s, Start: 1005.78s, End: 1017.90s, Text: And I think that that's just a, so I just said that there will come a time where you know it's not a human you're talking to, but it's probably more likely to solve your intent than the human person.\n",
            "Chunk ID: 84, Length: 10.04s, Start: 1017.90s, End: 1027.94s, Text: That's just something that I think that could happen. Okay, definitely controversial, but we'll let it go. What about the GPU glut?\n",
            "Chunk ID: 85, Length: 11.88s, Start: 1027.94s, End: 1039.82s, Text: No, no, yeah, so I don't think that, so I think that the fact that there is a tremendous shortage right now, I think that shortage will ease because that is how the cycles of things go, right?\n",
            "Chunk ID: 86, Length: 14.68s, Start: 1039.82s, End: 1054.50s, Text: when I think the fact that there was such a severe shortage last year basically caused a number of different players to ramp up in various kinds of forms. And I think that that will always go in a cycle.\n",
            "Chunk ID: 87, Length: 6.10s, Start: 1054.50s, End: 1060.60s, Text: But we may find out that there are many, many more interesting problems that people will be able to solve.\n",
            "Chunk ID: 88, Length: 15.00s, Start: 1060.60s, End: 1075.60s, Text: I still remember we were at a Gen AI event in Bangalore and we were talking to people and we said, you know, how many people have access to, you know, four A100s, this was the question that I'd asked, and nobody in the room, and these are all extremely enthusiastic Gen AI people, and nobody had access.\n",
            "Chunk ID: 89, Length: 13.40s, Start: 1081.58s, End: 1094.98s, Text: And I think that thing is going to change. You will be able to get these kinds of things and people who want to hack and do things will have access to these things without, you know, having to write a, you know, a major check.\n",
            "Chunk ID: 90, Length: 11.68s, Start: 1094.98s, End: 1106.66s, Text: Vivek is also a semiconductor guy before he went into Aadhaar, So I would take his predictions very seriously. So I don't know what I'm going to sell my media stock. I would not do that. But that's not what I said.\n",
            "Chunk ID: 91, Length: 12.18s, Start: 1106.66s, End: 1118.84s, Text: I want to blame you for this if it goes up. But the third one is pretty strange. You know, companies are born, companies die. But you said some companies will suddenly die. What does that mean?\n",
            "Chunk ID: 92, Length: 11.52s, Start: 1119.06s, End: 1130.58s, Text: No, I think, see, I think the interesting thing is, and I think that it comes back to the fundamental nature of AI. AI is a tool, right? And you have to use that.\n",
            "Chunk ID: 93, Length: 6.04s, Start: 1130.58s, End: 1136.62s, Text: And you have to use that within your business process, right? And how AI is being used.\n",
            "Chunk ID: 94, Length: 14.94s, Start: 1136.62s, End: 1151.56s, Text: And what's going to happen is that, I mean, I think this is true with, you know, when someone said in terms of, you know, people, they said that the people who leverage AI will be more effective than those who don't leverage AI.\n",
            "Chunk ID: 95, Length: 12.38s, Start: 1151.56s, End: 1163.94s, Text: And that is true for organizations also. So organizations that leverage AI fundamentally in their core business processes will be more effective than those who don't. And I think that's the thing.\n",
            "Chunk ID: 96, Length: 12.82s, Start: 1163.94s, End: 1176.76s, Text: And you won't know the difference until one day it becomes too obvious and it will be too late. And I think that's the reason why everybody needs to think about what it means for your business.\n",
            "Chunk ID: 97, Length: 14.26s, Start: 1176.76s, End: 1191.02s, Text: Because everything will be fine. Everything will be fine and one day somebody in your, either your competitor in your space or somebody brand new coming into your space will be reimagining your business process completely.\n",
            "Chunk ID: 98, Length: 7.82s, Start: 1191.02s, End: 1198.84s, Text: And at that stage you will find that it's a very big, very tall mountain to climb.\n",
            "Chunk ID: 99, Length: 10.50s, Start: 1198.84s, End: 1209.34s, Text: and that's why I think it's important for both people and entities to think about how they will upgrade themselves or they will modify their business processes.\n",
            "Chunk ID: 100, Length: 11.86s, Start: 1209.34s, End: 1221.20s, Text: That's a very nuanced answer and everybody here who's running a business should really think about it because life will be the same and then suddenly something will you know, then it will be a step change.\n",
            "Chunk ID: 101, Length: 13.88s, Start: 1221.20s, End: 1235.08s, Text: Vivek, I have a few more questions but I'm sure the audience has a lot of questions for you. So, how are we doing on time? Okay. So, does, okay, a lot of questions, so love to, is there a mic that we can pass around?\n",
            "Chunk ID: 102, Length: 14.28s, Start: 1240.58s, End: 1254.86s, Text: Thank you. My name is Karthik. I work for IT service industry. So you're saying that you're working on LLM, sorry, it's a fine-tuned LLM on top of Llama.\n",
            "Chunk ID: 103, Length: 10.04s, Start: 1254.86s, End: 1264.90s, Text: My basic question, fundamental question is we don't have a foundational model for India. Most of the models are basically using English or those kind of things.\n",
            "Chunk ID: 104, Length: 13.82s, Start: 1264.90s, End: 1278.72s, Text: For example, Andrew was talking about the tokenizers and things like that. So are you working on anything like that or do you want to use mostly the existing models and run on top of it? You asked a good question.\n",
            "Chunk ID: 105, Length: 1.98s, Start: 1278.72s, End: 1280.70s, Text: You asked a cherry question for himself.\n",
            "Chunk ID: 106, Length: 15.00s, Start: 1280.70s, End: 1295.70s, Text: No, I think the interesting thing is that if you look at, and we have actually a blog on this on our website, I think one of the things that we've done is we've actually built a customized tokenizer, which actually fundamentally changes the cost of some of these generations in Indian languages.\n",
            "Chunk ID: 107, Length: 15.00s, Start: 1298.56s, End: 1313.56s, Text: and I think that we are not just fine tuning, we are actually, we are leveraging the existing pre-training but we are doing what's known as continual pre-training which actually, but having said that, you know, I think that when we have to figure out where is the data to train an extremely large model from scratch and some of those things are things which will happen over time but I think that, I think that, yes, I think that we will be doing various kinds of things but the interesting thing is that if I want to change the accessibility problem with an existing open source model, how do I do that and that's the problem that we have that we think we have solved and it's going to be the heart of this OpenRTC series.\n",
            "Chunk ID: 108, Length: 7.78s, Start: 1340.42s, End: 1348.20s, Text: It's extremely well explained in the blog even I could understand it so. Hi, I'm Prashant, I work for a Fintech company.\n",
            "Chunk ID: 109, Length: 10.67s, Start: 1348.20s, End: 1358.87s, Text: My question is like unlike China we never had a consumer facing application coming out from India and in web 1, web 2, crypto and all.\n",
            "Chunk ID: 110, Length: 15.00s, Start: 1359.40s, End: 1374.40s, Text: Why do you think it will be different this time in like AI because will the DPI and other things will serve the same purpose what the great fire wall did in China or do you think like in because AI is a strategic sector no outside country can work in NASA projects, maybe all government contract will go to them, what is the moat here for an Indian company?\n",
            "Chunk ID: 111, Length: 15.00s, Start: 1391.19s, End: 1406.19s, Text: So I think the question is, I don't know the answer to these questions, right, I mean I think that it's difficult to predict but I do believe and as I'm repeating that the combinatorial effect of using Gen AI at a large scale in addition along with the DPI work that we've done in India will have people and I think that in the end it is the intent is that people need to be able to use it and they will vote by things that are useful for them and if that doesn't happen you're right and I think that we have to figure out what is the mechanism of delivery of apps right?\n",
            "Chunk ID: 112, Length: 14.68s, Start: 1433.30s, End: 1447.98s, Text: Where do Indians consume contact. That's the question. I'm so sorry but we are out of time. Vivek will be outside so he would be able to answer the question. Do we have time for one last question? Can I just take one last? Yeah. Thank you. Thank you.\n",
            "Chunk ID: 113, Length: 13.48s, Start: 1447.98s, End: 1461.46s, Text: I'm Manish Kothari. I'm from ISBR Business School. Good that I got a chance to ask you this question. During lunch time there were a few of our educationists whom we were talking about and there was one from school and we are from the MBA institutions.\n",
            "Chunk ID: 114, Length: 5.48s, Start: 1461.46s, End: 1466.94s, Text: We were thinking of these present generations how do we get them into what you are doing.\n",
            "Chunk ID: 115, Length: 12.57s, Start: 1467.51s, End: 1480.08s, Text: There is one thing that they have been regularly that the concentrations that they are working on but artificial intelligence and getting into this, getting them into their academics and making them a part of it is very important including the trainers who train them.\n",
            "Chunk ID: 116, Length: 15.00s, Start: 1480.08s, End: 1495.08s, Text: making them future ready into what you are doing is amazing and the speed that which is growing it is calling for a lot of training that needs to be done can you from your angle throw some light on how we could make them future ready how these people who are who are management graduates and from schools who are coming out how do we get into this part of technology that you spoke about So this is really a challenge because I think everyone will need to understand at some level what this technology does and I think that we have to rethink how we get everyone into these and this kind of education has to be at many different levels, right?\n",
            "Chunk ID: 117, Length: 15.00s, Start: 1522.10s, End: 1537.10s, Text: There are from a core set of having people who are extremely good at some and there you don't need as many but then there are basically vast numbers of people who can actually leverage By the way, the most important thing about, and maybe that's part of what makes an LLM interesting is that how you use it, your mileage varies by that.\n",
            "Chunk ID: 118, Length: 15.00s, Start: 1544.40s, End: 1559.40s, Text: And to understand how to actually leverage this in an interesting way is something that we have to widely teach many, many people and because asking the things in the right way and having the right kind of applications will make a huge difference to how people can leverage these tools.\n",
            "Chunk ID: 119, Length: 11.60s, Start: 1563.12s, End: 1574.72s, Text: Thank you. Thank you very much Vivek. Very good luck to Sarvam and good luck to India. I think it's going to be a lot of . Thanks Bala. Thank you Mr. Raghavan.\n",
            "Combined chunks saved to combined_audio_text_pairs.txt\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def parse_chunk_from_line(line):\n",
        "    match = re.match(r'Chunk ID: (\\d+), Length: ([\\d.]+)s, Start: ([\\d.]+)s, End: ([\\d.]+)s, Text: (.+)', line)\n",
        "    if match:\n",
        "        chunk_id = int(match.group(1))\n",
        "        chunk_length = float(match.group(2))\n",
        "        start_time = float(match.group(3))\n",
        "        end_time = float(match.group(4))\n",
        "        text = match.group(5)\n",
        "        return {\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"chunk_length\": chunk_length,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "            \"text\": text\n",
        "        }\n",
        "    return None\n",
        "\n",
        "def combine_chunks(chunks, max_duration=15):\n",
        "    combined_chunks = []\n",
        "    current_chunk = None\n",
        "\n",
        "    for chunk in chunks:\n",
        "        chunk_text = chunk[\"text\"]\n",
        "        chunk_start = chunk[\"start_time\"]\n",
        "        chunk_end = chunk[\"end_time\"]\n",
        "        chunk_duration = chunk_end - chunk_start\n",
        "\n",
        "        if current_chunk is None:\n",
        "            current_chunk = {\n",
        "                \"chunk_id\": len(combined_chunks) + 1,\n",
        "                \"text\": chunk_text,\n",
        "                \"start_time\": chunk_start,\n",
        "                \"end_time\": chunk_end,\n",
        "                \"chunk_length\": chunk_duration\n",
        "            }\n",
        "        else:\n",
        "            combined_text = current_chunk[\"text\"] + \" \" + chunk_text\n",
        "            combined_duration = chunk_end - current_chunk[\"start_time\"]\n",
        "\n",
        "            if combined_duration <= max_duration:\n",
        "                current_chunk[\"text\"] = combined_text\n",
        "                current_chunk[\"end_time\"] = chunk_end\n",
        "                current_chunk[\"chunk_length\"] = combined_duration\n",
        "            else:\n",
        "                combined_chunks.append(current_chunk)\n",
        "                current_chunk = {\n",
        "                    \"chunk_id\": len(combined_chunks) + 1,\n",
        "                    \"text\": chunk_text,\n",
        "                    \"start_time\": chunk_start,\n",
        "                    \"end_time\": chunk_end,\n",
        "                    \"chunk_length\": chunk_duration\n",
        "                }\n",
        "\n",
        "    if current_chunk is not None:\n",
        "        combined_chunks.append(current_chunk)\n",
        "\n",
        "    return combined_chunks\n",
        "\n",
        "# Read chunks from the file\n",
        "chunks = []\n",
        "with open(\"audio_text_pairs.txt\", \"r\") as file:\n",
        "    for line in file:\n",
        "        chunk = parse_chunk_from_line(line)\n",
        "        if chunk is not None:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "# Combine the chunks\n",
        "combined_chunks = combine_chunks(chunks)\n",
        "\n",
        "# Print and store the combined chunks\n",
        "with open(\"combined_audio_text_pairs.txt\", \"w\") as outfile:\n",
        "    for chunk in combined_chunks:\n",
        "        output_line = f\"Chunk ID: {chunk['chunk_id']}, Length: {chunk['chunk_length']:.2f}s, Start: {chunk['start_time']:.2f}s, End: {chunk['end_time']:.2f}s, Text: {chunk['text']}\\n\"\n",
        "        print(output_line, end=\"\")\n",
        "        outfile.write(output_line)\n",
        "\n",
        "print(\"Combined chunks saved to combined_audio_text_pairs.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgpkMu9WTZyv",
        "outputId": "bf1951df-2647-4a83-9b1d-3230429c245e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined chunks saved to combined_chunks.json\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def parse_chunk_from_line(line):\n",
        "    match = re.match(r'Chunk ID: (\\d+), Length: ([\\d.]+)s, Start: ([\\d.]+)s, End: ([\\d.]+)s, Text: (.+)', line)\n",
        "    if match:\n",
        "        chunk_id = int(match.group(1))\n",
        "        chunk_length = float(match.group(2))\n",
        "        start_time = float(match.group(3))\n",
        "        end_time = float(match.group(4))\n",
        "        text = match.group(5)\n",
        "        return {\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"chunk_length\": chunk_length,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "            \"text\": text\n",
        "        }\n",
        "    return None\n",
        "\n",
        "def combine_chunks(chunks, max_duration=15):\n",
        "    combined_chunks = []\n",
        "    current_combined = None\n",
        "\n",
        "    for chunk in chunks:\n",
        "        if current_combined is None:\n",
        "            current_combined = chunk.copy()\n",
        "        else:\n",
        "            if (current_combined['chunk_length'] + chunk['chunk_length']) <= max_duration:\n",
        "                current_combined['text'] += \" \" + chunk['text']\n",
        "                current_combined['end_time'] = chunk['end_time']\n",
        "                current_combined['chunk_length'] = current_combined['end_time'] - current_combined['start_time']\n",
        "            else:\n",
        "                combined_chunks.append(current_combined)\n",
        "                current_combined = chunk.copy()\n",
        "\n",
        "    if current_combined is not None:\n",
        "        combined_chunks.append(current_combined)\n",
        "\n",
        "    return combined_chunks\n",
        "\n",
        "# Read chunks from the file\n",
        "chunks = []\n",
        "input_file_path = \"audio_text_pairs.txt\"\n",
        "with open(input_file_path, \"r\") as file:\n",
        "    for line in file:\n",
        "        chunk = parse_chunk_from_line(line)\n",
        "        if chunk is not None:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "# Combine the chunks\n",
        "combined_chunks = combine_chunks(chunks)\n",
        "\n",
        "# Save combined chunks to a JSON file\n",
        "output_file_path = \"combined_chunks.json\"\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    json.dump(combined_chunks, output_file, indent=4)\n",
        "\n",
        "print(f\"Combined chunks saved to {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "VBB3UjDzTdgb",
        "outputId": "c440ffa3-3172-41e6-b67e-6be52b8fa309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e5866e8b0e16006271.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e5866e8b0e16006271.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import subprocess\n",
        "import whisper\n",
        "import gradio as gr\n",
        "from datetime import timedelta\n",
        "from pytube import YouTube\n",
        "from pyannote.audio.pipelines import VoiceActivityDetection\n",
        "from pyannote.core import Segment\n",
        "from pyannote.audio import Model, Inference\n",
        "\n",
        "def video2mp3(video_file, output_ext=\"wav\"):\n",
        "    filename, ext = os.path.splitext(video_file)\n",
        "    audio_file = f\"{filename}.{output_ext}\"\n",
        "    print(f\"Converting {video_file} to {audio_file}...\")\n",
        "    subprocess.call([\"ffmpeg\", \"-y\", \"-i\", video_file, audio_file],\n",
        "                    stdout=subprocess.DEVNULL,\n",
        "                    stderr=subprocess.STDOUT)\n",
        "    if os.path.exists(audio_file):\n",
        "        print(f\"Audio file {audio_file} created successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to create audio file {audio_file}.\")\n",
        "    return audio_file\n",
        "\n",
        "def download_video(url):\n",
        "    yt = YouTube(url)\n",
        "    video = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
        "    video_file = video.download()\n",
        "    if os.path.exists(video_file):\n",
        "        print(f\"Video file {video_file} downloaded successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to download video from {url}.\")\n",
        "    return video_file\n",
        "\n",
        "def transcribe(audio_file):\n",
        "    print(f\"Loading Whisper model and transcribing {audio_file}...\")\n",
        "    model = whisper.load_model(\"large-v3\")\n",
        "    result = model.transcribe(audio_file)\n",
        "    print(f\"Transcription completed.\")\n",
        "    return result[\"text\"]\n",
        "\n",
        "def align_transcript(audio_file, transcript):\n",
        "    transcript_path = \"temp_transcript.txt\"\n",
        "\n",
        "    with open(transcript_path, \"w\") as f:\n",
        "        f.write(transcript)\n",
        "\n",
        "    print(f\"Aligning transcript with audio...\")\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            \"ctc-forced-aligner\",\n",
        "            \"--audio_path\", audio_file,\n",
        "            \"--text_path\", transcript_path,\n",
        "            \"--language\", \"eng\",\n",
        "            \"--split_size\", \"sentence\",\n",
        "            \"--romanize\",\n",
        "            \"--window_size\", \"15\"\n",
        "        ], capture_output=True, text=True)\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(\"Error in forced alignment:\")\n",
        "            print(result.stderr)\n",
        "            return None  # Indicate failure\n",
        "        else:\n",
        "            print(\"Alignment completed successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"ctc-forced-aligner not found. Please ensure it is installed and in your PATH.\")\n",
        "        return None  # Indicate failure\n",
        "\n",
        "    output_path = f\"{os.path.splitext(audio_file)[0]}.txt\"\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"Alignment file {output_path} created successfully.\")\n",
        "        return output_path  # Return the path to the output file\n",
        "    else:\n",
        "        print(f\"Failed to create alignment file {output_path}.\")\n",
        "        return None  # Indicate failure\n",
        "\n",
        "def time_to_seconds(time_str):\n",
        "    parts = time_str.split(':')\n",
        "    if len(parts) == 3:\n",
        "        h, m, s = map(float, parts)\n",
        "    elif len(parts) == 2:\n",
        "        h = 0\n",
        "        m, s = map(float, parts)\n",
        "    elif len(parts) == 1:\n",
        "        h = 0\n",
        "        m = 0\n",
        "        s = float(parts[0])\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid time format: {time_str}\")\n",
        "    return timedelta(hours=h, minutes=m, seconds=s).total_seconds()\n",
        "\n",
        "\n",
        "# Parsing functions\n",
        "def parse_vad_file(vad_file):\n",
        "    vad_segments = []\n",
        "    with open(vad_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            match = re.match(r'\\[([\\d:.]+) --> ([\\d:.]+)\\] SPEECH', line)\n",
        "            if match:\n",
        "                start_time = match.group(1)\n",
        "                end_time = match.group(2)\n",
        "                vad_segments.append({\"start\": start_time, \"end\": end_time})\n",
        "    return vad_segments\n",
        "\n",
        "def parse_text_timestamps_file(text_file):\n",
        "    text_segments = []\n",
        "    try:\n",
        "        with open(text_file, \"r\") as f:\n",
        "            for line in f:\n",
        "                match = re.match(r'([\\d:.]+)-([\\d:.]+): (.+)', line)\n",
        "                if match:\n",
        "                    start_time = match.group(1)\n",
        "                    end_time = match.group(2)\n",
        "                    text = match.group(3)\n",
        "                    text_segments.append({\"start\": start_time, \"end\": end_time, \"text\": text})\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Text timestamps file '{text_file}' not found. Returning empty segments.\")\n",
        "    print(f\"Parsed {len(text_segments)} text segments.\")\n",
        "    return text_segments\n",
        "\n",
        "def combine_segments(vad_segments, text_segments, max_duration=15):\n",
        "    combined_pairs = []\n",
        "    chunk_id = 1\n",
        "\n",
        "    print(f\"Combining {len(vad_segments)} VAD segments with {len(text_segments)} text segments...\")\n",
        "\n",
        "    for text_segment in text_segments:\n",
        "        text_start = time_to_seconds(text_segment[\"start\"])\n",
        "        text_end = time_to_seconds(text_segment[\"end\"])\n",
        "\n",
        "        segment_vads = []\n",
        "        for vad_segment in vad_segments:\n",
        "            vad_start = time_to_seconds(vad_segment[\"start\"])\n",
        "            vad_end = time_to_seconds(vad_segment[\"end\"])\n",
        "\n",
        "            if vad_start < text_end and vad_end > text_start:\n",
        "                segment_vads.append(vad_segment)\n",
        "\n",
        "        if not segment_vads:\n",
        "            continue\n",
        "\n",
        "        current_text = text_segment[\"text\"]\n",
        "        current_start_time = max(text_start, time_to_seconds(segment_vads[0][\"start\"]))\n",
        "        current_end_time = min(text_end, time_to_seconds(segment_vads[-1][\"end\"]))\n",
        "        current_duration = current_end_time - current_start_time\n",
        "\n",
        "        while current_duration > max_duration:\n",
        "            split_end_time = current_start_time + max_duration\n",
        "            combined_pairs.append({\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"chunk_length\": max_duration,\n",
        "                \"text\": current_text.strip(),\n",
        "                \"start_time\": current_start_time,\n",
        "                \"end_time\": split_end_time\n",
        "            })\n",
        "            chunk_id += 1\n",
        "            current_text = \"\"\n",
        "            current_start_time = split_end_time\n",
        "            current_duration = current_end_time - current_start_time\n",
        "\n",
        "        combined_pairs.append({\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"chunk_length\": current_duration,\n",
        "            \"text\": current_text.strip(),\n",
        "            \"start_time\": current_start_time,\n",
        "            \"end_time\": current_end_time\n",
        "        })\n",
        "        chunk_id += 1\n",
        "\n",
        "    return combined_pairs\n",
        "\n",
        "def perform_vad(audio_file):\n",
        "    print(\"Performing Voice Activity Detection...\")\n",
        "    model = Model.from_pretrained(\"pyannote/segmentation-3.0\")\n",
        "    pipeline = VoiceActivityDetection(segmentation=model)\n",
        "    HYPER_PARAMETERS = {\n",
        "    # remove speech regions shorter than that many seconds.\n",
        "    \"min_duration_on\": 0.0,\n",
        "    # fill non-speech regions shorter than that many seconds.\n",
        "    \"min_duration_off\": 0.0\n",
        "    }\n",
        "    pipeline.instantiate(HYPER_PARAMETERS)\n",
        "    vad = pipeline(audio_file)\n",
        "    # `vad` is a pyannote.core.Annotation instance containing speech regions\n",
        "\n",
        "    # Save VAD results to file\n",
        "    with open(\"vad.txt\", 'w') as f:\n",
        "        for segment, _, label in vad.itertracks(yield_label=True):\n",
        "            if label == 'SPEECH':\n",
        "                f.write(f\"[{segment.start:.2f} --> {segment.end:.2f}] SPEECH\\n\")\n",
        "\n",
        "    print(\"VAD results saved to vad.txt\")\n",
        "\n",
        "def process_youtube_video(url):\n",
        "    try:\n",
        "        print(\"Downloading video...\")\n",
        "        video_file = download_video(url)\n",
        "        print(\"Video downloaded successfully.\")\n",
        "\n",
        "        print(\"Converting video to audio...\")\n",
        "        audio_file = video2mp3(video_file)\n",
        "        print(\"Video converted to audio successfully.\")\n",
        "\n",
        "        perform_vad(audio_file)\n",
        "\n",
        "        print(\"Transcribing audio...\")\n",
        "        transcript = transcribe(audio_file)\n",
        "        print(\"Audio transcribed successfully.\")\n",
        "\n",
        "        print(\"Aligning transcript...\")\n",
        "        alignment_output_path = align_transcript(audio_file, transcript)\n",
        "        if alignment_output_path is None:\n",
        "            print(\"Failed to align transcript. Exiting process.\")\n",
        "            return {\"error\": \"Failed to align transcript.\"}\n",
        "        print(\"Transcript aligned successfully.\")\n",
        "\n",
        "        vad_segments = parse_vad_file(\"vad.txt\")\n",
        "        print(f\"VAD Segments: {vad_segments}\")\n",
        "\n",
        "        text_segments = parse_text_timestamps_file(alignment_output_path)\n",
        "        print(f\"Text Segments: {text_segments}\")\n",
        "\n",
        "        audio_text_pairs = combine_segments(vad_segments, text_segments)\n",
        "        print(f\"Combined Segments: {audio_text_pairs}\")\n",
        "\n",
        "        json_output = json.dumps(audio_text_pairs, indent=4)\n",
        "        print(\"JSON output:\", json_output)  # Print the JSON output for debugging\n",
        "        return json_output\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", str(e))  # Print any errors that occur\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_youtube_video,\n",
        "    inputs=gr.Textbox(label=\"YouTube Video URL\"),\n",
        "    outputs=gr.JSON(label=\"Combined Chunks JSON\"),\n",
        "    title=\"YouTube Video Processor\",\n",
        "    description=\"Enter a YouTube video URL and get the combined chunks as JSON.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n",
        "\n",
        "# # For running the function directly without Gradio interface\n",
        "# url = \"https://www.youtube.com/shorts/HN0PZqL-CmE\"\n",
        "# process_youtube_video(url)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "027b21f5f03a41d7bcdb36de15571423": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02d2e85cab484b3a9091341c44780c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e8a28c89e7847689790f1a9e006d9e3",
            "max": 493869,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cea11a12caaa4bee9b2d682c4f68207f",
            "value": 493869
          }
        },
        "03ba1fea762644e8a76fd9488b884e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76dadf0106c441b1a457ad906fe0e05f",
            "max": 340,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be79fbc8ae974bd092568e0b65250859",
            "value": 340
          }
        },
        "0abbd7d7de9d425ca4fca03ea7c5c9c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0acc165857db47219da645c00c244d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d715b73411c24233ab69abd640018e97",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_702a05d600714cc791042a1d977130e4",
            "value": "merges.txt:â€‡100%"
          }
        },
        "0d4910a7cc674d628b465105629f7ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2727108623a45aba30bfa9290eadb15",
              "IPY_MODEL_4cb181b9886d486b859e8d8f384d0eb3",
              "IPY_MODEL_c07a544bab5f4c489acb7321b4186f0c"
            ],
            "layout": "IPY_MODEL_2c9afd0476b64685a6dabd116c23cf93"
          }
        },
        "0d4a8bfb80324bba880c526553b965d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "110f10a327c04a62b18d7e0f4247d6ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f59e2ccd844d788570f176acf769c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1627b08f94b845cba676b0c8a1e983fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_171255372fbe4e94b31c4a23932b5dcc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9f33c1a96bf54cf3be460ec8c4a29be0",
            "value": "â€‡340/340â€‡[00:00&lt;00:00,â€‡29.6kB/s]"
          }
        },
        "171255372fbe4e94b31c4a23932b5dcc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1747b4b3036d4442bc5b1a843ade94ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "190349a826344506b65a0a64f9aaeb7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c4b80da5a754e7381926bb01de12e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c69f9b7e5094de4984a74c6554956a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aac0a67eb85340abac426355616ec816",
              "IPY_MODEL_491fd71c28104ee4a6ffe21f6f2cecfe",
              "IPY_MODEL_7a9e265782264db8b2f36d5fccd37e27"
            ],
            "layout": "IPY_MODEL_110f10a327c04a62b18d7e0f4247d6ef"
          }
        },
        "1c8cabee92294e168e241810200c45bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d34984ebd6a4cef82eb970294a451ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d4cca895efc4bd4aab8e18e85e9e72f",
              "IPY_MODEL_afa1fb1306df45b2920320c159c66644",
              "IPY_MODEL_d9c64566a9f4493a92b0d5510b264872"
            ],
            "layout": "IPY_MODEL_1747b4b3036d4442bc5b1a843ade94ca"
          }
        },
        "1d784a56e98b410bb793169e066e9782": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e8a28c89e7847689790f1a9e006d9e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fee5e3a3b9e47eda98de86b711c97c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21ece2c7669f4ee2bb12c0131747ee9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23002b38f77e48679442d6479c0162c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85308ba994034a04968aa06e3187f429",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dc0cb85116124b0ab4d3719082860695",
            "value": "preprocessor_config.json:â€‡100%"
          }
        },
        "23bf5ee24e3247aba1f8681e957b6fb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29ed7cbe7301411f805aca69657feb53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b15ba5caf564fbcb0dc43d3a0d33c11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ba6265c0bce4d8387bff4b7b1d83928": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c9afd0476b64685a6dabd116c23cf93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ce338b0b9624e679a75825f1040e2ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29ed7cbe7301411f805aca69657feb53",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ebaf7333eee34b69a629b3fd3ef967a1",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "30becc36a8bf4333ba7b72f5d1027731": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "349936cae56345c59befaa0500120b76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35433bfd59c74d209b73fbbcb8bb7c2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3857690d87de4b0bb41d102b7910eea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39f494c8811644c18526e8c06f75e1f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c3b9c6bfbe84a4b8838c98fbd147192": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8126038da974a3cafedefad5b12ba3d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7ba2d4bf061b410e8e2b247b6480a8fb",
            "value": "â€‡2.07k/2.07kâ€‡[00:00&lt;00:00,â€‡194kB/s]"
          }
        },
        "3d4cca895efc4bd4aab8e18e85e9e72f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94c6383c98504cabab8b162cca1214c9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d25de3bfa9c24af4a1a25b0400e9e20d",
            "value": "vocab.json:â€‡100%"
          }
        },
        "40c7a0487cc344b09fd435802cbf9278": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_428aa8e8d063405e8dbca9269973ce85",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eb20742584f14a1a8162fa70162c1f8a",
            "value": "generation_config.json:â€‡100%"
          }
        },
        "428aa8e8d063405e8dbca9269973ce85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4382e0dd0d8e4089827f3b82aa042130": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a01895af9945e79358c730b3bfc028": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "452b28f0c75c47609a5d88f80ff8854a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "456e8542bf3e4e7caaad92cd8569a07b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5616f8eb0079488fb860a836fbc7e441",
              "IPY_MODEL_674322fe6a084d30ab4a8cd50a5fa431",
              "IPY_MODEL_d8fcece450524a47b92a9792825141bf"
            ],
            "layout": "IPY_MODEL_f075a5d2c2f54d1f97f17d9a5c47b80b"
          }
        },
        "491fd71c28104ee4a6ffe21f6f2cecfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e096cb47d6ca4dff98e4166efec79c92",
            "max": 52666,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f48a730b379428cb907f5cba43d560b",
            "value": 52666
          }
        },
        "4961955cfb4e421a97add6a1de50f19c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99c36fc0bd6c42f28435d1ce7613259f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fa7bf1bc98134d31b0818fc76fadf286",
            "value": "â€‡2.48M/2.48Mâ€‡[00:00&lt;00:00,â€‡2.60MB/s]"
          }
        },
        "4a384350d66d46eaa67c63ff5992e024": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a908170020f46abbb94cf21135e68c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed15e746ca0f450e9d2abb747e8680e3",
              "IPY_MODEL_837b6c50109e4cab8a62a31cb12888fb",
              "IPY_MODEL_85674eaf6dea4f0281c35bad82f47869"
            ],
            "layout": "IPY_MODEL_e7d1151b76ec4526828c44355deb8689"
          }
        },
        "4c3bc919f654439ba0ef81940630a668": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb181b9886d486b859e8d8f384d0eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_933092c73f2c4be79c1ee3e513eaf7ff",
            "max": 1272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c72657dc5f846108ee5c9fa09826be4",
            "value": 1272
          }
        },
        "4f7cd9541d9343a2a7cb05fd685ba5f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f9e9e0e61b54d4b9bd2dad21271344a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "511e08a898614bda952eede1bb20822c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56094354fe604dc38e3ef245fe4b6169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ae0f628aae94e368c929b0a8959a77a",
              "IPY_MODEL_d80c56618e2e49b2ae8a6387962bd392",
              "IPY_MODEL_5bc0bceec68841688c2b1dded7236056"
            ],
            "layout": "IPY_MODEL_30becc36a8bf4333ba7b72f5d1027731"
          }
        },
        "5616f8eb0079488fb860a836fbc7e441": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23bf5ee24e3247aba1f8681e957b6fb3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_43a01895af9945e79358c730b3bfc028",
            "value": "added_tokens.json:â€‡100%"
          }
        },
        "58d8dae90d16418ab362d0a3ab2d47e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "598c0ef4b53448559a9d4fa38917d232": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b885b3769564515a45223c0d06eff36": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bc0bceec68841688c2b1dded7236056": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b15ba5caf564fbcb0dc43d3a0d33c11",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6298f658e7d74cd2a65e123555902806",
            "value": "â€‡771/771â€‡[00:00&lt;00:00,â€‡59.7kB/s]"
          }
        },
        "5bc84f33a2434f1f825db47d40fa2739": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ce338b0b9624e679a75825f1040e2ec",
              "IPY_MODEL_84e705fda43f4efdbffaa443f2619cfd",
              "IPY_MODEL_73983045b6e6492882a247cb2cf04cb1"
            ],
            "layout": "IPY_MODEL_ed60357f7c60484ab609d1ab8066bade"
          }
        },
        "5f48a730b379428cb907f5cba43d560b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60c65e0c14564a3192f852980bba90ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6298f658e7d74cd2a65e123555902806": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65711156fc334393b147f46cb068bab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66c1c4eb76df4ba5af46bb176209da67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd38c873cacf4e008cf56a30ce3b79eb",
              "IPY_MODEL_791f516d0cec46dfaab43edb023981aa",
              "IPY_MODEL_3c3b9c6bfbe84a4b8838c98fbd147192"
            ],
            "layout": "IPY_MODEL_ade73ae644ba42c696d4879f11f039af"
          }
        },
        "674322fe6a084d30ab4a8cd50a5fa431": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a384350d66d46eaa67c63ff5992e024",
            "max": 34648,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39f494c8811644c18526e8c06f75e1f0",
            "value": 34648
          }
        },
        "6b4201ef260a4dc8afb9c5c3e7ff2d6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6decc45abd4a4a0db3323dc1d7dddd08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed667cf91a2444f5946de470ed9d5adb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_65711156fc334393b147f46cb068bab0",
            "value": "â€‡5.91M/5.91Mâ€‡[00:00&lt;00:00,â€‡78.6MB/s]"
          }
        },
        "702a05d600714cc791042a1d977130e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7111e0e805e64c3b90b8db7764f7bb8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f9e9e0e61b54d4b9bd2dad21271344a",
            "max": 2480617,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a62c8083bafe40b0a8f5a4bd5608b78e",
            "value": 2480617
          }
        },
        "724a2bbdb6c34759be0ad3290e6ab47f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7b54d0620fb45dfa3f930aacdbc3cf2",
              "IPY_MODEL_7111e0e805e64c3b90b8db7764f7bb8d",
              "IPY_MODEL_4961955cfb4e421a97add6a1de50f19c"
            ],
            "layout": "IPY_MODEL_b30f14f1704e44c2b5a92d3b5e8e14f9"
          }
        },
        "7302f9148882415f874f87bd2433da7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73983045b6e6492882a247cb2cf04cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec81609203484e209c070c2ecb24c7de",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0abbd7d7de9d425ca4fca03ea7c5c9c4",
            "value": "â€‡283k/283kâ€‡[00:00&lt;00:00,â€‡22.7MB/s]"
          }
        },
        "743c4121f60c41f5ab9ba621322ba9c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76dadf0106c441b1a457ad906fe0e05f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "781232b3a5ee416491de3f86e77c835c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "791f516d0cec46dfaab43edb023981aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e67b2c92d0a446ca9a9c43d8bd14cbbd",
            "max": 2072,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d51d6414acbd4fc2a08264021e317a47",
            "value": 2072
          }
        },
        "7a9e265782264db8b2f36d5fccd37e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f817e9acfd3242609e0f9102ddff45b2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3857690d87de4b0bb41d102b7910eea7",
            "value": "â€‡52.7k/52.7kâ€‡[00:00&lt;00:00,â€‡4.40MB/s]"
          }
        },
        "7ae0f628aae94e368c929b0a8959a77a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_743c4121f60c41f5ab9ba621322ba9c9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_190349a826344506b65a0a64f9aaeb7f",
            "value": "adapter_config.json:â€‡100%"
          }
        },
        "7ba2d4bf061b410e8e2b247b6480a8fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d8ed61c028c4df0a40a46d7d696043c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ed3d38024e8427ea8039ab4e58f2c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "806f9f0b340d4cbaa3d6eff4cee3a404": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "817dc66dcdd349dfa12b66a6f6a90502": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81fceb9226b54b268a0d084abc7712bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92cc37f9369049e0aa96c26df482f1e5",
              "IPY_MODEL_a50341e0ce6d423b82e3c7f3b2b37d51",
              "IPY_MODEL_8ca5cc57adeb42a588ae0d6bbfd7c30a"
            ],
            "layout": "IPY_MODEL_f6f1d1c1179f490ca16864caaff33c03"
          }
        },
        "822f030d7f4d4ac9b56ee20ad7daafd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_817dc66dcdd349dfa12b66a6f6a90502",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f444fff4de73484e8bbec1046e23b0b1",
            "value": "â€‡3.90k/3.90kâ€‡[00:00&lt;00:00,â€‡294kB/s]"
          }
        },
        "837b6c50109e4cab8a62a31cb12888fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cceb2a151bd54a39abb11b743a3603c3",
            "max": 62969640,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be6bc2e1de61476289ad1532c7208c5c",
            "value": 62969640
          }
        },
        "83f7268639274d409e3dc08c7f24c352": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84e705fda43f4efdbffaa443f2619cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1c61b153e714c1f92db915e9316c384",
            "max": 282843,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83f7268639274d409e3dc08c7f24c352",
            "value": 282843
          }
        },
        "85308ba994034a04968aa06e3187f429": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85674eaf6dea4f0281c35bad82f47869": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_511e08a898614bda952eede1bb20822c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a7ec526932fc41b088480a90e56c8bca",
            "value": "â€‡63.0M/63.0Mâ€‡[00:02&lt;00:00,â€‡21.1MB/s]"
          }
        },
        "8ca5cc57adeb42a588ae0d6bbfd7c30a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b885b3769564515a45223c0d06eff36",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eb65558f1f94460b9f4da9e1f9271083",
            "value": "â€‡3.09G/3.09Gâ€‡[00:09&lt;00:00,â€‡442MB/s]"
          }
        },
        "8fc9666c395641a8a6b2624c0017c32b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40c7a0487cc344b09fd435802cbf9278",
              "IPY_MODEL_b4db7a44620b4980b278ac4b930a5e59",
              "IPY_MODEL_822f030d7f4d4ac9b56ee20ad7daafd9"
            ],
            "layout": "IPY_MODEL_598c0ef4b53448559a9d4fa38917d232"
          }
        },
        "92cc37f9369049e0aa96c26df482f1e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_452b28f0c75c47609a5d88f80ff8854a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1c4b80da5a754e7381926bb01de12e2a",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "933092c73f2c4be79c1ee3e513eaf7ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94c6383c98504cabab8b162cca1214c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99c36fc0bd6c42f28435d1ce7613259f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c72657dc5f846108ee5c9fa09826be4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f33c1a96bf54cf3be460ec8c4a29be0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a33574da14104f5d95713859ce69f848": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23002b38f77e48679442d6479c0162c1",
              "IPY_MODEL_03ba1fea762644e8a76fd9488b884e6e",
              "IPY_MODEL_1627b08f94b845cba676b0c8a1e983fe"
            ],
            "layout": "IPY_MODEL_d22bb179d08b4842bf7ae02e1ec73ce7"
          }
        },
        "a3ac13ce490e477980d25dc78588dc06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3e38d5da3dc4368a234319de5ed2180": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a50341e0ce6d423b82e3c7f3b2b37d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ba6265c0bce4d8387bff4b7b1d83928",
            "max": 3087130976,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3efe8bb0df1418885086235f37e642a",
            "value": 3087130976
          }
        },
        "a62c8083bafe40b0a8f5a4bd5608b78e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7b54d0620fb45dfa3f930aacdbc3cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec59df4e6f546ef976d8a20e542f24c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7ed3d38024e8427ea8039ab4e58f2c71",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "a7ec526932fc41b088480a90e56c8bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aac0a67eb85340abac426355616ec816": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3e38d5da3dc4368a234319de5ed2180",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fe32e2aa853e4083a5cdee671e556204",
            "value": "normalizer.json:â€‡100%"
          }
        },
        "ac7a1907bf2847c3a21aaf535428ef39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21ece2c7669f4ee2bb12c0131747ee9f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d3c4fc6445ea48a0876826c698f463bc",
            "value": "â€‡399/399â€‡[00:00&lt;00:00,â€‡37.2kB/s]"
          }
        },
        "ade73ae644ba42c696d4879f11f039af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa1fb1306df45b2920320c159c66644": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7569c3838a445fa918f4872d073eac1",
            "max": 1036558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_349936cae56345c59befaa0500120b76",
            "value": 1036558
          }
        },
        "afcc3d193cad475c821d407d30028b46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b082c55371394c29bec699a6177b056f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b30f14f1704e44c2b5a92d3b5e8e14f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4db7a44620b4980b278ac4b930a5e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8b8a36590af4faca16e5b2cfed064b9",
            "max": 3903,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60c65e0c14564a3192f852980bba90ab",
            "value": 3903
          }
        },
        "b8dbe72f63dd490ab48d72e439d1b502": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b939b94ae190443b80a2b053f4751545": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c8cabee92294e168e241810200c45bb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7302f9148882415f874f87bd2433da7c",
            "value": "config.yaml:â€‡100%"
          }
        },
        "be6bc2e1de61476289ad1532c7208c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be79fbc8ae974bd092568e0b65250859": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c07a544bab5f4c489acb7321b4186f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4382e0dd0d8e4089827f3b82aa042130",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1d784a56e98b410bb793169e066e9782",
            "value": "â€‡1.27k/1.27kâ€‡[00:00&lt;00:00,â€‡112kB/s]"
          }
        },
        "c0b39e10035449fe8b5209f11603a14e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8237d6473584c079a392589708f9dae",
              "IPY_MODEL_ecaefa7978ca49189f01ca0b49367746",
              "IPY_MODEL_6decc45abd4a4a0db3323dc1d7dddd08"
            ],
            "layout": "IPY_MODEL_58d8dae90d16418ab362d0a3ab2d47e4"
          }
        },
        "c205a19fe8e14f1c97afa0cda2445c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6526dfb49f042e4aee5ccb580b87733": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0acc165857db47219da645c00c244d65",
              "IPY_MODEL_02d2e85cab484b3a9091341c44780c62",
              "IPY_MODEL_eb0fbe5ce2d04136bc711a4b146c44e4"
            ],
            "layout": "IPY_MODEL_afcc3d193cad475c821d407d30028b46"
          }
        },
        "cb7c2e49cc494bfead9548a69a39cb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cceb2a151bd54a39abb11b743a3603c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cea11a12caaa4bee9b2d682c4f68207f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d22bb179d08b4842bf7ae02e1ec73ce7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d25de3bfa9c24af4a1a25b0400e9e20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3c4fc6445ea48a0876826c698f463bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d51d6414acbd4fc2a08264021e317a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d715b73411c24233ab69abd640018e97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d80c56618e2e49b2ae8a6387962bd392": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35433bfd59c74d209b73fbbcb8bb7c2b",
            "max": 771,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c205a19fe8e14f1c97afa0cda2445c95",
            "value": 771
          }
        },
        "d8126038da974a3cafedefad5b12ba3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8b8a36590af4faca16e5b2cfed064b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8fcece450524a47b92a9792825141bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b4201ef260a4dc8afb9c5c3e7ff2d6d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e9d2c4e90bdd401a85ef7005ac1da98b",
            "value": "â€‡34.6k/34.6kâ€‡[00:00&lt;00:00,â€‡3.07MB/s]"
          }
        },
        "d9c64566a9f4493a92b0d5510b264872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f896c7bef5c1431dbf056b69eb12c40e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_806f9f0b340d4cbaa3d6eff4cee3a404",
            "value": "â€‡1.04M/1.04Mâ€‡[00:00&lt;00:00,â€‡4.53MB/s]"
          }
        },
        "da792b4d9ff742bf8a13dd078f368daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc0cb85116124b0ab4d3719082860695": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd38c873cacf4e008cf56a30ce3b79eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c3bc919f654439ba0ef81940630a668",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a3ac13ce490e477980d25dc78588dc06",
            "value": "special_tokens_map.json:â€‡100%"
          }
        },
        "e096cb47d6ca4dff98e4166efec79c92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c61b153e714c1f92db915e9316c384": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2727108623a45aba30bfa9290eadb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f703716be588407cb1bc8b8d69a86cef",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_781232b3a5ee416491de3f86e77c835c",
            "value": "config.json:â€‡100%"
          }
        },
        "e3021f5703154044bd05843fd0f1e6a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b939b94ae190443b80a2b053f4751545",
              "IPY_MODEL_f4c002f6418d4200b7afa94b54497406",
              "IPY_MODEL_ac7a1907bf2847c3a21aaf535428ef39"
            ],
            "layout": "IPY_MODEL_7d8ed61c028c4df0a40a46d7d696043c"
          }
        },
        "e3efe8bb0df1418885086235f37e642a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e67b2c92d0a446ca9a9c43d8bd14cbbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7d1151b76ec4526828c44355deb8689": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8237d6473584c079a392589708f9dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f09875eb3ae54b06bb05c39a169fa8a1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_da792b4d9ff742bf8a13dd078f368daa",
            "value": "pytorch_model.bin:â€‡100%"
          }
        },
        "e9d2c4e90bdd401a85ef7005ac1da98b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb0fbe5ce2d04136bc711a4b146c44e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_027b21f5f03a41d7bcdb36de15571423",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1fee5e3a3b9e47eda98de86b711c97c1",
            "value": "â€‡494k/494kâ€‡[00:00&lt;00:00,â€‡1.03MB/s]"
          }
        },
        "eb20742584f14a1a8162fa70162c1f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb65558f1f94460b9f4da9e1f9271083": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebaf7333eee34b69a629b3fd3ef967a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec81609203484e209c070c2ecb24c7de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecaefa7978ca49189f01ca0b49367746": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8dbe72f63dd490ab48d72e439d1b502",
            "max": 5905440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f7cd9541d9343a2a7cb05fd685ba5f6",
            "value": 5905440
          }
        },
        "ed15e746ca0f450e9d2abb747e8680e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13f59e2ccd844d788570f176acf769c2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b082c55371394c29bec699a6177b056f",
            "value": "adapter_model.safetensors:â€‡100%"
          }
        },
        "ed60357f7c60484ab609d1ab8066bade": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed667cf91a2444f5946de470ed9d5adb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f075a5d2c2f54d1f97f17d9a5c47b80b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f09875eb3ae54b06bb05c39a169fa8a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f444fff4de73484e8bbec1046e23b0b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4c002f6418d4200b7afa94b54497406": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d4a8bfb80324bba880c526553b965d4",
            "max": 399,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb7c2e49cc494bfead9548a69a39cb80",
            "value": 399
          }
        },
        "f6f1d1c1179f490ca16864caaff33c03": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f703716be588407cb1bc8b8d69a86cef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7569c3838a445fa918f4872d073eac1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f817e9acfd3242609e0f9102ddff45b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f896c7bef5c1431dbf056b69eb12c40e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa7bf1bc98134d31b0818fc76fadf286": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe32e2aa853e4083a5cdee671e556204": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fec59df4e6f546ef976d8a20e542f24c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
